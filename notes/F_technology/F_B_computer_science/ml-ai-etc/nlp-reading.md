# NLP Reading

## Chatbot

* [Chatbot Development : Make conversations flawless with a Dialog Manager](https://medium.com/@snehasish069/how-do-automated-conversational-agents-learn-80d2c62e5594?source=email-b12ac70303b2-1579309424199-digest.reader------0-59------------------f561c61d_2c66_4caf_ac44_039ac6d662f7-1-----&sectionName=top)
* [How Chatbots Hold Power to Revolutionize the Finance Services](https://www.datasciencecentral.com/profiles/blogs/how-chatbots-hold-power-to-revolutionize-the-finance-services)
* [huseinzol05/NLP-Models-Tensorflow/tree/master/chatbot](https://github.com/huseinzol05/NLP-Models-Tensorflow/tree/master/chatbot)
* [Inside The Machine Learning that Google Used to Build Meena: A Chatbot that Can Chat About Anything](https://kdnuggets.us12.list-manage.com/track/click?u=4f2891ebb155b23f120ece0bd&id=6bd8f46826&e=b34ab4e857)
* [rasa.com/docs/rasa/](https://rasa.com/docs/rasa/)
* [spacy.io/](https://spacy.io/)
* [RasaHQ/rasa](https://github.com/RasaHQ/rasa)    $ rasa-nlu-trainer
* [Rasa Examples](https://github.com/RasaHQ/rasa/tree/master/examples)
* [Chatbot with Rasa (Coding Tech)](https://www.youtube.com/watch?v=sazsWmP2d3o)
* [NLP for Developers](https://www.youtube.com/playlist?list=PL75e0qA87dlFJiNMeKltWImhQxfFwaxvv)
* [Rasa HQ Playlists](https://www.youtube.com/c/RasaHQ/playlists)
* [steadforce.com/chatbot-erstellen-rasa/](https://steadforce.com/chatbot-erstellen-rasa/)
* [tuhinssam/covid19-rasa-chatbot](https://github.com/tuhinssam/covid19-rasa-chatbot)
* [blog.moelter-online.de/wie-baue-ich-einen-chatbot/](http://blog.moelter-online.de/wie-baue-ich-einen-chatbot/)
* [Archived | Create a news chatbot to deliver content through Facebook Messenger – Build Smart. Build Secure. IBM Developer](https://developer.ibm.com/tutorials/cc-cognitive-chatbot-facebook/)
* [blog.rasa.com/tag/tutorials/](https://blog.rasa.com/tag/tutorials/)
* [Code a Discord Bot with Python - Host for Free in the Cloud - YouTube](https://www.youtube.com/watch?v=SPTfmiYiuok)
* [chatbot deutsch für ein restaurant - Cerca con Google](https://www.google.com/search?q=chatbot+deutsch+f%C3%BCr+ein+restaurant&ie=UTF-8&oe=UTF-8&hl=it-us&client=safari)
* [botfront/botfront: A powerful Rasa UI to build advanced AI assistants.](https://github.com/botfront/botfront)
* [Django-Rasa-Bot](https://github.com/Alexmhack/Django-Rasa-Bot)
* [BotFlow - a platform for creating contents for Rasa chatbots](https://github.com/lappis-unb/BotFlow)
* [Clofus® Chat Bot Platform](https://github.com/clofus/clofus-chatbot)
* [dhrubach/nlp_bot_restaurant: a restaurant chatbot using Rasa bot framework](https://github.com/dhrubach/nlp_bot_restaurant)
* [Rasa Helpdesk Assistant Example](https://github.com/RasaHQ/helpdesk-assistant)
* [jackdh/RasaTalk: A chatbot framework for Rasa NLU](https://github.com/jackdh/RasaTalk)
* [How Can We Improve Peer Review in NLP?](https://thegradient.pub/how-can-we-improve-peer-review-in-nlp/)
* [freecodecamp.org/news/create-a-discord-bot-with-python/](https://www.freecodecamp.org/news/create-a-discord-bot-with-python/)
* [rasa.com/docs/rasa/rules](https://rasa.com/docs/rasa/rules)
* [rasa.com/docs/rasa/forms](https://rasa.com/docs/rasa/forms)
* [rasa.com/docs/rasa/business-logic](https://rasa.com/docs/rasa/business-logic)
* [medium.com/rasa-blog/building-contextual-assistants-with-rasa-forms-7c034c9a8677](https://medium.com/rasa-blog/building-contextual-assistants-with-rasa-forms-7c034c9a8677)    RasaOpenSource2 code    Rasa Demo code
* [rasa.com/docs/rasa/glossary#form](https://rasa.com/docs/rasa/glossary#form)
* [How to Integrate Forms with Rasa](https://www.youtube.com/watch?v=qnqBHG0E9EY)
* [Implementing custom actions, forms and fallback | Rasa](https://www.youtube.com/watch?v=9POI7LiKH_8&t=199s)
* [FORMS IN DETAILS](https://www.youtube.com/watch?v=jj4BL9o3Q7o)
* [Dialect Quiz Bot (Rasa Forms)](https://www.youtube.com/watch?v=Seylv2TswMU)
* [rasa.com/docs/rasa/training-data-format](https://rasa.com/docs/rasa/training-data-format)    Financial Demo    * [Advances In Commercial Deployment Of Spoken Dialog Systems](http://libgen.rs/book/index.php?md5=8C9D0B86A30FE2FCA8633CE0130808A0)
* [Natural Interaction with Robots, Knowbots and Smartphones: Putting Spoken Dialog Systems into Practice](http://libgen.rs/book/index.php?md5=6A66480D5EE173C89BC6A994DCA68205)
* [Natural Language Dialog Systems and Intelligent Assistants](http://libgen.rs/book/index.php?md5=38BDA6DBF4D7EDAC56EA8CE843B9B735)
* [Towards Adaptive Spoken Dialog Systems](http://libgen.rs/book/index.php?md5=08CB051886710E3719F0E0E08FE7B19C)
* [The Rasa Masterclass Handbook: Episode 1](https://blog.rasa.com/the-rasa-masterclass-handbook-episode-1/)
* [The Rasa Masterclass Handbook: Episode 2](https://blog.rasa.com/the-rasa-masterclass-handbook-episode-2/)
* [The Rasa Masterclass Handbook: Episode 3](https://blog.rasa.com/the-rasa-masterclass-handbook-episode-3/)
* [The Rasa Masterclass Handbook: Episode 4](https://blog.rasa.com/the-rasa-masterclass-handbook-episode-4/)
* [The Rasa Masterclass Handbook: Episode 5](https://blog.rasa.com/the-rasa-masterclass-handbook-episode-5/)
* [The Rasa Masterclass Handbook: Episode 6](https://blog.rasa.com/the-rasa-masterclass-handbook-episode-6-2/)
* [The Rasa Masterclass Handbook: Episode 7](https://blog.rasa.com/the-rasa-masterclass-handbook-episode-7/)
* [The Rasa Masterclass Handbook: Episode 8](https://blog.rasa.com/the-rasa-masterclass-handbook-episode-8/)
* [The Rasa Masterclass Handbook: Episode 9](https://blog.rasa.com/the-rasa-masterclass-handbook-episode-9/)
* [The Rasa Masterclass Handbook: Episode 10](https://blog.rasa.com/the-rasa-masterclass-handbook-episode-10/)
* [The Rasa Masterclass Handbook: Episode 11](https://blog.rasa.com/the-rasa-masterclass-handbook-episode-11/)
* [The Rasa Masterclass Handbook: Episode 12](https://blog.rasa.com/the-rasa-masterclass-handbook-episode-12/)
* [Connect a Rasa Chatbot to a Django Website | Rasa Tutorial](https://www.youtube.com/watch?v=XMAw_bKTLbA)
* [Conversational AI Rasa chatbot tutorial - Hands-on building ResturantBot (Part 6) | Vishal Pandey](https://www.youtube.com/watch?v=m0T6TzLcCgY)
* [An Open-Source Chatbot Made With Rasa](https://www.youtube.com/watch?v=sazsWmP2d3o)
* [Create Basic Conversational AI Chatbot using RASA NLU](https://www.youtube.com/watch?v=hqkfHK7-O08)
* [Rasa Chatbot Tutorial | All important concept | NLU | CORE | Building First chatbot | RASA - 3](https://www.youtube.com/watch?v=Z9zTJS6cWiQ)
* [Building your first chat bot with UI using Rasa X](https://www.youtube.com/watch?v=GwaSJUlB8oA)
* [How to Create Conversational AI Chatbot using RASA (Python) by Cisco Data Scientist](https://www.youtube.com/watch?v=Nk9K4s8g9yQ)
* [Rasa Chatbot Tutorial | Custom Action Implementation | RASA -4](https://www.youtube.com/watch?v=AqNuAqb2oFg)
* [Create Basic Chatbot Using Rasa NLU and Rasa Core (Practical Example)](https://www.youtube.com/watch?v=sofMxJF4CZ4)    RASA OPEN SOURCE 2.0 | SLOT FILLING
* [Binod Suman Academy - Rasa](https://www.youtube.com/channel/UCIZ3CSNL-8mBHfFZLic1-aQ/search?query=rasa)
* [Innovate Yourself](https://www.youtube.com/c/InnovateYourselfashu/playlists)
* [RasaHQ/rasa-masterclass](https://github.com/RasaHQ/rasa-masterclass)
* [rasahq.github.io/rasa-nlu-examples/](https://rasahq.github.io/rasa-nlu-examples/)
* [kumar saurav](https://www.youtube.com/channel/UCGMWpYVwPwojcHXBO5HS6Aw)
* [towardsdatascience.com/chatbots-made-easier-with-rasa-2-0-b999323cdde](https://towardsdatascience.com/chatbots-made-easier-with-rasa-2-0-b999323cdde)
* [Rasa Masterclass](https://www.youtube.com/playlist?list=PL75e0qA87dlHQny7z43NduZHPo6qd-cRc)    Sample Repos
* [dhrubach/nlp_bot_restaurant](https://github.com/dhrubach/nlp_bot_restaurant)
* [shubh1608/Restaurant-Finder-Chatbot](https://github.com/shubh1608/Restaurant-Finder-Chatbot)
* [RakeshKumar045/Restaurant_Rasa_Chatbot_Python](https://github.com/RakeshKumar045/Restaurant_Rasa_Chatbot_Python)
* [Vaakash89/Rasa-Restaurant-Chatbot](https://github.com/Vaakash89/Rasa-Restaurant-Chatbot)
* [VishyNair/Restaurant-Chatbot-using-RASA](https://github.com/VishyNair/Restaurant-Chatbot-using-RASA)
* [shabbirc/Restaurant-Search-Chatbot](https://github.com/shabbirc/Restaurant-Search-Chatbot)
* [gauravgola96/rasa-slack-bot](https://github.com/gauravgola96/rasa-slack-bot)
* [AdamSpannbauer/app_rasa_chat_bot](https://github.com/AdamSpannbauer/app_rasa_chat_bot)
* [V-I-C-T-O-R/rasa-chatbot](https://github.com/V-I-C-T-O-R/rasa-chatbot)
* [LetsUpgrade/CHIT-CHAT](https://github.com/LetsUpgrade/CHIT-CHAT)
* [srslakshmi1997/query-bot-using-python](https://github.com/srslakshmi1997/query-bot-using-python)
* [Rabbia1203/chatbot-rasa](https://github.com/Rabbia1203/chatbot-rasa)
* [iamunhoz/librarian](https://github.com/iamunhoz/librarian)
* [bghanchi/AI-Chat-Bot-in-Python-using-NLP](https://github.com/bghanchi/AI-Chat-Bot-in-Python-using-NLP)
* [RasaHQ/rasa-demo](https://github.com/RasaHQ/rasa-demo)
* [Algorithm Whiteboard](https://www.youtube.com/watch?v=wWNMST6t1TA&list=PL75e0qA87dlG-za8eLI6t0_Pbxafk-cxb)
* [courses.washington.edu/ling575/SPR2017/index.html](http://courses.washington.edu/ling575/SPR2017/index.html)
* [dialog-systems-class.github.io/readings.html#now](https://dialog-systems-class.github.io/readings.html#now)
* [dialog-systems-class.github.io/resources.html](https://dialog-systems-class.github.io/resources.html)
* [ufal.mff.cuni.cz/courses/npfl123](https://ufal.mff.cuni.cz/courses/npfl123)
* [web.stanford.edu/~jurafsky/slp3/24.pdf](https://web.stanford.edu/~jurafsky/slp3/24.pdf)

## Viterbi    * [(ML 14.11) Viterbi algorithm (part 1)](https://www.youtube.com/watch?v=RwwfUICZLsA&list=PLVWtXi_Jrj_2CjtJJGAArDfyzKg2IVgkp&index=11)

* [(ML 14.12) Viterbi algorithm (part 2)](https://www.youtube.com/watch?v=t3JIk3Jgifs&list=PLVWtXi_Jrj_2CjtJJGAArDfyzKg2IVgkp&index=12)
* [(ML 14.7) Forward algorithm (part 1)](https://www.youtube.com/watch?v=M7afek1nEKM&list=PLVWtXi_Jrj_2CjtJJGAArDfyzKg2IVgkp&index=6)
* [(ML 14.8) Forward algorithm (part 2)](https://www.youtube.com/watch?v=MPmrFu4jFk4&list=PLVWtXi_Jrj_2CjtJJGAArDfyzKg2IVgkp&index=8)
* [(ML 14.9) Backward algorithm](https://www.youtube.com/watch?v=jwYuki9GgJo&list=PLVWtXi_Jrj_2CjtJJGAArDfyzKg2IVgkp&index=9)
* [ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial](https://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial)
* [Project 4: Ghostbusters](http://modelai.gettysburg.edu/2010/pacman/projects/tracking/busters.html)
* [Tutorial — hmmlearn 0.2.2.post2+g202e415 documentation](https://hmmlearn.readthedocs.io/en/latest/tutorial.html)
* [stellar.mit.edu/S/course/6/fa12/6.047/courseMaterial/topics/topic2/lectureNotes/Lecture07_HMMsIIb_6up/Lecture07_HMMsIIb_6up.pdf](http://stellar.mit.edu/S/course/6/fa12/6.047/courseMaterial/topics/topic2/lectureNotes/Lecture07_HMMsIIb_6up/Lecture07_HMMsIIb_6up.pdf)
* [hidden markov model - What is the difference between the forward-backward and Viterbi algorithms? - Cross Validated](https://stats.stackexchange.com/questions/31746/what-is-the-difference-between-the-forward-backward-and-viterbi-algorithms)
* [aldengolab/hidden-markov-model: A from-scratch Hidden Markov Model for hidden state learning from observation sequences.](https://github.com/aldengolab/hidden-markov-model)
* [cs.jhu.edu/~jason/465/hw-hmm/hw-hmm.pdf](http://www.cs.jhu.edu/~jason/465/hw-hmm/hw-hmm.pdf)
* [cs.jhu.edu/~jason/papers/jurafsky+martin.slp3draft.ch9.pdf](http://www.cs.jhu.edu/~jason/papers/jurafsky+martin.slp3draft.ch9.pdf)
* [cs.jhu.edu/~jason/papers/eisner.tnlp02.pdf](http://www.cs.jhu.edu/~jason/papers/eisner.tnlp02.pdf)
* [dorairajsanjay/hmm_tutorial: HMM Tutorial](https://github.com/dorairajsanjay/hmm_tutorial)
* [Code for a Hidden Markov Model, along with some sample data / parameters for testing.](https://gist.github.com/dougalsutherland/1329976)
* [modelai.gettysburg.edu/2017/hmm/files/hmm.py](http://modelai.gettysburg.edu/2017/hmm/files/hmm.py)
* [The Beginner Programmer: Basic Hidden Markov model](http://firsttimeprogrammer.blogspot.com/2015/08/basic-hidden-markov-model.html)
* [hmmlearn/lib/hmmlearn at master · hmmlearn/hmmlearn](https://github.com/hmmlearn/hmmlearn/tree/master/lib/hmmlearn)
* [cs.jhu.edu/~langmea/resources/lecture_notes/hidden_markov_models.pdf](http://www.cs.jhu.edu/~langmea/resources/lecture_notes/hidden_markov_models.pdf)
* [pl91.ddns.net/viterbi/tutorial.html](http://pl91.ddns.net/viterbi/tutorial.html)
* [Viterbi algorithm - Wikipedia](https://en.wikipedia.org/wiki/Viterbi_algorithm)
* [The Viterbi Algorithm: A Personal History](https://arxiv.org/pdf/cs/0504020v2.pdf)
* [kanungo.com/software/hmmtut.pdf](http://www.kanungo.com/software/hmmtut.pdf)
* [Viterbi algorithm - Scholarpedia](http://www.scholarpedia.org/article/Viterbi_algorithm)
* [A deep dive into part-of-speech tagging using the Viterbi algorithm](https://www.freecodecamp.org/news/a-deep-dive-into-part-of-speech-tagging-using-viterbi-algorithm-17c8de32e8bc/)
* [gipsa-lab.grenoble-inp.fr/~jean-marc.brossier/printable-slides-viterbi.pdf](http://www.gipsa-lab.grenoble-inp.fr/~jean-marc.brossier/printable-slides-viterbi.pdf)
* [Algorithm Implementation/Viterbi algorithm - Wikibooks, open books for an open world](https://en.wikibooks.org/wiki/Algorithm_Implementation/Viterbi_algorithm)
* [nlp - Viterbi algorithm in java - Stack Overflow](https://stackoverflow.com/questions/15348351/viterbi-algorithm-in-java)
* [Viterbi algorithm in R - Number of the replaced elements is not a multiple of replacement length error - Stack Overflow](https://stackoverflow.com/questions/24274901/viterbi-algorithm-in-r-number-of-the-replaced-elements-is-not-a-multiple-of-re)
* [Viterbi Decoding of Convolutional Codes](http://web.mit.edu/6.02/www/f2010/handouts/lectures/L9.pdf)
* [Python Implementation of Viterbi Algorithm - Stack Overflow](https://stackoverflow.com/questions/9729968/python-implementation-of-viterbi-algorithm)
* [betteridiot: Office Hours! Hidden Markov Models (HMMs) and the Viterbi Algorithm - YouTube](https://www.youtube.com/watch?v=gN3BIA11MtY)
* [(ML 14.11) Viterbi algorithm (part 1) - YouTube](https://www.youtube.com/watch?v=RwwfUICZLsA)
* [Hidden Markov Models - YouTube](https://www.youtube.com/watch?v=9yl4XGp5OEg&t=638s)
* [Mod-01 Lec-18 HMM, Viterbi, Forward Backward Algorithm - YouTube](https://www.youtube.com/watch?v=BJ2pAm2wBAU)
* [HMM– Baum Welsh and Viterbi Algorithms - YouTube](https://www.youtube.com/watch?v=h22nGEF8PUo)
* [Machine Learning #46 - HMMs #3 - Das Dekodierungsproblem und der Viterbi Algorithmus - YouTube](https://www.youtube.com/watch?v=lDh8i7AV0bE)
* [The Viterbi Algorithm - YouTube](https://www.youtube.com/watch?v=0dVUfYF8ko0&t=272s)
* [Viterbi matrix for calculating the best POS tag sequence of a HMM POS tagger - YouTube](https://www.youtube.com/watch?v=_568XqOByTs)
* [L-101 Viterbi Algorithm Basics, Process & Example with trellis diagram in digital communication - YouTube](https://www.youtube.com/watch?v=GNVVCDtYXU4)
* [Template Models: Hidden Markov Models - Stanford University - YouTube](https://www.youtube.com/watch?v=mNSQ-prhgsw)
* [(ML 14.12) Viterbi algorithm (part 2) - YouTube](https://www.youtube.com/watch?v=t3JIk3Jgifs)
* [Digital Communications: Viterbi Algorithm - YouTube](https://www.youtube.com/watch?v=dKIf6mQUfnY)
* [Lecture 18 HMMs - YouTube](https://www.youtube.com/watch?v=9dp4whVQv5s)
* [Viterbi algorithm: finding most likely sequence in HMM | Does this make sense?](https://jyyuan.wordpress.com/2014/01/22/viterbi-algorithm-finding-most-likely-sequence-in-hmm/)
* [Implement Viterbi Algorithm in Hidden Markov Model using Python and R - A Developer Diary](http://www.adeveloperdiary.com/data-science/machine-learning/implement-viterbi-algorithm-in-hidden-markov-model-using-python-and-r/)
* [Main Page - Lark](https://lark-parser.readthedocs.io/en/latest/)
* [trevorcohn.github.io/comp90042/slides/WSTA_L15_probabilistic_sequence_models.pdf](https://trevorcohn.github.io/comp90042/slides/WSTA_L15_probabilistic_sequence_models.pdf)
* [8. Analyzing Sentence Structure (Extras)
* [tomerfiliba/tau: A junkyard of all Tel Aviv University-related projects](https://github.com/tomerfiliba/tau)
* [Parsing PDF](https://wiki.tcl-lang.org/page/Parsing+PDF)

## Earley

* [Vorbereitung aufs Studium - Hojas de cálculo de Google](https://docs.google.com/spreadsheets/d/15f-YFveAHgixxzIFuwNSAyzvfi7A7TRg5Wt4LCxayuo/edit#gid=747140405)
* [Damir Cavar’s Scheme code page | Damir Cavar’s Homepage](http://damir.cavar.me/code/scheme.html)
* [cl.lingfil.uu.se/~sara/kurser/5LN455-2014/lectures/5LN455-F5.pdf](https://cl.lingfil.uu.se/~sara/kurser/5LN455-2014/lectures/5LN455-F5.pdf)
* [Polnisch Lernen! 500 Polnische Redewendungen + PARALLEL AUDIO #Teil 1 - YouTube](https://www.youtube.com/watch?v=rL5iK6d42ys)
* [Earley Parsing Explained](http://loup-vaillant.fr/tutorials/earley-parsing/)
* [From Imperative to Functional: how to make the leap.](http://loup-vaillant.fr/tutorials/from-imperative-to-functional)
* [inf.ed.ac.uk/teaching/courses/inf2a/slides2017/earley-example.pdf](https://www.inf.ed.ac.uk/teaching/courses/inf2a/slides2017/earley-example.pdf)
* [courses.washington.edu/ling571/ling571_fall_2010/slides/parsing_earley.pdf](http://courses.washington.edu/ling571/ling571_fall_2010/slides/parsing_earley.pdf)
* [cs.unm.edu/~luger/ai-final2/CH9_Dynamic](https://www.cs.unm.edu/~luger/ai-final2/CH9_Dynamic)
* [cse.unt.edu/~tarau/teaching/NLP/Earley parser.pdf](http://www.cse.unt.edu/~tarau/teaching/NLP/Earley%20parser.pdf)
* [cs.unm.edu/~luger/ai-final2/JAVA/CH](https://www.cs.unm.edu/~luger/ai-final2/JAVA/CH)
* [Ring (mathematics) - Wikipedia](https://en.wikipedia.org/wiki/Ring_(mathematics)#Basic_examples)
* [Why do good people do bad things?](https://www.youtube.com/results?search_query=%D1%86%D1%80%D0%BD+%D0%B2%D1%89+%D0%BF%D1%89%D1%89%D0%B2+%D0%B7%D1%83%D1%89%D0%B7%D0%B4%D1%83+%D0%B2%D1%89+%D0%B8%D1%84%D0%B2+%D0%B5%D1%80%D1%88%D1%82%D0%BF%D1%96)
* [Comedy Roast Show: Olaf Schubert | SPASSZONE - YouTube](https://www.youtube.com/watch?v=BeT_i21HwU8)
* [Algebraic structure - Wikipedia](https://en.wikipedia.org/wiki/Algebraic_structure)
* [(2) Mormon Stories Podcast Community](https://www.facebook.com/groups/mormonstories/?multi_permalinks=2687734341238021&notif_id=1567338943393480&notif_t=group_highlights)
* [Citrix Workspace](https://byuapps.cloud.com/Citrix/StoreWeb/#/apps/all)
* [Einführung in die Computerlinguistik Chart-Parsing](https://user.phil.hhu.de/~petersen/Einf_CL_0910/material/EinfCL_18_handout.pdf)
* [EarleyParser.pdf](https://www.english-linguistics.de/fr/teaching/ws06-07/cl2/slides/EarleyParser.pdf)
* Earley demo
* [earley-parser-js/earley-oop.js at master · lagodiuk/earley-parser-js](https://github.com/lagodiuk/earley-parser-js/blob/master/earley-oop.js)
* [Tiny JavaScript parser of the subset of German (Earley)
* [Automatische Syntaxanalyse (Parsing)](https://user.phil-fak.uni-duesseldorf.de/~kallmeyer/Parsing/)
* [Parsing - Earley Parsing](https://user.phil-fak.uni-duesseldorf.de/~kallmeyer/Parsing/earley.pdf)
* [4srcg-parsing.pdf](http://www.sfs.uni-tuebingen.de/emmy/parsing/slides/4srcg-parsing.pdf)
* [NLTK Earley example](http://www.cs.yale.edu/homes/radev/nlpclass/slides2017/245.pdf)
* [EarleyChart.pdf](http://www.coli.uni-saarland.de/courses/FLST/2007/slides/EarleyChart.pdf)
* [Earley parser - Wikipedia](https://en.wikipedia.org/wiki/Earley_parser)
* [Charty - Earley/Chart Parser: Scheme](https://web.archive.org/web/20160401103410/http://www.cavar.me/damir/charty/scheme/)
* [Charty - Earley/Chart Parser: Scheme](http://damir.cavar.me/charty/scheme/)
* [Earley Parsing Explained](http://loup-vaillant.fr/tutorials/earley-parsing/)
* [Earley/bench at master · ollef/Earley](https://github.com/ollef/Earley/tree/master/bench)
* [Python使用nltk进行Chart Parsing - lijiancheng0614](https://lijiancheng0614.github.io/2015/10/25/2015_10_25_Python_nltk/)
* [PowerPoint Presentation](http://demo.clab.cs.cmu.edu/NLP/S19/files/slides/13-earley.pdf)
* The Earley Recogniser
* [yaep/src at master · vnmakarov/yaep](https://github.com/vnmakarov/yaep/tree/master/src)

## GAN    * [Temporal coherency based criteria for predicting video frames using deep multi-stage generative adversarial networks. - Cerca con Google](https://www.google.com/search?channel=fs&client=ubuntu&q=Temporal+coherency+based+criteria+for+predicting+video+frames+using+deep+multi-stage+generative+adversarial+networks.)

* [cjbayron/c-rnn-gan.pytorch: PyTorch implementation of C-RNN-GAN for Music Generation](https://github.com/cjbayron/c-rnn-gan.pytorch)
* [sumuzhao/CycleGAN-Music-Style-Transfer: Symbolic Music Genre Transfer with CycleGAN](https://github.com/sumuzhao/CycleGAN-Music-Style-Transfer)
* [khornlund/CycleGAN-Music-Style-Transfer-1: Symbolic Music Genre Transfer with CycleGAN](https://github.com/khornlund/CycleGAN-Music-Style-Transfer-1)
* [C-RNN-GAN: Continuous recurrent neural networks with adversarial training | Papers With Code](https://paperswithcode.com/paper/c-rnn-gan-continuous-recurrent-neural)
* [SeqST-GAN: Seq2Seq生成对抗网络的多步城市人群流动预测_zuiyishihefang的博客-CSDN博客](https://blog.csdn.net/zuiyishihefang/article/details/116280116)
* [jianguoz/Seq2Seq-Gan-Autoencoder: GAN and Seq2Seq](https://github.com/jianguoz/Seq2Seq-Gan-Autoencoder)
* [improving_conditional_sequence.pdf](https://openreview.net/pdf?id=HyxSG1Z3IX)
* [anuprulez/clade_prediction: Sequence generation/sequence to sequence learning using amino acid sequences of Corona virus (SARS-COV2) by training on generative adversial network (GAN)](https://github.com/anuprulez/clade_prediction)
* [[1908.05551] Conditional LSTM-GAN for Melody Generation from Lyrics](https://arxiv.org/abs/1908.05551)
* [yy1lab/Lyrics-Conditioned-Neural-Melody-Generation](https://github.com/yy1lab/Lyrics-Conditioned-Neural-Melody-Generation)
* [Time-series Generative Adversarial Networks - NeurIPS-2019-time-series-generative-adversarial-networks-Paper.pdf](https://papers.nips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf)
* [tsgan/alg/timegan at master · firmai/tsgan](https://github.com/firmai/tsgan/tree/master/alg/timegan)
* [firmai/mtss-gan: MTSS-GAN: Multivariate Time Series Simulation with Generative Adversarial Networks (by @firmai)](https://github.com/firmai/mtss-gan)
* [amitadate/S-LSTM-GAN-MNIST: This repository contains the source for the paper "S-LSTM-GAN: Shared recurrent neural networks with adversarial training"](https://github.com/amitadate/S-LSTM-GAN-MNIST)    An LSTM Based Generative Adversarial Architecture for Robotic Calligraphy Learning System - sustainability-12-09092-v2.pdf
* [CycleGAN-VC3/model.py at main · jackaduma/CycleGAN-VC3](https://github.com/jackaduma/CycleGAN-VC3/blob/main/model.py#L94)
* [jackaduma/CycleGAN-VC3: Voice Conversion by CycleGAN (语音克隆/语音转换)：CycleGAN-VC3](https://github.com/jackaduma/CycleGAN-VC3)
* [[2105.01531] VQCPC-GAN: Variable-Length Adversarial Audio Synthesis Using Vector-Quantized Contrastive Predictive Coding](https://arxiv.org/abs/2105.01531)
* [When GloVe Meets GAN](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/reports/6908905.pdf)
* [GANChat](https://www.diva-portal.org/smash/get/diva2:1452195/FULLTEXT02)
* [2021-Waspaa-nistal.pdf](https://perso.telecom-paristech.fr/grichard/Publications/2021-Waspaa-nistal.pdf)
* [seqgan](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14344/14489)    SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient - 14344-66977-1-PB.pdf
* [google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjS-6rn4NDyAhWOCewKHVF9CnAQFnoECAMQAQ&url=https%3A%2F%2Fwww.mdpi.com%2F2227-7390%2F9%2F4%2F387%2Fpdf&usg=AOvVaw1Bt1-i7aM7Fp8OmwRJ2GtX](https://www.mdpi.com/2227-7390/9/4/387/pdf)    INCO-GAN: Variable-Length Music Generation Method Based on Inception Model-Based Conditional GAN - mathematics-09-00387.pdf
* [MaskCycleGAN-VC/test.py at main · GANtastic3/MaskCycleGAN-VC](https://github.com/GANtastic3/MaskCycleGAN-VC/blob/main/mask_cyclegan_vc/test.py)    Sequence-to-Sequence Voice Conversion with Similarity Metric Learned Using Generative Adversarial Networks
* [Rhythm-Flexible Voice Conversion without Parallel Data Using Cycle-GAN over Phoneme Posteriorgram Sequences](https://arxiv.org/abs/1808.03113)
* [Parallel-Data-Free Voice Conversion Using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1711.11293)
* [Cyclegan-VC2: Improved Cyclegan-based Non-parallel Voice Conversion](https://ieeexplore.ieee.org/abstract/document/8682897)
* [CycleGAN-VC3: Examining and Improving CycleGAN-VCs for Mel-spectrogram Conversion](https://paperswithcode.com/paper/cyclegan-vc3-examining-and-improving-cyclegan)
* [ATTS2S-VC: Sequence-to-sequence Voice Conversion with Attention and Context Preservation Mechanisms](https://ieeexplore.ieee.org/abstract/document/8683282)
* [AUTOVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss](https://paperswithcode.com/paper/zero-shot-voice-style-transfer-with-only)
* [Parallel-Data-Free Voice Conversion Using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1711.11293)
* [CycleGAN-VC3: Examining and Improving CycleGAN-VCs for Mel-spectrogram Conversion](https://arxiv.org/abs/2010.11672)
* [One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization](https://paperswithcode.com/paper/one-shot-voice-conversion-by-separating)
* [CycleGAN-VC2: Improved CycleGAN-based Non-parallel Voice Conversion](https://paperswithcode.com/paper/cyclegan-vc2-improved-cyclegan-based-non)
* [Vector-quantized neural networks for acoustic unit discovery in the ZeroSpeech 2020 challenge](https://paperswithcode.com/paper/vector-quantized-neural-networks-for-acoustic)
* [MelGAN-VC: Voice Conversion and Audio Style Transfer on arbitrarily long samples using Spectrograms](https://paperswithcode.com/paper/melgan-vc-voice-conversion-and-audio-style)
* [acetylSv/rhythmic-flexible-vc-arch](https://github.com/acetylSv/rhythmic-flexible-vc-arch)
* [Voice Conversion from Unaligned Corpora using Variational Autoencoding Wasserstein Generative Adversarial Networks](https://paperswithcode.com/paper/voice-conversion-from-unaligned-corpora-using)
* [Voice Conversion from Non-parallel Corpora Using Variational Auto-encoder](https://paperswithcode.com/paper/voice-conversion-from-non-parallel-corpora)
* [FragmentVC: Any-to-Any Voice Conversion by End-to-End Extracting and Fusing Fine-Grained Voice Fragments With Attention](https://paperswithcode.com/paper/fragmentvc-any-to-any-voice-conversion-by-end)
* [Assem-VC: Realistic Voice Conversion by Assembling Modern Speech Synthesis Techniques](https://paperswithcode.com/paper/assem-vc-realistic-voice-conversion-by)
* [Voice Conversion Based on Cross-Domain Features Using Variational Auto Encoders](https://paperswithcode.com/paper/voice-conversion-based-on-cross-domain)
* [MaskCycleGAN-VC: Learning Non-parallel Voice Conversion with Filling in Frames](https://arxiv.org/abs/2102.12841)
* [MaskCycleGAN-VC: Learning Non-parallel Voice Conversion with Filling in Frames](https://paperswithcode.com/paper/maskcyclegan-vc-learning-non-parallel-voice)
* [CycleGAN-VC3: Examining and Improving CycleGAN-VCs for Mel-spectrogram Conversion](https://paperswithcode.com/paper/cyclegan-vc3-examining-and-improving-cyclegan)
* [kecl.ntt.co.jp/people/kaneko.takuhiro/projects/maskcyclegan-vc/index.html](http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/maskcyclegan-vc/index.html)
* [kecl.ntt.co.jp/people/kaneko.takuhiro/projects/cyclegan-vc3/index.html](http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/cyclegan-vc3/index.html)
* [SamuelBroughton/StarGAN-Voice-Conversion-2](https://github.com/SamuelBroughton/StarGAN-Voice-Conversion-2)    StarGAN-VC2: Rethinking Conditional Methods for StarGAN-Based Voice Conversion    * [inzva/Audio-Style-Transfer](https://github.com/inzva/Audio-Style-Transfer)
* [Audio Style Transfer](http://cs230.stanford.edu/projects_spring_2021/reports/76.pdf)
* [SANE2019 | Hirokazu Kameoka - Voice conversion with image-to-image translation and seq2seq learning](https://www.youtube.com/watch?v=2A8XByosfnw)
* [[INTERSPEECH 2020] Voice Conversion Using Speech-to-Speech Neuro-Style Transfer](https://www.youtube.com/watch?v=zbVQwqx-kYk)
* [Neural Voice Cloning (video)](https://www.youtube.com/watch?v=gVehTbi6Ipc)
* [[1703.02291] Triple Generative Adversarial Nets](https://arxiv.org/abs/1703.02291)
* [[1711.05084] TripletGAN: Training Generative Model with Triplet Loss](https://arxiv.org/abs/1711.05084)
* [A Multi-Discriminator CycleGAN for Unsupervised Non-Parallel Speech Domain Adaptation](https://arxiv.org/abs/1804.00522)
* [ADAGAN:   ADAPTIVE GAN FOR MANY-TO-MANY NON-PARALLEL VOICE CONVERSION](https://openreview.net/pdf?id=HJlk-eHFwH)
* [ADAGAN: ADAPTIVE GAN FOR MANY-TO-MANY NON-PARALLEL VOICE CONVERSION (under review for ICLR 2020)](https://openreview.net/pdf?id=HJlk-eHFwH)    Attentive evolutionary generative adversarial network
* [Augmented Cyclic Adversarial Learning for Low Resource Domain Adaptation](https://arxiv.org/abs/1807.00374)
* [CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training](https://arxiv.org/abs/1703.10155)
* [Generative Adversarial Source Separation](https://ieeexplore.ieee.org/abstract/document/8461671?casa_token=2tknZ3qtTy8AAAAA:8ntqh1TKUZb0YZFWiMQTKiEiYFYkYpc1HCvrWYYlb649J1vYjQNyBpITohIixKt_WkMierVO8w)
* [leimao/Voice-Converter-CycleGAN](https://github.com/leimao/Voice-Converter-CycleGAN)
* [junyanz.github.io/CycleGAN/](https://junyanz.github.io/CycleGAN/)
* [tifgan.github.io/](https://tifgan.github.io/)
* [towardsdatascience.com/voice-translation-and-audio-style-transfer-with-gans-b63d58f61854](https://towardsdatascience.com/voice-translation-and-audio-style-transfer-with-gans-b63d58f61854)    Importance Weighted Generative Networks
* [Importance Weighted Generative Networks](https://www.ecmlpkdd2019.org/downloads/paper/21.pdf)    Improving Dysarthric Speech Intelligibility using Cycle-consistent Adversarial Training
* [Learning the Base Distribution in Implicit Generative Models](https://arxiv.org/abs/1803.04357)
* [MB-MelGAN](https://arxiv.org/abs/2005.05106)
* [MelGAN](https://arxiv.org/abs/1910.06711)
* [MelGAN: Generative Adversarial Networks forConditional Waveform Synthesis](https://arxiv.org/pdf/1910.06711.pdf)
* [MelGAN: Generative Adversarial Networks forConditional Waveform Synthesis](https://arxiv.org/pdf/1910.06711.pdf)
* [Pronunciation Variation Analysis and CycleGAN-based Feedback Generation for CAPT](https://s-space.snu.ac.kr/bitstream/10371/167962/1/000000160998.pdf)
* [Rhythm-Flexible Voice Conversion Without Parallel Data Using Cycle-GAN Over Phoneme Posteriorgram Sequences - IEEE Conference Publication](https://ieeexplore.ieee.org/abstract/document/8639647?casa_token=rDkrtgeEXM4AAAAA:zPybFeJOu8sPcDzn9icMhLP65Xax_lPhfiXa_gqEzNzs2x2U7bLnewK40oyZIyE_n2j6UQ2yjQ)
* [SEGAN: Speech Enhancement Generative Adversarial Network](https://arxiv.org/abs/1703.09452)    Self-Imitating Feedback Generation Using GAN for Computer-Assisted Pronunciation Training    Self-imitating Feedback Generation Using GAN for Computer-Assisted Pronunciation Training
* [SinGAN: Learning a Generative Model from a Single Natural Image](https://openaccess.thecvf.com/content_ICCV_2019/papers/Shaham_SinGAN_Learning_a_Generative_Model_From_a_Single_Natural_Image_ICCV_2019_paper.pdf)
* [Source Separation with Deep Generative Priors](http://proceedings.mlr.press/v119/jayaram20a.html)
* [StarGAN](https://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf)
* [StarGAN-VC: non-parallel many-to-many Voice Conversion Using Star Generative Adversarial Networks](https://www.semanticscholar.org/paper/dee9c8aab638a909d265e17d84dbbf69dda65470)
* [liusongxiang/StarGAN-Voice-Conversion](https://github.com/liusongxiang/StarGAN-Voice-Conversion)    This paper employs a novel approach to accent transfer in which a ___GAN is trained on pre-converted voice data triplets which are pre-processed to match on pitch and timbre    Voice impersonation using generative adversarial networks
* [WAVEGLOW: A FLOW-BASED GENERATIVE NETWORK FOR SPEECH SYNTHESIS](https://arxiv.org/pdf/1811.00002.pdf)
* [[1910.02519] FIS-GAN: GAN with Flow-based Importance Sampling](https://arxiv.org/abs/1910.02519)
* [[2102.12841] MaskCycleGAN-VC: Learning Non-parallel Voice Conversion with Filling in Frames](https://arxiv.org/abs/2102.12841)
* [A New Way to look at GANs. The Discriminator is more useful than… | by Marco Pasini | Towards Data Science](https://towardsdatascience.com/a-new-way-to-look-at-gans-7c6b6e6e9737)
* [Anjaney1999/image-captioning-seqgan: An image captioning model that is inspired by the Show, Attend and Tell paper (https://arxiv.org/abs/1502.03044) and the Sequence Generative Adversarial Network paper (https://arxiv.org/abs/1609.05473)](https://github.com/Anjaney1999/image-captioning-seqgan)
* [CONDITIONAL GAN FOR TIMESERIES GENERATION](https://arxiv.org/pdf/2006.16477.pdf)
* [cyclegan vc2](https://arxiv.org/pdf/1904.04631.pdf)
* [CycleGAN-VC2](http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/cyclegan-vc2/index.html)
* [GANtastic3/MaskCycleGAN-VC: Implementation of Kaneko et al.'s MaskCycleGAN-VC model for non-parallel voice conversion.](https://github.com/GANtastic3/MaskCycleGAN-VC)
* [Glove GAN](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/reports/6908905.pdf)
* [How to Develop a Conditional GAN (cGAN) From Scratch](https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/)
* [GANtastic3/MaskCycleGAN-VC](https://github.com/GANtastic3/MaskCycleGAN-VC)
* [GANtastic3/MaskCycleGAN-VC](https://github.com/GANtastic3/MaskCycleGAN-VC)
* [hegde95/GAN-for-speech-spectrogram](https://github.com/hegde95/GAN-for-speech-spectrogram)
* [hegde95/GAN-for-speech-spectrogram/blob/master/EE599_report_final.pdf](https://github.com/hegde95/GAN-for-speech-spectrogram/blob/master/EE599_report_final.pdf)
* [jackaduma/CycleGAN-VC2](https://github.com/jackaduma/CycleGAN-VC2)
* [marcoppasini/MelGAN-VC](https://github.com/marcoppasini/MelGAN-VC)
* [marcoppasini/MelGAN-VC/blob/master/MelGAN_VC.ipynb](https://github.com/marcoppasini/MelGAN-VC/blob/master/MelGAN_VC.ipynb)
* [MuradBozik/audio-style-transfer](https://github.com/MuradBozik/audio-style-transfer)
* [seungwonpark/melgan](https://github.com/seungwonpark/melgan)
* [sy2358/accent_conversion_GAN](https://github.com/sy2358/accent_conversion_GAN)
* [sy2358/accent_conversion_GAN](https://github.com/sy2358/accent_conversion_GAN)
* [Yashkataria/CGAN-for-time-series](https://github.com/Yashkataria/CGAN-for-time-series)
* [towardsdatascience.com/voice-translation-and-audio-style-transfer-with-gans-b63d58f61854](https://towardsdatascience.com/voice-translation-and-audio-style-transfer-with-gans-b63d58f61854)
* [towardsdatascience.com/voice-translation-and-audio-style-transfer-with-gans-b63d58f61854](https://towardsdatascience.com/voice-translation-and-audio-style-transfer-with-gans-b63d58f61854)
* [connectedpapers.com/main/498cdaa589a17bf9a28f85005617088f39685fc2/MelGANVC-Voice-Conversion-and-Audio-Style-Transfer-on-arbitrarily-long-samples-using-Spectrograms/graph](https://www.connectedpapers.com/main/498cdaa589a17bf9a28f85005617088f39685fc2/MelGANVC-Voice-Conversion-and-Audio-Style-Transfer-on-arbitrarily-long-samples-using-Spectrograms/graph)
* [semanticscholar.org/paper/Frame-level-speech-enhancement-based-on-Wasserstein-Chuan-Lan/51a32df93c5f316284c5140baf81d53e1cafcbb7](https://www.semanticscholar.org/paper/Frame-level-speech-enhancement-based-on-Wasserstein-Chuan-Lan/51a32df93c5f316284c5140baf81d53e1cafcbb7)
* [jackaduma (Kun Ma) / Repositories](https://github.com/jackaduma?tab=repositories)
* [jackaduma/CycleGAN-VC2: Voice Conversion by CycleGAN (语音克隆/语音转换): CycleGAN-VC2](https://github.com/jackaduma/CycleGAN-VC2)
* [jackaduma/CycleGAN-VC3: Voice Conversion by CycleGAN (语音克隆/语音转换)：CycleGAN-VC3](https://github.com/jackaduma/CycleGAN-VC3)
* [Jeremy Fisher :: Running CycleGAN programmatically](https://www.jeremyafisher.com/running-cyclegan-programmatically.html)
* [jxzhanggg/Voice_Converter_CycleGAN: Voice Converter Using CycleGAN and Non-Parallel Data](https://github.com/jxzhanggg/Voice_Converter_CycleGAN)
* [leimao/Voice-Converter-CycleGAN: Voice Converter Using CycleGAN and Non-Parallel Data](https://github.com/leimao/Voice-Converter-CycleGAN)
* [MaskCycleGAN-VC](http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/maskcyclegan-vc/index.html)
* [MelGAN](https://arxiv.org/abs/1910.06711)
* [MelGAN-VC: Voice Conversion and Audio Style Transfer on arbitrarily long samples using Spectrograms](https://arxiv.org/abs/1910.03713)
* [MelGAN-VC/MelGAN_VC.ipynb at master · marcoppasini/MelGAN-VC · GitHub](https://github.com/marcoppasini/MelGAN-VC/blob/master/MelGAN_VC.ipynb)
* [Pascalson/Conditional-Seq-GANs](https://github.com/Pascalson/Conditional-Seq-GANs)
* [RNN W1L11 : Bidirectional RNN - YouTube](https://www.youtube.com/watch?v=bTXGpATdKRY)
* [Self-imitating Feedback Generation Using GAN for Computer-Assisted Pronunciation Training](https://arxiv.org/pdf/1904.09407.pdf)    SeqGAN
* [spectrogram GAN - Ricerca Google](https://www.google.com/search?q=spectrogram+GAN&client=ubuntu&hs=wFF&channel=fs&sxsrf=AOaemvJ85W-rO1Fbr7SX_xOr2wDybzliTA:1630525577419&tbm=isch&source=iu&ictx=1&fir=EIYJXTpbE3Ax8M%252CrbDE8Oi0zpZOYM%252C_&vet=1&usg=AI4_-kQKvYq6eeplLAEOSGNAICct7fSebQ&sa=X&ved=2ahUKEwjn8NqOxd7yAhVQyqQKHeaRDBgQ9QF6BAgYEAE#imgrc=N9b7DaOCSRQnTM)
* [StarGAN-VC2: Rethinking Conditional Methods for StarGAN-Based Voice Conversion](https://www.semanticscholar.org/paper/3830bb029e2abd6240ecacf74ec95f584aa2c167)
* [suragnair/seqGAN: A simplified PyTorch implementation of "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient." (Yu, Lantao, et al.)](https://github.com/suragnair/seqGAN)
* [Synthetic Time-Series Load Data via Conditional Generative Adversarial Networks](https://arxiv.org/pdf/2107.03545.pdf)
* [TalkToTheGAN/REGAN: ReGAN: Sequence GAN using RE[INFORCE|LAX|BAR] based PG estimators](https://github.com/TalkToTheGAN/REGAN)
* [The Math Behind Generative Adversarial Networks Clearly Explained! - YouTube](https://www.youtube.com/watch?v=Gib_kiXgnvA)
* [Understanding StarGAN - YouTube](https://www.youtube.com/watch?v=8XfcDkkFbMs)
* [WGANSing | Papers With Code](https://paperswithcode.com/paper/wgansing)
* [WGANSing: A Multi-Voice Singing Voice Synthesizer Based on the Wasserstein-GAN | Papers With Code](https://cs.paperswithcode.com/paper/wgansing-a-multi-voice-singing-voice)
* [What The Heck Are VAE-GANs?. Yep, you read the title correctly… | by Enoch Kan | Towards Data Science](https://towardsdatascience.com/what-the-heck-are-vae-gans-17b86023588a)
* [X-czh/SeqGAN-PyTorch: Implementation of Sequence Generative Adversarial Nets with Policy Gradient in PyTorch](https://github.com/X-czh/SeqGAN-PyTorch)
* [yunjey/stargan: StarGAN - Official PyTorch Implementation (CVPR 2018)](https://github.com/yunjey/StarGAN)
* [ZiJianZhao/SeqGAN-PyTorch: A implementation of SeqGAN in PyTorch, following the implementation in tensorflow.](https://github.com/ZiJianZhao/SeqGAN-PyTorch)    A Multi-Resolution Approach to GAN-Based Speech Enhancement - applsci-11-00721-v2.pdf
* [Generative Adversarial Network Definition | DeepAI](https://deepai.org/machine-learning-glossary-and-terms/generative-adversarial-network)
* [Improved Wasserstein conditional generative adversarial network speech enhancement | EURASIP Journal on Wireless Communications and Networking | Full Text](https://jwcn-eurasipjournals.springeropen.com/articles/10.1186/s13638-018-1196-0)    Loss Functions of Generative Adversarial Networks (GANs): Opportunities and Challenges
* [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784)
* [Face editing with GANs](https://www.youtube.com/watch?v=dCKbRCUyop8)
* [GAN Paper](https://42papers.com/p/dino-a-conditional-energy-based-gan-for-domain-translation)
* [Generative Adversarial Networks (GANs) - Computerphile](https://youtu.be/Sw9r8CL98N0)
* [GitHub - deepakbaby/se_relativisticgan: Keras framework for speech enhancement using relativistic GANs](https://github.com/deepakbaby/se_relativisticgan)
* [towardsdatascience.com/voice-translation-and-audio-style-transfer-with-gans-b63d58f61854](https://towardsdatascience.com/voice-translation-and-audio-style-transfer-with-gans-b63d58f61854)
* [HumanACGAN: conditional generative adversarial network with human-based auxiliary classifier and its evaluation in phoneme perception](https://arxiv.org/abs/2102.04051)
* [Multi-objective training of Generative Adversarial Networks with multiple discriminator](https://arxiv.org/abs/1901.08680)
* [Multi-objective training of Generative Adversarial Networks with multiple discriminators](https://arxiv.org/abs/1901.08680)
* [This AI Makes "Audio Deepfakes"](https://youtu.be/VQgYPv8tb6A)

## Sequence

* [FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow](https://arxiv.org/abs/1909.02480)
* [Alibaba Submission for WMT18 Quality Estimation Task](https://www.aclweb.org/anthology/W18-6465.pdf)
* [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271)
* [Convolutional Sequence Modeling Revisited](https://openreview.net/forum?id=rk8wKk-R-)
* [AuCson/PyTorch-Batch-Attention-Seq2seq: PyTorch implementation of batched bi-RNN encoder and attention-decoder.](https://github.com/AuCson/PyTorch-Batch-Attention-Seq2seq)
* [keon/seq2seq: Minimal Seq2Seq model with Attention for Neural Machine Translation in PyTorch](https://github.com/keon/seq2seq)
* [ml-toolkit/rnn.py at master · chrisvdweth/ml-toolkit](https://github.com/chrisvdweth/ml-toolkit/blob/master/pytorch/models/text/classifier/rnn.py)
* [practical-pytorch/seq2seq-translation at master · spro/practical-pytorch](https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation)
* [PyTorch RNN Tutorial - Name Classification Using A Recurrent Neural Net - YouTube](https://www.youtube.com/watch?v=WEV61GmmPrk)
* [Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/)
* [Deep Learning Techniques for Music Generation](http://libgen.rs/book/index.php?md5=C8341B9DE2E9C2B3FCCC1A125ECA4856)
* [sgrvinod/a-PyTorch-Tutorial-to-Sequence-Labeling](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Sequence-Labeling)
* [hhexiy.github.io/nlp/notes/sequence_labeling.html](https://hhexiy.github.io/nlp/notes/sequence_labeling.html)
* [PyTorch Time-Series Prediction](https://www.youtube.com/watch?v=AvKSPZ7oyVg)

## Features and Feature Learning

* [Deep Representation Learning Techniques forAudio Signal Processing](https://mediatum.ub.tum.de/doc/1463108/document.pdf)
* [Unsupervised Pretraining Transfers well Across Languages](https://arxiv.org/abs/2002.02848)
* [Unsupervised Pretraining Transfers well Across Languages](https://arxiv.org/abs/2002.02848)
* [(PDF)
* [Acoustic- and EGG-parametrisations of Phonatory Quality Provide Voice Profiles of Normal Speakers.]](https://www.researchgate.net/publication/266152934_Acoustic-_and_EGG-parametrisations_of_Phonatory_Quality_Provide_Voice_Profiles_of_Normal_Speakers)
* [brianmcfee.net/papers/scipy2015_librosa.pdf](https://brianmcfee.net/papers/scipy2015_librosa.pdf)
* [librosa.org/doc/latest/feature.html](https://librosa.org/doc/latest/feature.html)
* [kaggle.com/ilyamich/mfcc-implementation-and-tutorial](https://www.kaggle.com/ilyamich/mfcc-implementation-and-tutorial)
* [log Filterbank features - Google Suche](https://www.google.com/search?client=firefox-b-d&q=log+Filterbank+features)
* [Speech Processing for Machine Learning: Filter banks, Mel-Frequency Cepstral Coefficients (MFCCs) and What’s In-Between | Haytham Fayek](https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html)

## Representations

* [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)
* -> Representing text in natural language processing - Towards Data Science
* -> Using word embeddings

### **LearnedVector (Michael Nguyen)

* [Sentence embedding - Wikipedia](https://en.wikipedia.org/wiki/Sentence_embedding)
* [Разработка поисковой системы на основе векторных представлений слов30 переглядів](https://vk.com/video-138477641_456241662?list=838e8272a7a210202a)
* [facebookresearch/StarSpace: Learning embeddings for classification, retrieval and ranking.](https://github.com/facebookresearch/StarSpace)
* [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)
* [blog.manash.me/how-to-use-pre-trained-word-vectors-from-facebooks-fasttext-a71e6d55f27](https://blog.manash.me/how-to-use-pre-trained-word-vectors-from-facebooks-fasttext-a71e6d55f27)
* [en.wikipedia.org/wiki/WordNet](https://en.wikipedia.org/wiki/WordNet)
* [facebookresearch/fastText/blob/master/pretrained-vectors.md](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md)
* [facebookresearch/MUSE](https://github.com/facebookresearch/MUSE)
* [GeniSysAI/NLU/blob/master/README.md](https://github.com/GeniSysAI/NLU/blob/master/README.md)
* [Hironsan/awesome-embedding-models](https://github.com/Hironsan/awesome-embedding-models)
* [sites.google.com/site/rmyeid/projects/polyglot](https://sites.google.com/site/rmyeid/projects/polyglot)
* [Анна Потапенко, ВШЭ: «Векторные представления слов и документов» - YouTube](https://www.youtube.com/watch?v=KEXWC-ICH_Y)
* [Google’s ALBERT Is a Leaner BERT; Achieves SOTA on 3 NLP Benchmarks](https://medium.com/syncedreview/googles-albert-is-a-leaner-bert-achieves-sota-on-3-nlp-benchmarks-f64466dd583)
* [Deep Learning, NLP, and Representations - colah's blog](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)

## Phonetics and Praat    What are formants? (=> Praat)

* [uni-frankfurt.de/43157681/Phonetik-und-Phonologie-4_MA](http://www.uni-frankfurt.de/43157681/Phonetik-und-Phonologie-4_MA)
* [2.2. Formants of Vowels – Phonetics and Phonology](https://corpus.eduhk.hk/english_pronunciation/index.php/2-2-formants-of-vowels/)
* [Acoustic Phonetics: Formants](https://home.cc.umanitoba.ca/~krussll/phonetics/acoustic/formants.html)
* [Interpreting vowel articulation from formant frequencies | Welcome to SWPhonetics](https://swphonetics.com/methods/vowel-articulation-from-formants/)
* [Phonetic Transcriptions of IDEA Samples | IDEA: International Dialects of English Archive](https://www.dialectsarchive.com/phonetic-transcriptions)
* [jaekookang/Articulatory-Data-Extractor: Python Procedure for Articulatory (sensor locations) and Acoustic Data (eg. formants) Extraction](https://github.com/jaekookang/Articulatory-Data-Extractor)
* [2 Kaldi | Corpus Phonetics Tutorial](https://eleanorchodroff.com/tutorial/kaldi.html)
* [en.wikipedia.org/wiki/Luciano_Canepari](https://en.wikipedia.org/wiki/Luciano_Canepari)
* [internationalphoneticassociation.org/contact](https://www.internationalphoneticassociation.org/contact)
* [video](https://www.youtube.com/watch?v=nxkQ4vucBWc)
* [Links to Phonetics Resources | International Phonetic Association](https://www.internationalphoneticassociation.org/content/links-phonetics-resources)
* [Phonetik lehren und lernen](https://de1lib.org/book/2978287/a43317)
* [Phonetik/Phonologie](https://youtu.be/jncQS7iRpFk)
* [What do phone embeddings learn about Phonology?](https://www.aclweb.org/anthology/W19-4219.pdf)
* [Quelle-Filter-Modell](https://home.uni-leipzig.de/jtrommer/phonetik07/k5b.pdf)
* [menzerath.phonetik.uni-frankfurt.de/teaching/K31/material3.pps](http://menzerath.phonetik.uni-frankfurt.de/teaching/K31/material3.pps)
* [Phonetische Analysen mit Praat](https://praatpfanne.lingphon.net/downloads/praat_manual.pdf)
* [ipds.uni-kiel.de/Dokumente/ModulA/L06.pdf](http://www.ipds.uni-kiel.de/Dokumente/ModulA/L06.pdf)
* [(PDF) Evaluation of Phonatory Behavior of German and French Speakers in Native and Non-Native Speech](https://www.researchgate.net/publication/305602839_Evaluation_of_Phonatory_Behavior_of_German_and_French_Speakers_in_Native_and_Non-Native_Speech)
* [cdn.intechopen.com/pdfs/15948/InTech-Phoneme_recognition_on_the_timit_database.pdf](https://cdn.intechopen.com/pdfs/15948/InTech-Phoneme_recognition_on_the_timit_database.pdf)
* [Linguistiklabor Albert-Ludwigs-Universität Freiburg](https://www.youtube.com/channel/UCqU1JprLislAb-lV8Fw_FqA)
* [Transcribing speech with PRAAT](https://www.youtube.com/watch?v=YVJbi0WPqPw)
* [Ling 441 - Advanced Phonetics - Making a text grid in Praat](https://www.youtube.com/watch?v=VOFwu5fuK2w)
* [Creating phonemic and syllabic tier in PRAAT from SPPAS and P2FA](https://www.youtube.com/watch?v=TqMh8JmOKrg)
* [Transcribing with the full IPA](https://www.youtube.com/watch?v=GKx7nmOVNKs)
* [PRAAT for Transcription (linguistics research) mini-tutorial](https://www.youtube.com/watch?v=vhkqOcpGKts)
* [timmahrt/praatIO](https://github.com/timmahrt/praatIO)
* [Legisign/Praat-textgrids](https://github.com/Legisign/Praat-textgrids)
* [Legisign/Praat-textgrids](https://github.com/Legisign/Praat-textgrids)
* [praatscripting.lingphon.net/](https://praatscripting.lingphon.net/)
* [pypi.org/project/praat-textgrids/](https://pypi.org/project/praat-textgrids/)

## Audio

* [Mel-Frequency Cepstral Coefficients Explained Easily](https://www.youtube.com/watch?v=4_SH2nfbQZ8)
* [seaandsailor.com/initial_representation.html](http://www.seaandsailor.com/initial_representation.html)
* [home.cc.umanitoba.ca/~robh/howto.html](https://home.cc.umanitoba.ca/~robh/howto.html)
* [home.cc.umanitoba.ca/~krussll/phonetics/acoustic/spectrogram-sounds.html](http://home.cc.umanitoba.ca/~krussll/phonetics/acoustic/spectrogram-sounds.html)
* [australianlinguistics.com/acoustic-analysis-2/spectrograms/](http://australianlinguistics.com/acoustic-analysis-2/spectrograms/)
* [australianlinguistics.com/](http://australianlinguistics.com/)

## Style Transfer

* [Neural Audio Style Transfer - CMPUT 466 - YouTube](https://www.youtube.com/watch?v=jmB4IhfGhuY)
* [AST ](https://egrinstein.github.io/2017/10/25/ast.html)
* [Style Transfer](https://www.google.com/search?q=how+does+style+transfer+keep+the+essence+of+what+it+transforms&rlz=1CDGOYI_enUS835US835&oq=how+does+style+transfer+keep+the+essence+of+what+it+transforms&aqs=chrome..69i57.23098j0j4&hl=it&sourceid=chrome-mobile&ie=UTF-8)
* [Style Transfer](https://www.google.com/search?q=accent+style+transfer&rlz=1CDGOYI_enUS835US835&oq=accent+style+transfer&aqs=chrome..69i57.5681j0j7&hl=it&sourceid=chrome-mobile&ie=UTF-8)
* [TF Style Transfer](https://www.tensorflow.org/tutorials/generative/style_transfer)
* [neural style transfer in Julia - Cerca con Google](https://www.google.com/search?q=neural+style+transfer+in+Julia&ie=UTF-8&oe=UTF-8&hl=it-us&client=safari)
* [style transfer with accents - Cerca con Google](https://www.google.com/search?q=style+transfer+with+accents&ie=UTF-8&oe=UTF-8&hl=it-us&client=safari)
* [how do you hold certain aspects fixed during style transfer? - Cerca con Google](https://www.google.com/search?q=how+do+you+hold+certain+aspects+fixed+during+style+transfer%3F&ie=UTF-8&oe=UTF-8&hl=it-us&client=safari)
* [PacktPublishing/Hands-On-Transfer-Learning-with-TensorFlow-2.0-Video/tree/master](https://github.com/PacktPublishing/Hands-On-Transfer-Learning-with-TensorFlow-2.0-Video/tree/master)
* [Image Style Transfer Using Convolutional Neural Networks](https://rn-unison.github.io/articulos/style_transfer.pdf)
* [MOSAIC STYLE TRANSFER USING SPARSE AUTOCORRELOGRAMS](http://archives.ismir.net/ismir2019/paper/000109.pdf)    Applying visual domain style transfer and texture synthesis techniques to audio: insights and challenges
* [inzva/Audio-Style-Transfer](https://github.com/inzva/Audio-Style-Transfer)    A Neural Algorithm of Artistic Style.    A Style Transfer Approach to Source Separation
* [Accent modification for speech recognition of non-native speakers using neural style transfer](https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00199-3)    Accent modification for speech recognition of non-native speakers using neural style transfer    Audio style transfe (Grinstein et al.)
* [Audio Style Transfer for Accents](https://shuby.de/files/11-785_project.pdf)
* [Audio Style Transfer Slides](http://www2.ece.rochester.edu/~zduan/teaching/ece477/lectures/StudentPresentations/2018/Audio%20Style%20Transfer_final.pdf)
* [Audio style transfer using shallow convolutional networks and random filters](https://link.springer.com/article/10.1007/s11042-020-08798-6)
* [Control Method Of perceptual Elements Based On Universal Neural Style Transfer](https://ieeexplore.ieee.org/abstract/document/8778630/?casa_token=KeiMFufQzfkAAAAA:r8zJhO4IDo1QdXcFRNWE_N3HOHk2C7aWP2GU0u9vrPV7KZYGzISOAswYwaDZnJL5ZgfJd67oJA)    Emotion Transfer Using Vector-Valued Infinite Task Learning    Groove2Groove: One-Shot Music Style Transfer With Supervision From Synthetic Data
* [audiostyletransfer.wordpress.com/](https://audiostyletransfer.wordpress.com/)
* [dmitryulyanov.github.io/audio-texture-synthesis-and-style-transfer/](https://dmitryulyanov.github.io/audio-texture-synthesis-and-style-transfer/)    Mosaic Style Transfer Using Sparse Autocorrelograms    Multi-Reference Neural TTS Stylization with Adversarial Cycle Consistency
* [Neural Style Transfer Based Voice Mimicking for Personalized Audio Stories](https://dl.acm.org/doi/abs/10.1145/3422839.3423063)    Neural Style Transfer for Audio Spectograms
* [Neural style transfer for audio spectrograms](https://arxiv.org/abs/1801.01589)
* [Non-native speech recognition using audio style transfer](http://koral.ise.pw.edu.pl/~rrom/SPIE/SPIE11176-Wilga2019/source/4-biomed%20applic/049-radzikowski.pdf)    Perceptual Losses forReal-Time Style Transfer and Super-Resolution    Self-Supervised VQ-VAE For One-Shot Music Style Transfer
* [Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis](http://proceedings.mlr.press/v80/wang18h/wang18h.pdf)
* [Style Transfer for Audio using Convolutional Neural Networks](https://www.researchgate.net/profile/Swati-Mali/publication/320463069_Style_Transfer_for_Audio_using_Convolutional_Neural_Networks/links/5b5ed7c3a6fdccf0b200ad83/Style-Transfer-for-Audio-using-Convolutional-Neural-Networks.pdf)
* [Style Transfer for Prosodic Speech](http://web.stanford.edu/class/cs224s/project/reports_2017/Anthony_Perez.pdf)    Time Domain Neural Audio Style Transfer    Universal styletransfer via feature transforms
* [Diverse Image-to-Image Translation via Disentangled Representations - YouTube](https://www.youtube.com/watch?v=H5B7Xas3jbU)
* [junyanz/pytorch-CycleGAN-and-pix2pix: Image-to-Image Translation in PyTorch](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)
* [inzva/Audio-Style-Transfer](https://github.com/inzva/Audio-Style-Transfer)
* [Audio Style Transfer (Grinstein et al.)
* [Deep Learning for Audio Style Transfer](http://www2.ece.rochester.edu/~zduan/teaching/ece477/projects/2018/ZhixianHuang_ShaotianChen_BingjingZhu_ReportFinal.pdf)    Audio texture synthesis and style transfer
* [AI Hub - Audio Style Transfer](https://aihub.cloud.google.com/p/products%2F17e98ca1-99c4-45b6-9f93-5e75645f07e5)
* [dmitryulyanov.github.io/audio-texture-synthesis-and-style-transfer/](https://dmitryulyanov.github.io/audio-texture-synthesis-and-style-transfer/)    Time Domain Neural Audio Style Transfer    Audio Texture Synthesis with Random Neural Networks: Improving Diversity and Quality (poster)    Audio Texture Synthesis with Random Neural Networks: Improving Diversity and Quality (poster)
* [Applying Visual Domain Style Transfer and Texture Synthesis Techniques to Audio - Insights and Challenges](https://www.researchgate.net/publication/330726451_Applying_Visual_Domain_Style_Transfer_and_Texture_Synthesis_Techniques_to_Audio_-_Insights_and_Challenges)

## ASR + SS

* [espnet/espnet](https://github.com/espnet/espnet)

## SP

* [juliapackages.com/p/dsp](https://juliapackages.com/p/dsp)
* [seaandsailor.com/audiosp_julia.html](http://www.seaandsailor.com/audiosp_julia.html)
* [juliapackages.com/p/signalanalysis](https://juliapackages.com/p/signalanalysis)
* [3 . Traitement de signal - corrélation / convolution](https://youtu.be/cVLvyLLznsk)
* [PSOLA](https://en.wikipedia.org/wiki/PSOLA)
* [DSPRelated.com - All About Digital Signal Processing](https://www.dsprelated.com/)
* [Mel Spectrograms Explained Easily - YouTube](https://www.youtube.com/watch?v=9GHCiiDLHQ4)
* [Signalverarbeitung in der Informatik - YouTube](https://www.youtube.com/playlist?list=PLNmsVeXQZj7qvLZlbEiRwHoP0mbmAHNId)
* [Spectrogram, Cepstrum and Mel-Frequency : Kishore Prahallad : Free Download, Borrow, and Streaming : Internet Archive](https://archive.org/details/SpectrogramCepstrumAndMel-frequency_636522)
* [Audio Signal Processing for Machine Learning <> 1. Audio Signal Processing for Machine Learning <> Valerio Velardo - The Sound of AI](https://www.youtube.com/c/ValerioVelardoTheSoundofAI)
* [Audio Signal Processing for Machine Learning <> 2. Sound and](https://www.youtube.com/watch?v=bnHHVo3j124&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=2)
* [Audio Signal Processing for Machine Learning <> 3. Intensity, Loudness, a](https://www.youtube.com/watch?v=Jkoysm1fHUw&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=3)
* [Audio Signal Processing for Machine Learning <> 4. Understanding Audio Signals for Machine](https://www.youtube.com/watch?v=daB9naGBVv4&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=4)
* [Audio Signal Processing for Machine Learning <> 5. Types of Audio Features for Machine](https://www.youtube.com/watch?v=ZZ9u1vUtcIA&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=5)
* [Audio Signal Processing for Machine Learning <> 6. How to Extract Audio](https://www.youtube.com/watch?v=8A-W1xk7qs8&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=6)
* [Audio Signal Processing for Machine Learning <> 7. Understanding Time Domain Audio](https://www.youtube.com/watch?v=SRrQ_v-OOSg&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=7)
* [Audio Signal Processing for Machine Learning <> 8. Extracting the amplitude envelope feature from scratch](https://www.youtube.com/watch?v=rlypsap6Wow&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=8)
* [Audio Signal Processing for Machine Learning <> 9. How to Extract Root-Mean Square Energy and Zero-Crossing Rate f](https://www.youtube.com/watch?v=EycaSbIRx-0&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=9)
* [Audio Signal Processing for Machine Learning <> 10. Demystifying the Fourier Transform: The](https://www.youtube.com/watch?v=XQ45IgG6rJ4&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=10)
* [Audio Signal Processing for Machine Learning <> 11. Complex Numbers for Audio Signal P](https://www.youtube.com/watch?v=DgF4m0AWCgA&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=11)
* [Audio Signal Processing for Machine Learning <> 12. Defining the Fourier Transform with Comple](https://www.youtube.com/watch?v=KxRmbtJWUzI&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=12)
* [Audio Signal Processing for Machine Learning <> 13. Discrete Fourier Transform Explain](https://www.youtube.com/watch?v=ZUi_jdOyxIQ&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=13)
* [Audio Signal Processing for Machine Learning <> 14. How to Extract the Fourier Transform wi](https://www.youtube.com/watch?v=R-5uxKTRjzM&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=14)
* [Audio Signal Processing for Machine Learning <> 15. Short-Time Fourier Transform Explain](https://www.youtube.com/watch?v=-Yxj3yfvY-4&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=15)
* [Audio Signal Processing for Machine Learning <> 16. How to Extract Spectrograms from Audio wi](https://www.youtube.com/watch?v=3gzI4Z2OFgY&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=16)
* [Audio Signal Processing for Machine Learning <> 17. Mel Spectrograms Explain](https://www.youtube.com/watch?v=9GHCiiDLHQ4&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=17)
* [Audio Signal Processing for Machine Learning <> 18. Extracting Mel Spectrograms wi](https://www.youtube.com/watch?v=TdnVE5m3o_0&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=18)
* [Audio Signal Processing for Machine Learning <> 19. Mel-Frequency Cepstral Coefficients Explain](https://www.youtube.com/watch?v=4_SH2nfbQZ8&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=19)
* [Audio Signal Processing for Machine Learning <> 20. Extracting Mel-Frequency Cepstral Coefficients wi](https://www.youtube.com/watch?v=WJI-17MNpdE&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=20)
* [Audio Signal Processing for Machine Learning <> 21. Frequency-Domain Audio](https://www.youtube.com/watch?v=3-bjAoAxQ9o&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=21)
* [Audio Signal Processing for Machine Learning <> 22. Implementing Band Energy Ratio in Python fro](https://www.youtube.com/watch?v=8UJ8ZDR7yUs&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=22)
* [Audio Signal Processing for Machine Learning <> 23. Extracting Spectral Centroid and Bandwidth with Python an](https://www.youtube.com/watch?v=j6NTatoi928&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=23)
* [Akustische Kommunikation: Grundlagen mit Hörbeispielen](https://de1lib.org/book/2092369/1411a5)
* [Digital Signal Processing, Fourth Edition](https://de1lib.org/book/2928831/fcf19c)
* [Discrete-Time Signal Processing](http://library.lol/main/F463F6828CE5DA6BFEE3860D96886F39)
* [Grundkurs Spracherkennung : vom Sprachsignal zum Dialog ; Grundlagen und Anwendung verstehen ; mit praktischen Übungen](https://de1lib.org/book/2032592/693b03)
* [libgen.rs/search.php?&req=Signalverarbeitung&phrase=1&view=simple&column=def&sort=year&sortmode=DESC](http://libgen.rs/search.php?&req=Signalverarbeitung&phrase=1&view=simple&column=def&sort=year&sortmode=DESC)
* [auspicious3000/SpeechSplit](https://github.com/auspicious3000/SpeechSplit)
* [CODEJIN/SPEECHSPLIT](https://github.com/CODEJIN/SPEECHSPLIT)
* [video](https://www.youtube.com/watch?v=sIlQ3GcslD8)
* [maelfabien.github.io/blog/](https://maelfabien.github.io/blog/)
* [animations.physics.unsw.edu.au/jw/voice.html](https://www.animations.physics.unsw.edu.au/jw/voice.html)

## Sound    Introduction to Voice Computing in Python

* [Voicebook](https://github.com/jim-schwoebel/voicebook)
* [maelfabien.github.io/machinelearning/Speech8/#](https://maelfabien.github.io/machinelearning/Speech8/#)
* [maelfabien.github.io/machinelearning/Speech9/#](https://maelfabien.github.io/machinelearning/Speech9/#)
* [sound file preprocessing for deep learning](https://www.google.com/search?q=sound+file+preprocessing+for+deep+learning&rlz=1C5CHFA_enUS800US800&oq=sound+file+preprocessing+for+deep+learning&aqs=chrome..69i57.5935j0j7&sourceid=chrome&ie=UTF-8)    Audiodesign: Akustische Kommunikation, akustische Signale und Systeme, psychoakustische Grundlagen, Klangsynthese, Audioediting und Effektbearbeitung, Sounddesign, Bild-Ton-Beziehungen
* [How to apply machine learning and deep learning methods to audio analysis](https://towardsdatascience.com/how-to-apply-machine-learning-and-deep-learning-methods-to-audio-analysis-615e286fcbbc)
* [haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html](https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html)
* [jonathan-hui.medium.com/speech-recognition-phonetics-d761ea1710c0](https://jonathan-hui.medium.com/speech-recognition-phonetics-d761ea1710c0)
* [google.com/search?client=firefox-b-d&q=log+Filterbank+features](https://www.google.com/search?client=firefox-b-d&q=log+Filterbank+features)    Mel Frequency Cepstral Coefficients    Filterbank Energies    Log Filterbank Energies    Spectral Subband Centroids
* [jameslyons/python_speech_features](https://github.com/jameslyons/python_speech_features)
* [i-vector](http://people.csail.mit.edu/sshum/talks/ivector_tutorial_interspeech_27Aug2011.pdf)
* [x-vector](http://danielpovey.com/files/2017_interspeech_embeddings.pdf)
* [Parselmouth - Praat in Python, the Pythonic way](https://github.com/YannickJadoul/Parselmouth)
* [internationalphoneticassociation.org/content/links-phonetics-resources#A2](https://www.internationalphoneticassociation.org/content/links-phonetics-resources#A2)

## Accent

* [duckduckgo.com/?t=ffab&q=accent+transfer+with+cyclegan&ia=web](https://duckduckgo.com/?t=ffab&q=accent+transfer+with+cyclegan&ia=web)
* [shaojinding/Golden-Speaker-Builder](https://github.com/shaojinding/Golden-Speaker-Builder)
* [GSB Demo](https://www.youtube.com/watch?v=pfwGb0VqF-8)
* [goldenspeaker.engl.iastate.edu/speech/](https://goldenspeaker.engl.iastate.edu/speech/)
* [(PDF) Accent Conversion Using Phonetic Posteriorgrams](https://www.researchgate.net/publication/327814834_Accent_Conversion_Using_Phonetic_Posteriorgrams)
* [[1904.04169] Parrotron: An End-to-End Speech-to-Speech Conversion Model and its Applications to Hearing-Impaired Speech and Speech Separation](https://arxiv.org/abs/1904.04169)
* [A New Approach to Accent Recognition and Conversion for Mandarin Chinese](https://arxiv.org/abs/2008.03359)
* [Accent and Speaker Disentanglement in Many-to-many Voice Conversion](https://arxiv.org/pdf/2011.08609.pdf)
* [Accent and Speaker Disentanglement in Many-to-many Voice Conversion](https://arxiv.org/pdf/2011.08609.pdf)
* [Accent Classification and Neural Accent Transfer of English Speech](http://cs230.stanford.edu/files_winter_2018/projects/6939642.pdf)
* [Accent Classification of Non-Native English Speakers](http://web.stanford.edu/class/cs224s/project/reports_2017/Albert_Chu.pdf)
* [Accent Classificationusing Machine Learning](https://www.irjet.net/archives/V7/i11/IRJET-V7I11105.pdf)
* [Accent Conversion Using Artificial Neural Networks](http://web.stanford.edu/class/cs224s/project/reports_2017/Amy_Bearman.pdf)
* [Accent conversion using phonetic posteriorgrams](https://psi.engr.tamu.edu/wp-content/uploads/2018/03/zhao2018icassp.pdf)
* [Accent Conversion Using Phonetic Posteriorgrams - IEEE Conference Publication](https://ieeexplore.ieee.org/abstract/document/8462258?casa_token=IgvSItA156YAAAAA:8tC5g1lZx9uW7g30zCUSbOMnNj7MtmZ_Wfo2kx_sO30xz062M1KGJAtv05P0CQEXnxE0Y6ME6w)
* [Accent neutralization for speech recognition of non-native speakers](https://dl.acm.org/doi/pdf/10.1145/3366030.3366083)
* [Accent neutralization for speech recognition of non-native speakers | Proceedings of the 21st International Conference on Information Integration and Web-based Applications & Services](https://dl.acm.org/doi/abs/10.1145/3366030.3366083)    An investigation of accent conversion for non-native and native varieties of English
* [Articulatory-based conversion of foreign accents with deep neural networks](https://isca-speech.org/archive/interspeech_2015/papers/i15_3385.pdf)
* [Boosting Universal Speech Attributes Classification with Deep Neural Networkfor Foreign Accent Characterization](http://cs.joensuu.fi/~villeh/DeepAtributes.pdf)
* [Converting Foreign Accent Speech Without a Reference](https://guanlongzhao.github.io/demo/reference-free-ac/)
* [Discriminatively trained acoustic model for improving mispronunciation detection and  diagnosis in computer-aided pronunciation training (CAP](http://www.se.cuhk.edu.hk/hccl/publications/pub/xiaojun_interspeech2010.pdf)
* [End-to-end accent conversion without using native utterances](https://ieeexplore.ieee.org/document/9053797)
* [End-To-End Accent Conversion Without Using Native Utterances](https://liusongxiang.github.io/end2endAC/)
* [liusongxiang.github.io/end2endAC/](https://liusongxiang.github.io/end2endAC/)    End-to-end accent conversion without using native utterances
* [English Language Accent Classification and Conversion using Machine Learning](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3600748)
* [guanlongzhao/fac-via-ppg](https://github.com/guanlongzhao/fac-via-ppg)
* [Foreign Accent Conversion by Synthesizing Speech from Phonetic Posteriorgrams - f148e4aad6827b4abe2922e16e6b62dc0abb.pdf](https://pdfs.semanticscholar.org/e4b4/f148e4aad6827b4abe2922e16e6b62dc0abb.pdf)
* [Foreign Accent Conversion with Neural Acoustic Modeling](https://oaktrust.library.tamu.edu/handle/1969.1/192349)
* [guanlongzhao/fac-via-ppg: Foreign Accent Conversion by Synthesizing Speech from Phonetic Posteriorgrams (Interspeech'19)](https://github.com/guanlongzhao/fac-via-ppg)
* [guanlongzhao.github.io/](https://guanlongzhao.github.io/)
* [End-to-End Accent Conversion Demo](https://liusongxiang.github.io/end2endAC/)
* [Improving Accent Conversion with Reference Encoder and End-To-End Text-To-Speech](https://arxiv.org/abs/2005.09271)
* [Demo](https://kal009l.github.io/ac-demo/)
* [Non native speech recognition using audio style transfer](http://koral.ise.pw.edu.pl/~rrom/SPIE/SPIE11176-Wilga2019/source/4-biomed%20applic/049-radzikowski.pdf)
* [Parrotron: An End-to-End Speech-to-Speech Conversion Model and its Applications to Hearing-Impaired Speech and Speech Separation](https://arxiv.org/abs/1904.04169)
* [Phonetic Feedback for Speech Enhancement with and Without Parallel Speech Data](https://ieeexplore.ieee.org/abstract/document/9054001?casa_token=aLFgXS4X2U8AAAAA:1yBlI8kFocYN7pY4L8YnlNWY2zV7-RU1EGfELAjovqX2VAbgn-T3I0RMhKdB4k9XmM_XibA4Rw)
* [OSU-slatelab/mimic-enhance](https://github.com/OSU-slatelab/mimic-enhance)
* [SAR-Net: A End-to-End Deep Speech Accent Recognition Network | Papers With Code](https://paperswithcode.com/paper/sar-net-a-end-to-end-deep-speech-accent)
* [Sci-Hub | End-To-End Accent Conversion Without Using Native Utterances. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) | 10.1109/ICASSP40776.2020.9053797](https://sci-hub.se/10.1109/ICASSP40776.2020.9053797)
* [Solving the Problem of the Accents for Speech Recognition Systems](http://www.ijsps.com/uploadfile/2016/0628/20160628103620514.pdf)
* [SPOKEN ENGLISH INTELLIGIBILITY REMEDIATION WITH POCKETSPHINXALIGNMENT AND FEATURE EXTRACTION IMPROVES SUBSTANTIALLY OVER THESTATE OF THE ART](https://raw.githubusercontent.com/jsalsman/featex/master/Spoken-English-Intelligibility-Remediation.pdf)    Using phonetic posteriorgram based frame pairing for segmental accent conversion
* [Using Phonetic Posteriorgram Based Frame Pairingfor Segmental Accent Conversion](https://psi.engr.tamu.edu/wp-content/uploads/2020/04/zhao2019taslp.pdf)
* [guanlongzhao/ppg-gmm](https://github.com/guanlongzhao/ppg-gmm)
* [zhao2018icassp.pdf](https://psi.engr.tamu.edu/wp-content/uploads/2018/03/zhao2018icassp.pdf)
* REDAT: Accent-Invariant Representation for End-to-End ASR by Domain Adversarial Training with Relabeling
* A New Approach to Accent Recognition and Conversion for Mandarin Chinese - 42Papers
* GitHub - lynneeai/Mandarin_Accent_Conversion
* [Non-native speech recognition sentences: A new materials set for non-native speech perception research](https://link.springer.com/article/10.3758/s13428-019-01251-z)
* [duckduckgo.com/?t=ffab&q=accent+transfer+with+cyclegan&ia=web](https://duckduckgo.com/?t=ffab&q=accent+transfer+with+cyclegan&ia=web)
* [speech accent GAN](https://www.reddit.com/r/MachineLearning/comments/awmjm8/d_how_long_are_we_from_voice_style_transfer_voice/)
* One-Shot Voice Conversion with Disentangled Representations by Leveraging Phonetic Posteriorgrams - 1798.pdf
* Voice Conversion Chapter
* [Deep Learning for Audio Style Transfer](http://www2.ece.rochester.edu/~zduan/teaching/ece477/projects/2018/ZhixianHuang_ShaotianChen_BingjingZhu_ReportFinal.pdf)
* [Deep Voice Conversion](https://github.com/andabi/deep-voice-conversion)    Are there already accent filter aplications?
* [>](https://www.google.com/search?q=technology+behind+accent+changer&rlz=1C5CHFA_enUS800US800&oq=technology+behind+accent+changer&aqs=chrome..69i57.5067j0j9&sourceid=chrome&ie=UTF-8)
* [>](https://www.google.com/search?q=accent+filter+for+voice&rlz=1C5CHFA_enUS800US800&sxsrf=ALeKk00hKsjqtzJrulZPqG0s7LDe689dlg:1606128335737&source=lnms&tbm=vid&sa=X&ved=2ahUKEwiT5qinvpjtAhU3A2MBHUTlBlYQ_AUoA3oECAMQBQ&biw=1092&bih=460)
* You can now speak using someone else’s voice with Deep Learning
* [Accent Classification and Neural Transfer of English Speech](http://cs230.stanford.edu/files_winter_2018/projects/6939642.pdf)
* [Voice Accent Transfer Using Recurrent Neural Networks on Spectrograms](https://cs230.stanford.edu/projects_fall_2018/reports/12449240.pdf)
* [Phonetic Posteriorgrams based Many-to-Many Singing Voice Conversion via Adversarial Training](https://arxiv.org/abs/2012.01837)
* [Accent conversion – PSI Lab](https://psi.engr.tamu.edu/portfolio/accent-conversion/)
* [sy2358/accent_conversion_GAN: Spectrogram2Spectrogram Translation and Speech Conversion](https://github.com/sy2358/accent_conversion_GAN)
* [bhaprayan/unbabel](https://github.com/bhaprayan/unbabel)

## ASR

* [assemblyai.com/blog/conformer-1](https://www.assemblyai.com/blog/conformer-1)
* [A Deep Learning Algorithm for Objective Assessment of Hypernasality in Children with Cleft Palate](https://www.researchgate.net/publication/344373574_A_Deep_Learning_Algorithm_for_Objective_Assessment_of_Hypernasality_in_Children_with_Cleft_Palate)    file:///home/isaac/Projects/Thesis/papers/Introduction%20to%20Automatic%20Speech%20Recognition%20(ASR)%20-.html#statistical-historical-approach-to-asr
* [maelfabien.github.io/machinelearning/speech_reco/](https://maelfabien.github.io/machinelearning/speech_reco/)
* [maelfabien.github.io/machinelearning/speech_reco/#2-hmm-dnn-acoustic-model](https://maelfabien.github.io/machinelearning/speech_reco/#2-hmm-dnn-acoustic-model)
* [uhh-lt/kaldi-tuda-de](https://github.com/uhh-lt/kaldi-tuda-de)    A TRANSFORMER WITH INTERLEAVED SELF-ATTENTION AND CONVOLUTION FORHYBRID ACOUSTIC MODELS
* [Probabilistic Linear Discriminant Analysisfor Acoustic Modelling](https://home.ttic.edu/~llu/pdf/llu_spl14.pdf)
* [noopkat/acoustic-model-machine](https://github.com/noopkat/acoustic-model-machine)
* [syhw/wer_are_we](https://github.com/syhw/wer_are_we)
* [r/speechrecognition/](https://www.reddit.com/r/speechrecognition/)
* [r/speechtech/](https://www.reddit.com/r/speechtech/)
* [home.ttic.edu/~llu/](https://home.ttic.edu/~llu/)
* [by2101/OpenASR](https://github.com/by2101/OpenASR)
* [en.wikipedia.org/wiki/Speech_recognition#Hidden_Markov_models](https://en.wikipedia.org/wiki/Speech_recognition#Hidden_Markov_models)
* [raminnakhli/GMM-HMM-from-scratch](https://github.com/raminnakhli/GMM-HMM-from-scratch)
* [pypi.org/project/hmms/](https://pypi.org/project/hmms/)
* [TreB1eN/HiddenMarkovModel_Pytorch/blob/master/HiddenMarkovModel.py](https://github.com/TreB1eN/HiddenMarkovModel_Pytorch/blob/master/HiddenMarkovModel.py)
* [iitg.ac.in/samudravijaya/tutorialSlides/gmmHmmTutoChiefWiSSAP09.pdf](http://iitg.ac.in/samudravijaya/tutorialSlides/gmmHmmTutoChiefWiSSAP09.pdf)
* [A pitch extraction algorithm tuned for automatic speech recognition](https://www.researchgate.net/publication/312462439_A_pitch_extraction_algorithm_tuned_for_automatic_speech_recognition)    Front-EndFactor Analysis for Speaker Verification
* [AppleHolic/PytorchSR](https://github.com/AppleHolic/PytorchSR)
* [idiap/pkwrap](https://github.com/idiap/pkwrap)
* [at16k/at16k](https://github.com/at16k/at16k)
* [kaldi-asr.org/models.html](http://kaldi-asr.org/models.html)
* [cmusphinx.github.io/wiki/tutorial/](https://cmusphinx.github.io/wiki/tutorial/)
* [cmusphinx.github.io/wiki/tutorialam/](https://cmusphinx.github.io/wiki/tutorialam/)
* [sourceforge.net/projects/cmusphinx/files/](https://sourceforge.net/projects/cmusphinx/files/)
* [cmusphinx/](https://github.com/cmusphinx/)
* [cmusphinx.github.io/wiki/tutorialadapt/](https://cmusphinx.github.io/wiki/tutorialadapt/)
* [cmusphinx.github.io/wiki/download/](https://cmusphinx.github.io/wiki/download/)
* [kaldi-asr.org/doc/model.html](https://kaldi-asr.org/doc/model.html)
* [jrmeyer.github.io/asr/2016/12/15/DNN-AM-Kaldi.html](http://jrmeyer.github.io/asr/2016/12/15/DNN-AM-Kaldi.html)
* [CoEDL/elpis/](https://github.com/CoEDL/elpis/)
* [Speech Recognition — Phonetics. Finding the core principle and focus is… | by Jonathan Hui | Medium](https://jonathan-hui.medium.com/speech-recognition-phonetics-d761ea1710c0)
* [Dan Povey](https://www.danielpovey.com/kaldi-lectures.html)
* [artemvang/lstm_gan: Implementation of LSTM GAN for twitter posts generating.](https://github.com/artemvang/lstm_gan)
* [Understanding Normalizing Flows and Its Use Case in Speech Synthesis (Part 2) | by Sanjay G | Subex AI Labs | Medium](https://medium.com/subex-ai-labs/understanding-normalizing-flows-and-its-use-case-in-speech-synthesis-part-2-3e19840e80b5)
* [Understanding Normalizing Flows and Its Use Case in Speech Synthesis (Part 1) | by Sanjay G | Subex AI Labs | Medium](https://medium.com/subex-ai-labs/understanding-normalizing-flows-and-its-use-case-in-speech-synthesis-part-1-5f805c2d43ce)
* [20200324_AGIST_WaveGlow_윤상휴.pdf - Google Drive](https://drive.google.com/file/d/1VhN7Y223UVOZ_uhFNuXDtLE0THk54TnN/view)
* [Sanskrit TTS using Tacotron2, WaveGlow and Transfer Learning - YouTube](https://www.youtube.com/watch?v=ENonNFyXM-g)
* [Tacotron 2 - THE BEST TEXT TO SPEECH AI YET! - YouTube](https://www.youtube.com/watch?v=le1LH4nPfmE)
* [Taras Sereda "Waveglow. Generative modeling for audio synthesis" - YouTube](https://www.youtube.com/watch?v=B3WlTVvdI5M)
* [WaveGlow: A flow-based generative network to synthesize speech](https://techxplore.com/news/2018-11-waveglow-flow-based-network-speech.html)
* [generative-models - Collection of generative models, e.g. GAN, VAE in Pytorch and Tensorflow.](https://www.findbestopensource.com/product/wiseodd-generative-models)
* [scjs/buckeye](https://github.com/scjs/buckeye)
* [pytorch.org/hub/nvidia_deeplearningexamples_waveglow/](https://pytorch.org/hub/nvidia_deeplearningexamples_waveglow/)
* [alphacephei.com/vosk/](https://alphacephei.com/vosk/)
* [nawarhalabi/Arabic-Phonetiser/blob/master/phonetise-Arabic.py](https://github.com/nawarhalabi/Arabic-Phonetiser/blob/master/phonetise-Arabic.py)
* [nawarhalabi/segmentationevaluation](https://github.com/nawarhalabi/segmentationevaluation)
* [kth.se/is/tmh/speech-communication/wikispeech2-a-speech-corpus-collector-for-a-more-accessible-wikipedia-through-wikispeech-1.931945](https://www.kth.se/is/tmh/speech-communication/wikispeech2-a-speech-corpus-collector-for-a-more-accessible-wikipedia-through-wikispeech-1.931945)    SpeechDat  *  Speech  Databases  for  Creation  of  Voice  Driven Teleservices    Automatic Speech Attribute Transcription (ASAT) - The Front End Processor
* [Feature maps of the acoustic spectrum of the voice](https://www.sciencedirect.com/science/article/pii/S0892199718301851)
* [GitHub - lowerquality/gentle: gentle forced aligner](https://github.com/lowerquality/gentle)
* [duckduckgo.com/?t=ffab&q=gentle+guide+to+kaldi&ia=software](https://duckduckgo.com/?t=ffab&q=gentle+guide+to+kaldi&ia=software)
* [TMH Publications (latest 50) | KTH](https://www.kth.se/is/tmh/publications)
* [02: Taxonomie von Spracherkennungssystemen, Schwierigkeiten von ASR, Stimmhafte & stimmlose Sprache - YouTube](https://www.youtube.com/watch?v=-umSC2t52Rw&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=2)
* [03: Sprachlaute, Ort und Art der Artikulation, Schalldruckpegel und -energie, Hörfläche - YouTube](https://www.youtube.com/watch?v=QltjEroSLrw&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=3)
* [Campo aleatorio condicional - Wikipedia, la enciclopedia libre](https://es.wikipedia.org/wiki/Campo_aleatorio_condicional)
* [Graphical model - Wikipedia](https://en.wikipedia.org/wiki/Graphical_model)
* [Alignment techniques — Montreal Forced Aligner 2.0.0a15 documentation](https://montreal-forced-aligner.readthedocs.io/en/latest/alignment_techniques.html)
* [Data preparation — Montreal Forced Aligner 2.0.0a15 documentation](https://montreal-forced-aligner.readthedocs.io/en/latest/data_prep.html)
* [Kaldi: gmmbin/gmm-align-compiled.cc File Reference](http://kaldi-asr.org/doc/gmm-align-compiled_8cc.html)
* [Kaldi: gmmbin/gmm-boost-silence.cc File Reference](http://kaldi-asr.org/doc/gmm-boost-silence_8cc.html)
* [Montreal-Forced-Aligner/alignment.py at 027417db978d8ac9074087007119bb9d28db7eba · MontrealCorpusTools/Montreal-Forced-Aligner](https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner/blob/027417db978d8ac9074087007119bb9d28db7eba/montreal_forced_aligner/multiprocessing/alignment.py#L265)
* [Index of /static/model/recognition/allosaurus](https://www.pyspeech.com/static/model/recognition/allosaurus/)
* [dmort27/panphon: Python package and data files for manipulating phonological segments (phones, phonemes) in terms of universal phonological features.](https://github.com/dmort27/panphon)
* [Contextual predictability and phonetic attention](https://www.sciencedirect.com/science/article/abs/pii/S0095447018301517)
* [Phoneme Boundary Detection using Deep Bidirectional LSTMs](https://www.semanticscholar.org/paper/Phoneme-Boundary-Detection-using-Deep-Bidirectional-Franke-M%C3%BCller/090e8ff713e0420741526de0a5bc01fe74b8be32)    ACCURATE SPEECH SEGMENTATION BY MIMICKING HUMAN AUDITORY PROCESSING
* [Simultaneous Detection and Segmentation](https://link.springer.com/content/pdf/10.1007/978-3-319-10584-0_20.pdf)
* [Simultaneous Detection and Segmentation](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/shape/sds/)
* [Advanced Neural Networks - Slides](https://www.ee.columbia.edu/~stanchen/spring16/e6870/slides/lecture14.pdf)
* [Columbia ASR Course](https://www.ee.columbia.edu/~stanchen/spring16/e6870/slides/)
* [Introduction to Neural Topic Models](https://zll17.github.io/2020/11/17/Introduction-to-Neural-Topic-Models/)
* [Using Forced Alignment for Phonetics Research](https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/springer2018-forced-alignment-phonetics.pdf)
* [xinjli/allosaurus](https://github.com/xinjli/allosaurus)
* [Speech Recognition Using Articulatory and Excitation Source Features | springerprofessional.de](https://www.springerprofessional.de/speech-recognition-using-articulatory-and-excitation-source-feat/11992526)
* [docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html)    SLP 26. Automatic Speech Recognition and Text-to-Speech    Automatic Speech Recognition. A Deep Learning Approach    Robust Adaptation to Non-Native Accents in ASR
* [01: Automatische Spracherkennung: Anwendungen, Vorteile und Nachteile, Taxonomie vo](https://www.youtube.com/watch?v=9_CDtVhqX7Y&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=1)
* [02: Taxonomie von Spracherkennungssystemen, Schwierigkeiten von ASR, Stimmhafte & stimmlos](https://www.youtube.com/watch?v=-umSC2t52Rw&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=2)
* [03: Sprachlaute, Ort und Art der Artikulation, Schalldruckpegel und -energie,](https://www.youtube.com/watch?v=QltjEroSLrw&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=3)
* [04: Schriftsysteme, Mikrofone, Wandlerprinzipien, Akustische Bauformen, Geschicht](https://www.youtube.com/watch?v=yfDCUXYlW0g&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=4)
* [05: Wortfehlrate, Grundlagen der Signalver](https://www.youtube.com/watch?v=4x6RESjrVyc&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=5)
* [06: Fourierreihe, Fouriertransformation, Quantisierung, Kurzzeitspektr](https://www.youtube.com/watch?v=88uZziyqfDg&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=6)
* [07: Mel Skalierung, Helmholtzresonator, Vokaldreieck, Lineare Vorhersage (LPC), All-Po](https://www.youtube.com/watch?v=XCz4W4YuzVw&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=7)
* [08: Typische Vorverarbeitung, Klassifikation, Überwachtes vs. Unüberwacht](https://www.youtube.com/watch?v=47IXltTRLnQ&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=8)
* [09: Spracherkennung mit Musterklassifikation, Dynamic Time Warping (DTW), One](https://www.youtube.com/watch?v=XCEzbpTlrkM&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=9)
* [10: KNN, LVQ, stochastische](https://www.youtube.com/watch?v=5bpcWDcL-pw&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=10)
* [11: Hidden Markov Model Ansatz: Definition, Beobachtungsgenerierung, Trellis,](https://www.youtube.com/watch?v=skD44ueka00&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=11)
* [12: Expectation Maximiza](https://www.youtube.com/watch?v=leu7l6b2ELs&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=12)
* [13: HMMs in ASR, HMM Trainingszyklus, Etikettierte Daten, Neutral Gas Al](https://www.youtube.com/watch?v=bMi1i_OcKz8&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=13)
* [14: Kontextabhängige Modellierun](https://www.youtube.com/watch?v=HvrjOW7PITY&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=14)
* [15: Kontextabhängige AM, Aussprachewör](https://www.youtube.com/watch?v=nLs5S-ehr04&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=15)
* [16: Multiworte, Spra](https://www.youtube.com/watch?v=2FgunvEy55I&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=16)
* [17: Back-Off Sprachmodelle, Katz Backoff, Kneser-Ney Backoff, Besondere Arten von Sprac](https://www.youtube.com/watch?v=pgQqN-CuklA&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=17)
* [18: CFGs als Sprachmodel](https://www.youtube.com/watch?v=vypUJErBpgE&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=18)
* [19: Sprachmodelle in der Suche, Vite](https://www.youtube.com/watch?v=mZmLD3K1O2M&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=19)
* [20: Systemkombination, Diskriminatives](https://www.youtube.com/watch?v=hCLcC6ldcrc&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=20)
* [21: Diskriminatives Training,](https://www.youtube.com/watch?v=lOdVuWknvks&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=21)
* [22: Entropie einer Wortquelle, Perplexität,](https://www.youtube.com/watch?v=5uLy4yVaBSk&list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T&index=22)
* [kaldi-asr.org/](http://kaldi-asr.org/)
* [eleanorchodroff.com/tutorial/kaldi/introduction.html](https://eleanorchodroff.com/tutorial/kaldi/introduction.html)    neural networks for dialect recognition
* [English ASR Challenge](https://github.com/Speech-Lab-IITM/English_ASR_Challenge)
* [Uberi speech recognition](https://github.com/Uberi/speech_recognition)

###  GitHub - gooofy/zamia-speech: Open tools and data for cloudless automatic speech recognition

* [medium.com/descript/the-state-of-automatic-speech-recognition-q-a-with-kaldis-dan-povey-c860aada9b85](https://medium.com/descript/the-state-of-automatic-speech-recognition-q-a-with-kaldis-dan-povey-c860aada9b85)
* [PwC STT](https://paperswithcode.com/task/speech-to-text-translation)
* [CS224](http://web.stanford.edu/class/cs224s/syllabus/)
* [Speech Lab IITM](https://github.com/Speech-Lab-IITM)
* [gooofy/zamia-speech](https://github.com/gooofy/zamia-speech)
* [HF ASR](https://huggingface.co/facebook/wav2vec2-base-960h)
* [A Basic Introduction to Speech Recognition (Hidden Markov Model & Neural...](https://youtu.be/U0XtE4_QLXI)
* [Speech and Audio Processing 1: Introduction to Speech Processing](https://youtu.be/Xjzm7S__kBU)
* [analyticsvidhya.com/blog/2019/07/learn-build-first-speech-to-text-model-python/](https://www.analyticsvidhya.com/blog/2019/07/learn-build-first-speech-to-text-model-python/)    Accented Indian English ASR    Accoustic Modeling for Development of Accented Indian English ASR
* [Multi-Accent Deep Neural Network Acoustic Model with Accent-Specific Top Layer Using the KLD-Regularized Model Adaptation](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/IS140113.pdf)
* [Wave2vec](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)
* [A FULLY CONVOLUTIONAL NEURAL NETWORK FOR SPEECH ENHANCEMENT](https://arxiv.org/pdf/1609.07132v1.pdf)
* [WAV2VEC: UNSUPERVISEDPRE-TRAINING  FORSPEECHRECOGNITION](https://arxiv.org/pdf/1904.05862.pdf)
* [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/pdf/2006.11477.pdf)
* [Introducing phonetic information to speaker embedding for speaker verification](https://asmp-eurasipjournals.springeropen.com/track/pdf/10.1186/s13636-019-0166-8.pdf)
* [video](https://www.youtube.com/results?search_query=edinburgh+asr+course)
* [I Built a Personal Speech Recognition System for my AI Assistant](https://www.youtube.com/watch?v=YereI6Gn3bM)
* [assemblyai.com/blog/end-to-end-speech-recognition-pytorch](https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch)
* [kaggle.com/solomonk/pytorch-speech-recognition-challenge-wip](https://www.kaggle.com/solomonk/pytorch-speech-recognition-challenge-wip)
* [mravanelli/pytorch_MLP_for_ASR](https://github.com/mravanelli/pytorch_MLP_for_ASR)
* [huggingface.co/blog/fine-tune-wav2vec2-english](https://huggingface.co/blog/fine-tune-wav2vec2-english)
* [duckduckgo.com/?t=canonical&q=phoneme+error+rate&ia=web](https://duckduckgo.com/?t=canonical&q=phoneme+error+rate&ia=web)    Personalized, Cross-lingual TTS Using Phonetic Posteriorgrams
* [stats.stackexchange.com/questions/183438/what-are-the-senones-in-a-deep-neural-network](https://stats.stackexchange.com/questions/183438/what-are-the-senones-in-a-deep-neural-network)
* [MichaelFeng87/CGN_speech_recognition](https://github.com/MichaelFeng87/CGN_speech_recognition)
* [kaldi-asr.org/doc/transform.html#transform_lda](https://kaldi-asr.org/doc/transform.html#transform_lda)    Convolutional Neural Networks for Speech Recognition
* [Aipnet: Generative Adversarial Pre-Training of Accent-Invariant Networks for End-To-End Speech Recognition](https://ieeexplore.ieee.org/abstract/document/9053098)
* [Aipnet: Generative Adversarial Pre-Training of Accent-Invariant Networks for End-To-End Speech Recognition - IEEE Conference Publication](https://ieeexplore.ieee.org/abstract/document/9053098?casa_token=HGKDc2uPLXcAAAAA:9kwcCBW-PsdY9hz1qkbDcwGCL0izbiFLQeKnjQOJNhx7shP1xNEiBEWFGpV3624GQr4oFvK3_g)
* [Automatic Speech Recognition-enabled Pronunciation Learning System Development for Second Language Speakers of Korean](https://www.semanticscholar.org/paper/dd00bc31bb5bebb09d9c524298cea1effbec53dd)    Combining time- and frequency-domain convolution in convolutional neural network-based phone recognition
* [Deep Neural Network acoustic models for ASR](https://tspace.library.utoronto.ca/bitstream/1807/44123/1/Mohamed_Abdel-rahman_201406_PhD_thesis.pdf)
* [Deep Neural Network acoustic models for ASR](https://tspace.library.utoronto.ca/bitstream/1807/44123/1/Mohamed_Abdel-rahman_201406_PhD_thesis.pdf)    Deep Speech 2: End-to-End Speech Recognitionin English and Mandarin
* [Domain Adaptation Using Class Similarity for Robust Speech Recognition | Papers With Code](https://paperswithcode.com/paper/domain-adaptation-using-class-similarity-for)    Dualsupervised learning for non-native speech recognition
* [End-to-end Accented Speech Recognition](https://pdfs.semanticscholar.org/974c/3a3ed4a18990fd7f77edea511b22de16811c.pdf)
* [End-to-End Accented Speech Recognition](https://www.semanticscholar.org/paper/End-to-End-Accented-Speech-Recognition-Viglino-Motl%C3%ADcek/974c3a3ed4a18990fd7f77edea511b22de16811c)
* [End-to-End Accented Speech Recognition - 3a3ed4a18990fd7f77edea511b22de16811c.pdf](https://pdfs.semanticscholar.org/974c/3a3ed4a18990fd7f77edea511b22de16811c.pdf)
* [GitHub - CoEDL/elpis: 🙊 WIP software for creating speech recognition models.](https://github.com/CoEDL/elpis/)
* [GitHub - pzelasko/wav2letter: Facebook AI Research Automatic Speech Recognition Toolkit](https://github.com/pzelasko/wav2letter)    Improved accented speech recognition using accent embeddings and multi-task learning    Improving speech recognition using data augmentation and acoustic model fusion    Lexical modeling of non-native speech for automatic speech recognition    Multilingual Speech Recognition with a Single End-to-End Model    Recognizing Non-native Speech: Characterizing and Adapt-ing to Non-native Usage    Speech recognition with deep recurrentneural networks    * [State of the Art in Continuous Speech Recognition | Voice Communication Between Humans and Machines | The National Academies Press](https://www.nap.edu/read/2308/chapter/14)
* [Unsupervised Discovery of Non-native Phonetic Patterns in L2 English Speech for Mispronunciation Detection and Diagnosis.](https://www.researchgate.net/profile/Xixin_Wu/publication/327388973_Unsupervised_Discovery_of_Non-native_Phonetic_Patterns_in_L2_English_Speech_for_Mispronunciation_Detection_and_Diagnosis/links/5d2edb9d458515c11c36d1cc/Unsupervised-Discovery-of-Non-native-Phonetic-Patterns-in-L2-English-Speech-for-Mispronunciation-Detection-and-Diagnosis.pdf)
* [jrmeyer.github.io/](http://jrmeyer.github.io/)
* [A Basic Introduction to Speech Recognition](https://youtu.be/U0XtE4_QLXI)    asr for sound to phoneme mapping
* [CBMM Workshop on "Speech Representation, Perception and Recognition"](https://youtube.com/playlist?list=PLyGKBDfnk-iD9ZJcxO_4DIylbG2_6N13o)    CUNI Neural ASR with Phoneme-Level Intermediate Step for Non-Native SLT at IWSLT 2020 (Drive)
* [Decoupling Pronunciation and Language for End-to-end Code-switching Automatic Speech Recognition](https://arxiv.org/abs/2010.14798)
* [ESPnet: End-to-End Speech Processing Toolkit](https://arxiv.org/abs/1804.00015)
* [GitHub - hirofumi0810/neural_sp: End-to-end ASR/LM implementation with PyTorch](https://github.com/hirofumi0810/neural_sp)
* [arxiv.org/abs/2002.11800](https://arxiv.org/abs/2002.11800)
* [ei.uni-paderborn.de/fileadmin/elektrotechnik/fg/nth/Forschung/Publications/HaebUmbach_Publist.pdf](https://ei.uni-paderborn.de/fileadmin/elektrotechnik/fg/nth/Forschung/Publications/HaebUmbach_Publist.pdf)
* [paperswithcode.com/paper/parallel-wavenet-conditioned-on-vae-latent](https://paperswithcode.com/paper/parallel-wavenet-conditioned-on-vae-latent)
* [scholar.google.de/citations?user=C-cln60AAAAJ&hl=de](https://scholar.google.de/citations?user=C-cln60AAAAJ&hl=de)
* [csl.uni-bremen.de/GlobalPhone/](https://www.csl.uni-bremen.de/GlobalPhone/)
* [isca-speech.org/archive/SLTU_2018/pdfs/Hardik1.pdf](https://www.isca-speech.org/archive/SLTU_2018/pdfs/Hardik1.pdf)    Language recognition via i-vectors and dimensionality reduction
* [SAR-Net: A End-to-End Deep Speech Accent Recognition Network](https://arxiv.org/abs/2011.12461)
* [Speech Recognition using Python](https://youtu.be/K_WbsFrPUCk)
* [Stanford Seminar - Deep Learning in Speech Recognition](https://youtu.be/RBgfLvAOrss)
* [STAT: Speech Transcription Analysis Tool](https://www.aclweb.org/anthology/N09-5003.pdf)
* [Train Wav2Vec2 on TIMIT](https://cur.at/wxmHagz?m=email&sid=NRVm9Oa)
* [Train Wav2Vec2 on TIMIT](https://cur.at/GwujgTJ?m=email&sid=NRVm9Oa)
* [Train XLSR-Wav2Vec2 on Common Voice](https://cur.at/a2CcG5O?m=email&sid=NRVm9Oa)
* [Train XLSR-Wav2Vec2 on Common Voice](https://cur.at/za3hrMs?m=email&sid=NRVm9Oa)
* [Transfer Learning for ASR to Deal with Low-Resource Data ...](https://iust-deep-learning.github.io/972/static_files/project_reports/asr.pdf)
* [78 Best Speech Recognition Books of All Time - BookAuthority](https://bookauthority.org/books/best-speech-recognition-books)
* [Kaldi ASR](http://kaldi-asr.org/models/m13)
* [kaldi/egs/librispeech/s5 at master · kaldi-asr/kaldi](https://github.com/kaldi-asr/kaldi/tree/master/egs/librispeech/s5)
* [Automatic speech recognition using Kaldi - oplatek_thesis013.pdf](https://raw.githubusercontent.com/oplatek/kaldi-thesis/master/text/tags/oplatek_thesis013.pdf)
* [11-785, Spring 2020, Homework 1 Part 2 | Kaggle](https://www.kaggle.com/c/11-785-s20-hw1p2/overview)
* [11-785, Spring 2020, Homework 1 Part 2 | Kaggle](https://www.kaggle.com/c/11-785-s20-hw1p2/data)
* [ASR.pdf](http://pli1988.github.io/papers/ASR.pdf)
* [Developments and directions in speech recognition and understanding, Part 1 [DSP Education]](https://ieeexplore.ieee.org/document/4815544)
* [cse.iitb.ac.in/~pjyothi/cs753/slides/lecture8.pdf](https://www.cse.iitb.ac.in/~pjyothi/cs753/slides/lecture8.pdf)
* [jaquesgrobler.github.io/online-sklearn-build/modules/hmm.html](http://jaquesgrobler.github.io/online-sklearn-build/modules/hmm.html)
* [Forced alignment using FAVE and DARLA](https://www.youtube.com/watch?v=xqeZgfUVL5A)
* [cse.iitb.ac.in/~pjyothi/cs753/](https://www.cse.iitb.ac.in/~pjyothi/cs753/)
* [desh2608.github.io/2019-05-21-chain/](https://desh2608.github.io/2019-05-21-chain/)
* [dtjchen/spoken-command-processor](https://github.com/dtjchen/spoken-command-processor)
* [lrouviere.github.io/TUTO_ML/correction/lda.html](https://lrouviere.github.io/TUTO_ML/correction/lda.html)
* [Kaldi Tutorial](https://eleanorchodroff.com/tutorial/kaldi/introduction.html)
* [LEARNING ACOUSTIC FRAME LABELING FOR SPEECH RECOGNITION WITH RECURRENT NEURAL NETWORKS - ctc.pdf](https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenTerm3201415/ctc.pdf)
* [Papers With Code : Search for acoustic segmentation | Papers With Code](https://paperswithcode.com/search?q_meta=&q_type=&q=acoustic+segmentation)
* [Contrastive Learning of General-Purpose Audio Representations | Papers With Code](https://paperswithcode.com/paper/contrastive-learning-of-general-purpose-audio)
* [Wav2Letter: an End-to-End ConvNet-based Speech Recognition System | Papers With Code](https://paperswithcode.com/paper/wav2letter-an-end-to-end-convnet-based-speech)
* [Sound Event Detection and Time-Frequency Segmentation from Weakly Labelled Data | Papers With Code](https://cs.paperswithcode.com/paper/sound-event-detection-and-time-frequency)
* [Guide to Speech Recognition with Python | Hacker News](https://news.ycombinator.com/item?id=16671610)
* [DeepSpeech Model — DeepSpeech 0.9.3 documentation](https://deepspeech.readthedocs.io/en/latest/DeepSpeech.html)
* [kastnerkyle/ez-phones: Wrapper to pocketsphinx phoneme labeling tools](https://github.com/kastnerkyle/ez-phones)
* [IPA vowel chart with audio - Wikipedia](https://en.wikipedia.org/wiki/IPA_vowel_chart_with_audio)
* [An introduction to libROSA for working with audio](https://iq.opengenus.org/introduction-to-librosa/)
* [Some Commonly Used Speech Feature Extraction Algorithms | IntechOpen](https://www.intechopen.com/books/from-natural-to-artificial-intelligence-algorithms-and-applications/some-commonly-used-speech-feature-extraction-algorithms)
* [Multi-Task Learning as Multi-Objective Optimization | Papers With Code](https://paperswithcode.com/paper/multi-task-learning-as-multi-objective)
* [Learning audio sequence representations for acoustic event classification - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0957417421004486)
* [An analysis of the influence of deep neural network (DNN) topology in bottleneck feature based language recognition](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0182580)
* [Deep neural network-based bottleneck feature and denoising autoencoder-based dereverberation for distant-talking speaker identification | EURASIP Journal on Audio, Speech, and Music Processing | Full Text](https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-015-0056-7)
* [(PDF) The language-independent bottleneck features](https://www.researchgate.net/publication/261421300_The_language-independent_bottleneck_features)    Automatic Pronunciation Feedback via Articulatory Feature Recognition
* [Attention-Based Models for Speech Recognition](https://arxiv.org/abs/1506.07503)
* [Purely sequence-trained neural networks for ASR based on lattice-free MMI](https://www.danielpovey.com/files/2016_interspeech_mmi.pdf)
* [Cross-lingual Automatic Speech Recognition using Tandem Features](https://era.ed.ac.uk/bitstream/handle/1842/5773/Lal2011.pdf)
* [Multilingual Phone Recognition: Comparison of Traditional versus Common Multilingual Phone-Set Approaches and Applications in Code-Switching](https://link.springer.com/chapter/10.1007/978-981-15-4828-4_7)
* [Phone recognition on TIMIT database](https://www.researchgate.net/publication/221912979_Phone_recognition_on_TIMIT_database)
* [Universal Phone Recognition with a Multilingual Allophone System](https://arxiv.org/abs/2002.11800)    phoneme boundary detection    IMPROVING DEEP NEURAL NETWORK ACOUSTIC MODELS USING GENERALIZED MAXOUT NETWORKS (drive)
* [Google Research on End-to-End Models for Speech Recognition](https://youtu.be/LTOu9_IWMyQ)
* [A Basic Introduction to Speech Recognition](https://youtu.be/U0XtE4_QLXI)
* [Speech Recognition 8 - Pocketsphinx Use Your Own Models Or Files](https://youtu.be/pl1w_p8dXuY)
* [13. Speech Recognition with Convolutional Neural Networks in Keras/TensorFlow](https://youtu.be/Qf4YJcHXtcY)
* [Hidden Markov Model Clearly Explained! Part - 5](https://youtu.be/RWkHJnFj5rY)
* [Training CMU Sphinx Speech Recognition](https://youtu.be/IAHH6-t9jK0)
* [Lecture 9 - Speech Recognition (ASR)](https://youtu.be/HyUtT_z-cms)    frame classification in acoustic modeling    Acoustic modeling in Automatic Speech Recognition – A Survey (Waris & Aggarwal) (Drive)
* [Automatic speech recognition: a survey](https://link.springer.com/article/10.1007/s11042-020-10073-7)
* [Semi-Supervised Training for Improving Data Efficiency in End-to-End Speech Synthesis](https://arxiv.org/abs/1808.10128)
* [The 2020 ESPnet update: new features, broadened applications, performance improvements, and future plans](https://arxiv.org/abs/2012.13006)
* [Articulatory-WaveNet: Autoregressive Model For Acoustic-to-Articulatory Inversion](https://arxiv.org/abs/2006.12594)
* [How To Process & Extract Features From Sound Signals | Deep Learning | Data Science | Introduction - YouTube](https://www.youtube.com/watch?v=eJcM-tGPslg)
* [Getting to Know the Mel Spectrogram](https://towardsdatascience.com/getting-to-know-the-mel-spectrogram-31bca3e2d9d0)
* [Think DSP. Цифровая обработка сигналов на Python](http://libgen.rs/book/index.php?md5=B02A39546D466343825F71649A7D792F)
* [[2004.11284] Unsupervised Speech Decomposition via Triple Information Bottleneck](https://arxiv.org/abs/2004.11284)    change pitch of voice sample without changing phonetic content    csound open source
* [Deep Learning for Audio Signal Processing](https://arxiv.org/pdf/1905.00078.pdf?)
* [GitHub - ryokamoi/ppg_vc: Implementation of voice conversion system utilizing phonetic posteriorgrams (status: archive)](https://github.com/ryokamoi/ppg_vc)
* [How to change speed of audio samples without changing pitch?](https://dsp.stackexchange.com/questions/45794/how-to-change-speed-of-audio-samples-without-changing-pitch)
* [en.wikipedia.org/wiki/Mel-frequency_cepstrum](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)
* [RichardLitt/low-resource-languages](https://github.com/RichardLitt/low-resource-languages)
* [it.wikipedia.org/wiki/Spettrogramma](https://it.wikipedia.org/wiki/Spettrogramma)
* [scholar.google.de/scholar?q=Najim+Dehak+i-vectors&hl=en&as_sdt=0&as_vis=1&oi=scholart](https://scholar.google.de/scholar?q=Najim+Dehak+i-vectors&hl=en&as_sdt=0&as_vis=1&oi=scholart)
* [kaggle.com/ilyamich/mfcc-implementation-and-tutorial](https://www.kaggle.com/ilyamich/mfcc-implementation-and-tutorial)    i-vector speech    LEARNING AUDIO REPRESENTATIONS WITH SELF-SUPERVISION    Oppenheim Schäfer
* [Part 2 - Extracting Audio Features | Kaggle](https://www.kaggle.com/ejlok1/part-2-extracting-audio-features)    Phase    phonetic posteriorgram
* [Speech and Audio Processing 1: Introduction to Speech Processing](https://youtu.be/Xjzm7S__kBU)
* [SpeechSplit Demo](https://auspicious3000.github.io/SpeechSplit-Demo/)    Techniques for pitch shifting vocals while maintaining a natural sound
* [Unsupervised Audio Spectrogram Compression using Vector Quantized Autoencoders](https://www.diva-portal.org/smash/get/diva2:1376201/FULLTEXT01.pdf)    UNSUPERVISED LEARNING OF SEMANTIC AUDIO REPRESENTATIONS
* [intechopen.com/books/from-natural-to-artificial-intelligence-algorithms-and-applications/some-commonly-used-speech-feature-extraction-algorithms](https://www.intechopen.com/books/from-natural-to-artificial-intelligence-algorithms-and-applications/some-commonly-used-speech-feature-extraction-algorithms)
* [Excitation Features of Speech for Emotion Recognition Using Neutral Speech as Reference](https://link.springer.com/article/10.1007/s00034-020-01377-y)
* [Fourier Transformation for Pedestrians [2 ed.]](http://libgen.li/item/index.php?md5=668C3D3063726A1FC089C3902FB92376)
* [DFT - Diskrete Fourier-Transformation: Elementare Einfuhrung [1 ed.]](http://libgen.li/item/index.php?md5=1F38EA012B540D21A2425ED5A760FB7A)
* [Laplace-, Fourier- und z-Transformation: Grundlagen und Anwendungen [10 ed.]](http://libgen.li/item/index.php?md5=98864E63A074B4BD0DA994F78D0D16EA)
* [Fourier-Transformation zur Signal- und Systembeschreibung: Kompakt, visuell, intuitiv verständlich [1. Aufl.]](http://libgen.li/item/index.php?md5=14442ABFE1EC9774F8C2279EF7D4FB91)
* [Mathematik verstehen und anwenden - von den Grundlagen bis zu Fourier-Reihen und Laplace-Transformation](http://libgen.li/item/index.php?md5=7146782AE227286331CFAEC11BC8D9AA)
* [Ma cos'è una trasformata di Fourier? Un'introduzione visuale](https://youtu.be/spUNpyF58BY)
* [Signalverarbeitung #2 - Signale](https://youtu.be/S-dp5N7j62Q)
* [differencebetween.net/technology/difference-between-narrowband-and-wideband/](http://www.differencebetween.net/technology/difference-between-narrowband-and-wideband/)
* [Wide vs Narrow](http://www.cas.usf.edu/~frisch/SPA3011_L07.html)
* [The National Center for Voice and Speech - Tutorials](http://www.ncvs.org/ncvs/tutorials/voiceprod/tutorial/spectral.html)
* [How to read a spectrogram - Rob Hagiwara](https://home.cc.umanitoba.ca/~robh/howto.html)
* [Signals and images : advances and results in speech, estimation, compression, recognition, filtering, and processing](http://libgen.rs/book/index.php?md5=F0D99055CB6AF3959B36812B42F9D687)
* [(2) Traitement de Signal - YouTube](https://www.youtube.com/playlist?list=PLwL_MKgwKjiLqm47oKIxOg57pwghZJ5kn)
* [(2) TRAITEMENT NUMERIQUE DU SIGNAL - YouTube](https://www.youtube.com/playlist?list=PLIDlTAsMIE3fiNP10xd6Pcs9xxHHaI0tp)
* [timbretron.pdf](https://nips2018creativity.github.io/doc/timbretron.pdf)    Expression in Speech: Analysis and Synthesis    Dynamic Speech Models
* [Sebastian STÜKER | Karlsruhe Institute of Technology, Karlsruhe | KIT | Institute of Anthropomatics](https://www.researchgate.net/profile/Sebastian-Stueker)
* [voxforge.org/home/dev](http://www.voxforge.org/home/dev)
* [voxforge.org/home/downloads](http://www.voxforge.org/home/downloads)
* [1ytic/open_stt_e2e: PyTorch end-to-end speech recognition](https://github.com/1ytic/open_stt_e2e)
* [maelfabien.github.io/machinelearning/speech_reco/](https://maelfabien.github.io/machinelearning/speech_reco/)

## VC    * [One-Shot Voice Conversion with Disentangled Representations by Leveraging Phonetic Posteriorgrams - 1798.pdf](https://www.isca-speech.org/archive/Interspeech_2019/pdfs/1798.pdf)

* [rf5.github.io/sacair2020/](https://rf5.github.io/sacair2020/)
* [Convolutional Variational Autoencoders for Spectrogram Compression in Automatic Speech Recognition](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7988421/)
* [Library Genesis: Serov V - Fourier series, Fourier transform and their applications to mathematical physics](http://libgen.rs/book/index.php?md5=DC959302A900C96A3DECE5E86B1551F2)    WaveNet Vocoder with Limited Training Data for Voice Conversion
* [cjerry1243/TransferLearning-CLVC](https://github.com/cjerry1243/TransferLearning-CLVC)
* [ryokamoi/ppg_vc](https://github.com/ryokamoi/ppg_vc)
* [jxzhanggg/nonparaSeq2seqVC_code](https://github.com/jxzhanggg/nonparaSeq2seqVC_code)
* [andabi/deep-voice-conversion](https://github.com/andabi/deep-voice-conversion)
* [hhguo/EA-SVC](https://github.com/hhguo/EA-SVC)
* [shaojinding/Adversarial-Many-to-Many-VC](https://github.com/shaojinding/Adversarial-Many-to-Many-VC)
* [hhguo/EA-SVC](https://github.com/hhguo/EA-SVC)
* [sysu16340234/-PHONETIC-POSTERIORGRAMS-FOR-MANY-TO-ONE-VOICE-CONVERSION-WITHOUT-PARALLEL-DATA-TRAINING-](https://github.com/sysu16340234/-PHONETIC-POSTERIORGRAMS-FOR-MANY-TO-ONE-VOICE-CONVERSION-WITHOUT-PARALLEL-DATA-TRAINING-)
* [Taco-VC: A Single Speaker Tacotron based VoiceConversion with Limited Data](https://arxiv.org/abs/1904.03522)
* [[1803.00860] Can we steal your vocal identity from the Internet?: Initial investigation of cloning Obama's voice using GAN, WaveNet and low-quality found data](https://arxiv.org/abs/1803.00860)
* [[1804.00047] Conditional End-to-End Audio Transforms](https://arxiv.org/abs/1804.00047)
* [[2008.12527] Voice Conversion Challenge 2020: Intra-lingual semi-parallel and cross-lingual voice conversion](https://arxiv.org/abs/2008.12527)
* [An Overview of Voice Conversion and its Challenges. From Statistical Modeling  to Deep Learning](https://arxiv.org/pdf/2008.03648.pdf)
* [Any-to-Many Voice Conversion with Location-Relative Sequence-to-Sequence Modeling](https://arxiv.org/pdf/2009.02725.pdf)
* [Attention-Based Speaker Embeddings for One-Shot Voice Conversion](http://www.interspeech2020.org/uploadfile/pdf/Mon-2-7-8.pdf)
* [Attention-Based Speaker Embeddings for One-Shot Voice Conversion - Mon-2-7-8.pdf](http://www.interspeech2020.org/uploadfile/pdf/Mon-2-7-8.pdf)
* [AutoVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss](http://proceedings.mlr.press/v97/qian19c.html)
* [AUTOVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss](https://arxiv.org/pdf/1905.05879.pdf)
* [Blow: a single-scale hyperconditioned flow fornon-parallel raw-audio voice conversion](https://arxiv.org/pdf/1906.00794.pdf)
* [Conditional End-to-End Audio Transforms](https://arxiv.org/abs/1804.00047)
* [Conditional end-to-end audio transforms](https://arxiv.org/abs/1804.00047)
* [crank: An Open-Source Software for Nonparallel Voice Conversion Based on Vector-Quantized Variational Autoencoder](https://arxiv.org/pdf/2103.02858)
* [Explicit Prosodic Modelling and Deep Speaker Embedding Learning for Non-standard Voice Conversion](https://arxiv.org/pdf/2011.01678.pdf)
* [F0-consistent many-to-many non-parallel voice conversion via conditional autoencoder](https://arxiv.org/pdf/2004.07370)
* [FROM LOUIS ARMSTRONG’S SAXOPHONE TO ELLA FITZGERALD’SVOICE - MUSICAL TIMBRE TRANSFER](https://mac.kaist.ac.kr/~juhan/gct634/Finals/From_Louis_Armstrongs_Saxophone_to_Ella_Fitzgeralds_Voice_Musical_Timbre_Transfer.pdf)
* [Hierarchical Timbre-Painting and Articulation Generation](https://arxiv.org/abs/2008.13095)
* [High-Quality Nonparallel Voice Conversion Based on Cycle-Consistent Adversarial Network - IEEE Conference Publication](https://ieeexplore.ieee.org/abstract/document/8462342?casa_token=cxW4BVBYHnsAAAAA:34uzZY7QBLSEDlKlpvjYFpGuqC6QJc-4NO2rlRZ4umjcpPAcDMgoZt7-ldvFL3zv9xkx2XGL6w)
* [andabi/deep-voice-conversion](https://github.com/andabi/deep-voice-conversion)
* [mosheman5/timbre_painting](https://github.com/mosheman5/timbre_painting)
* [niral28/generative-vc](https://github.com/niral28/generative-vc)
* [Investigating Deep Neural Structures and their Interpretability in the Domain of Voice Conversion](https://arxiv.org/abs/2102.11420)    Investigation of using disentangled and interpretable representationsfor one-shot cross-lingual voice conversion    * [jxzhanggg/nonparaSeq2seqVC_code: Implementation code of non-parallel sequence-to-sequence VC](https://github.com/jxzhanggg/nonparaSeq2seqVC_code)
* [jxzhanggg/Seq2SeqVC: Sequence-to-sequence Acoustic Modelling for Voice Conversion](https://github.com/jxzhanggg/Seq2SeqVC)
* [Learning to Speak Fluently in a Foreign Language:Multilingual Speech Synthesis and Cross-Language Voice Cloning](https://arxiv.org/pdf/1907.04448.pdf)
* [Mon-2-7-8 Attention-Based Speaker Embeddings for One-Shot Voice Conversion - Voice Conversion and Adaptation I - INTERSPEECH 2020](http://www.interspeech2020.org/index.php?m=content&c=index&a=show&catid=269&id=474)
* [Neural Voice Cloning with a Few Samples](https://arxiv.org/pdf/1802.06006.pdf)
* [Non-Parallel Sequence-to-Sequence Voice Conversion with Disentangled Linguistic and Speaker Representations](https://arxiv.org/abs/1906.10508)
* [Non-Parallel Sequence-to-Sequence Voice Conversion with Disentangled Linguistic and Speaker Representations](https://arxiv.org/abs/1906.10508)
* [Nonparallel Emotional Speech Conversion Using VAE-GAN](http://www.interspeech2020.org/uploadfile/pdf/Wed-3-10-4.pdf)
* [One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization](https://arxiv.org/abs/1904.05742)
* [One-shot Voice Conversion with Disentangled Representations by Leveraging Phonetic Posteriorgrams](https://www.isca-speech.org/archive/Interspeech_2019/pdfs/1798.pdf)
* [Papers With Code : Search for disentangled voice | Papers With Cod](https://paperswithcode.com/search?q_meta=&q=disentangled+voice)
* [Parallel-Data-Free Voice Conversion Using Cycle-Consistent Adversarial Networks](https://www.semanticscholar.org/paper/8c59e737d25775d5d3e23fdadc523329955f06c3)    Parallel-Data-Free Voice Conversion Using Cycle-Consistent Adversarial Networks    * [PHONETIC FEEDBACK FOR SPEECH ENHANCEMENTWITH AND WITHOUT PARALLEL SPEECH DATA](https://arxiv.org/pdf/2003.01769.pdf)
* [Phonetic Posteriorgrams based Many-to-Many Singing Voice Conversion via Adversarial Training](https://arxiv.org/pdf/2012.01837.pdf)
* [Phonetic posteriorgrams for many-to-one voice conversion without parallel data training](https://ieeexplore.ieee.org/document/7552917)
* [Pretraining Techniques for Sequence-to-Sequence Voice Conversion](https://www.semanticscholar.org/paper/3503d8b1d6e93569fcf5aa7d2c31192477f82261)
* [Recognition-Synthesis Based Non-Parallel Voice Conversion with Adversarial Learning](http://www.interspeech2020.org/uploadfile/pdf/Mon-2-7-1.pdf)
* [Sequence-to-Sequence Acoustic Modeling for Voice Conversion](https://arxiv.org/abs/1810.06865)    Spectral Mapping Using Artificial Neural Networks for Voice Conversion    Statistical voice conversion with wavenet-based waveform generation    * [Submission from SRCB for Voice Conversion Challenge 2020](https://www.isca-speech.org/archive/VCC_BC_2020/pdfs/VCC2020_paper_41.pdf)
* [The Sequence-to-Sequence Baseline for the Voice Conversion Challenge 2020: Cascading ASR and TTS](https://www.semanticscholar.org/paper/bed0452305633791340f80cb0be02f46e4a34b0d)    Timbretron: Awavenet (cyclegan (cqt (audio))) pipeline for musical timbre transfer
* [Towards Natural and Controllable Cross-Lingual Voice Conversion Based on Neural TTS Model and Phonetic Posteriorgram](https://arxiv.org/abs/2102.01991)
* [Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion](https://arxiv.org/pdf/1905.11563)
* [Voice Accent Transfer Using Recurrent Neural Networks on Spectrograms](https://cs230.stanford.edu/projects_fall_2018/reports/12449240.pdf)    Voice conversion using Artificial Neural Networks
* [Voice conversion using deep Bidirectional Long Short-Term Memory based Recurrent Neural Networks](https://ieeexplore.ieee.org/document/7178896)
* [Voice Conversion using Generative Techniques](http://cs230.stanford.edu/projects_fall_2020/reports/55721255.pdf)    Voice conversion with conditional sampler nn
* [Voice conversion with conditional SampleRNN](https://arxiv.org/abs/1808.08311)
* [Voice Disentangling](https://arxiv.org/pdf/2007.15064.pdf)
* [Voice Transformer Network: Sequence-to-Sequence Voice Conversion UsingTransformer with Text-to-Speech Pretraining](http://www.interspeech2020.org/uploadfile/pdf/Thu-3-4-1.pdf)
* [VQVC+: One-Shot Voice Conversion by Vector Quantization and U-Net architecture](https://arxiv.org/abs/2006.04154)
* [[INTERSPEECH 2020] Voice Conversion Using Speech-to-Speech Neuro-Style Transfer - YouTube](https://www.youtube.com/watch?v=zbVQwqx-kYk)
* [SANE2019 | Hirokazu Kameoka - Voice conversion with image-to-image translation and seq2seq learning - YouTube](https://www.youtube.com/watch?v=2A8XByosfnw&t=1083s)
* [Voice Translation and Audio Style Transfer with GANs | by Marco Pasini | Towards Data Science](https://towardsdatascience.com/voice-translation-and-audio-style-transfer-with-gans-b63d58f61854)

## SS

* [heartbeat.fritz.ai/a-2019-guide-to-speech-synthesis-with-deep-learning-630afcafb9dd](https://heartbeat.fritz.ai/a-2019-guide-to-speech-synthesis-with-deep-learning-630afcafb9dd)
* [syncedreview.com/2020/05/19/facebooks-highly-efficient-new-real-time-text-to-speech-system-runs-on-cpus/](https://syncedreview.com/2020/05/19/facebooks-highly-efficient-new-real-time-text-to-speech-system-runs-on-cpus/)
* [ai.facebook.com/blog/a-highly-efficient-real-time-text-to-speech-system-deployed-on-cpus/](https://ai.facebook.com/blog/a-highly-efficient-real-time-text-to-speech-system-deployed-on-cpus/)
* [Texture Synthesis Using Convolutional Neural Networks](https://proceedings.neurips.cc/paper/2015/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf)
* [Empirical Evaluation of Deep Learning Model CompressionTechniques on the WaveNet Vocoder](https://arxiv.org/pdf/2011.10469.pdf)
* [paperswithcode.com/paper/empirical-evaluation-of-deep-learning-model#code](https://paperswithcode.com/paper/empirical-evaluation-of-deep-learning-model#code)
* [Expediting TTS Synthesis with Adversarial Vocoding](https://arxiv.org/abs/1904.07944)
* [paarthneekhara/advoc](https://github.com/paarthneekhara/advoc)
* [Modern Speech Synthesis and Phonetic Sciences: A Discussion and an Evaluation](https://www.diva-portal.org/smash/get/diva2:1356115/FULLTEXT01.pdf)
* [Sound texture synthesis using convolutional neural networks](https://arxiv.org/abs/1905.03637)
* [Waveglow: A flow-based generative network for speech synthesis](https://arxiv.org/abs/1811.00002v1)
* [paperswithcode.com/paper/waveglow-a-flow-based-generative-network-for#code](https://paperswithcode.com/paper/waveglow-a-flow-based-generative-network-for#code)
* [WaveGrad: Estimating Gradients for Wave Form Generation](https://arxiv.org/pdf/2009.00713.pdf)
* [ivanvovk/WaveGrad](https://github.com/ivanvovk/WaveGrad)
* [TTS Skins: Speaker Conversion via ASR](https://indico2.conference4me.psnc.pl/event/35/contributions/2887/attachments/538/564/Mon-2-7-4.pdf)
* [Wave-Tacotron: Spectrogram-free end-to-end text-to-speech synthesis](https://arxiv.org/abs/2011.03568)
* [Continuous vocoder applied in deep neural network based voice conversion](https://link.springer.com/article/10.1007/s11042-019-08198-5)
* [WaveFlow: A Compact Flow-based Model for Raw Audio](https://arxiv.org/pdf/1912.01219.pdf)
* [paperswithcode.com/paper/waveflow-a-compact-flow-based-model-for-raw-1#code](https://paperswithcode.com/paper/waveflow-a-compact-flow-based-model-for-raw-1#code)
* [Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram](https://arxiv.org/abs/1910.11480)
* [paperswithcode.com/paper/parallel-wavegan-a-fast-waveform-generation#code](https://paperswithcode.com/paper/parallel-wavegan-a-fast-waveform-generation#code)
* [AdaSpeech: Adaptive TTS for Custom Voice](https://arxiv.org/abs/2103.00993)
* [paperswithcode.com/paper/adaspeech-adaptive-text-to-speech-for-custom-1#code](https://paperswithcode.com/paper/adaspeech-adaptive-text-to-speech-for-custom-1#code)
* [Neural Speech Synthesis with Transformer Network](https://arxiv.org/abs/1809.08895)
* [paperswithcode.com/paper/neural-speech-synthesis-with-transformer#code](https://paperswithcode.com/paper/neural-speech-synthesis-with-transformer#code)
* [Deep Voice: Real-time Neural Text-to-Speech](https://arxiv.org/abs/1702.07825)
* [paperswithcode.com/paper/deep-voice-real-time-neural-text-to-speech#code](https://paperswithcode.com/paper/deep-voice-real-time-neural-text-to-speech#code)
* [AlignTTS: Efficient Feed-Forward Text-to-Speech System without Explicit Alignment](https://arxiv.org/abs/2003.01950)
* [Cross-lingual Multi-speaker Text-to-speech Synthesis for Voice Cloning without Using Parallel Corpus for Unseen Speakers](https://arxiv.org/abs/1911.11601)
* [Diff-TTS: A Denoising Diffusion Model for Text-to-Speech](https://arxiv.org/abs/2104.01409)
* [Espnet-TTS: Unified, Reproducible, and Integratable Open Source End-to-End Text-to-Speech Toolkit](https://www.semanticscholar.org/paper/f1520d88a760ef6f013fceb5d4c8b1b9321c3d8c)
* [Flavored Tacotron: Conditional Learning for Prosodic-linguistic Features](https://arxiv.org/abs/2104.04050)
* [Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis](https://arxiv.org/abs/2005.05957)
* [paperswithcode.com/paper/flowtron-an-autoregressive-flow-based](https://paperswithcode.com/paper/flowtron-an-autoregressive-flow-based)
* [Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search](https://arxiv.org/abs/2005.11129)
* [One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech](https://arxiv.org/abs/2008.00768v1)
* [Tomiinek/Multilingual_Text_to_Speech](https://github.com/Tomiinek/Multilingual_Text_to_Speech)
* [FastSpeech: Fast, Robust and Controllable Text to Speech](https://arxiv.org/abs/1905.09263)
* [xcmyz/FastSpeech](https://github.com/xcmyz/FastSpeech)
* [End-to-End Adversarial Text-to-Speech](https://arxiv.org/abs/2006.03575)
* [yanggeng1995/EATS](https://github.com/yanggeng1995/EATS)
* [Speech waveform synthesis from MFCC sequences with generative adversarial networks (ResGAN))
* [paperswithcode.com/paper/speech-waveform-synthesis-from-mfcc-sequences](https://paperswithcode.com/paper/speech-waveform-synthesis-from-mfcc-sequences)
* [Improved parallel WaveGAN vocoder with perceptually weighted spectrogram loss](https://arxiv.org/abs/2101.07412)
* [Learning to Speak Fluently in a Foreign Language: Multilingual Speech Synthesis and Cross-Language Voice Cloning](https://arxiv.org/abs/1907.04448v2)
* [paperswithcode.com/paper/learning-to-speak-fluently-in-a-foreign](https://paperswithcode.com/paper/learning-to-speak-fluently-in-a-foreign)
* [MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis](https://arxiv.org/abs/1910.06711)
* [Multi-band MelGAN: Faster Waveform Generation for High-Quality Text-to-Speech](https://arxiv.org/abs/2005.05106)
* [Non-Autoregressive Neural Text-to-Speech](https://arxiv.org/abs/1905.08459)
* [FastSpeech 2: Fast and High-Quality End-to-End Text to Speech](https://arxiv.org/abs/2006.04558)
* [ming024/FastSpeech2](https://github.com/ming024/FastSpeech2)    WORLD: A Vocoder-Based High-Quality Speech Synthesis System for Real-Time Applications <> WORLD
* [Towards Natural and Controllable Cross-Lingual Voice Conversion Based on Neural TTS Model and Phonetic Posteriorgra](https://arxiv.org/abs/2102.01991)
* [Improving Accent Conversion with Reference Encoder and End-To-End Text-To-Speech](https://arxiv.org/abs/2005.09271)
* [Fine-grained style modelling and transfer in text-to-speech synthesis via content-style disentanglement](https://arxiv.org/abs/2011.03943)
* [Wavenet: A generative model for raw audio](https://arxiv.org/abs/1609.03499)
* [paperswithcode.com/paper/wavenet-a-generative-model-for-raw-audio#code](https://paperswithcode.com/paper/wavenet-a-generative-model-for-raw-audio#code)
* [Signal estimation from modified short-time fourier transform](http://hil.t.u-tokyo.ac.jp/~kameoka/SAP/papers/Griffin1984__Signal_Estimation_from_Modified_Short-Time_Fourier_Transform.pdf)
* [itinerarium.github.io/phoneme-synthesis/](https://itinerarium.github.io/phoneme-synthesis/)
* [Speech Synthesis Markup Language (SSML) Version 1.1](https://www.w3.org/TR/speech-synthesis11/)
* [SpeechBrain: A PyTorch Speech Toolkit](https://speechbrain.github.io/)
* [HTS](http://hts.sp.nitech.ac.jp/)
* [5 Best Text To Speech Software For YouTube Videos (#1 Real Human Voice)](https://youtu.be/1Vp-G5-TZxU)
* [How Speech Synthesizers Work](https://youtu.be/XsMRxNSDccc)
* [Lecture 3.2.3 Text to Speech](https://youtu.be/WOQEsJ7dFsg)
* [SPCC 2016 - Simon King - Speech synthesis using HMM](https://youtu.be/3Ffd75PVjjc)
* [This AI Clones Your Voice After Listening for 5 Seconds](https://youtu.be/0sR1rU3gLzQ)
* [Vocal synthesis](https://youtube.com/playlist?list=PLbxCiHBOYAAsCQzpCvPjZ2qIrTpcQ6DMj)
* [[DLHLP 2020] Speech Synthesis (1/2) - Tacotron](https://youtu.be/DMxKeHW8KdM)
* [TEXT TO SPEECH IN PYTHON | Convert Text to Speech in Python](https://youtu.be/_Q8wtPCyMdo)
* [speech synthesis playlist](https://youtube.com/playlist?list=PLP872v1Ob6shQGUq4k5jKsY2Qr8Ve0TGE)
* [Speech Synthesis Vid](https://youtu.be/Jcymn3RGkF4)    Efficient Neural Audio Synthesis <>  <> GitHub - fatchord/WaveRNN: WaveRNN Vocoder + TTS
* [Parallel WaveGAN (+ MelGAN & Multi-band MelGAN) implementation with Pytorch](https://github.com/kan-bayashi/ParallelWaveGAN)
* [docs.openvinotoolkit.org/2021.3/omz_models_model_wavernn.html](https://docs.openvinotoolkit.org/2021.3/omz_models_model_wavernn.html)
* [docs.openvinotoolkit.org/2021.3/omz_models_model_wavernn.html](https://docs.openvinotoolkit.org/2021.3/omz_models_model_wavernn.html)
* [EFFICIENTLY TRAINABLE TEXT-TO-SPEECH SYSTEM BASED ONDEEP CONVOLUTIONAL NETWORKS WITH GUIDED ATTENTION](https://arxiv.org/pdf/1710.08969.pdf)
* [Speech Synthesis and Control Using Differentiable DSP](https://arxiv.org/abs/2010.15084)
* [Fast Spectrogram Inversion using Multi-head Convolutional Neural Networks](https://arxiv.org/abs/1808.06719)
* [tuan3w/cnn_vocoder](https://github.com/tuan3w/cnn_vocoder)
* [Tacotron: Towards End-to-End Speech Synthesis](http://export.arxiv.org/abs/1703.10135)
* [tuan3w/cnn_vocoder](https://github.com/tuan3w/cnn_vocoder)
* [2-Minute Papers](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjZ9ueig_zvAhVj_rsIHd2KCXkQwqsBMAN6BAgFEBY&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DCqFIVCD1WWo&usg=AOvVaw3ADzOzkdZOhaqmijBYzcp4)
* [tuan3w/cnn_vocoder](https://github.com/tuan3w/cnn_vocoder)
* [35 Best Speech Synthesis Books of All Time - BookAuthority](https://bookauthority.org/books/best-speech-synthesis-books)
* [How long will it take to get a good result? · Issue #132 · NVIDIA/waveglow](https://github.com/NVIDIA/waveglow/issues/132)
* [speech.zone/courses/speech-synthesis/](http://speech.zone/courses/speech-synthesis/)
* [semanticscholar.org/paper/EXTRACTION-OF-EXCITATION-INFORMATION-FROM-SPEECH-Kadiri-Yegnanarayana/33dc3dd6ac53baa0e22b1d066ea1d9c24271554d](https://www.semanticscholar.org/paper/EXTRACTION-OF-EXCITATION-INFORMATION-FROM-SPEECH-Kadiri-Yegnanarayana/33dc3dd6ac53baa0e22b1d066ea1d9c24271554d)
* [duckduckgo.com/?t=lm&q=state+of+the+art+in+speech+synthesis&ia=web](https://duckduckgo.com/?t=lm&q=state+of+the+art+in+speech+synthesis&ia=web)
* [How Speech Synthesizers Work](https://youtu.be/XsMRxNSDccc)
* [Generative Model-Based Text-to-Speech Synthesis](https://youtu.be/nsrSrYtKkT8)
* [This AI Clones Your Voice After Listening for 5 Seconds](https://youtu.be/0sR1rU3gLzQ)
* [PwC: TTS Synthesis](https://paperswithcode.com/task/text-to-speech-synthesis)
* [GitHub - TTS-cdac-mumbai/TBT](https://github.com/TTS-cdac-mumbai/TBT)
* [WaveNet Blog Post](https://deepmind.com/blog/article/wavenet-generative-model-raw-audio)
* [WaveNet - A Generative Model for Raw Audio](https://arxiv.org/pdf/1609.03499.pdf)
* [b-ok.cc/s/speech%20synthesis](https://b-ok.cc/s/speech%20synthesis)
* [libgen.rs/search.php?&req=speech+synthesis&phrase=1&view=simple&column=def&sort=year&sortmode=DESC](http://libgen.rs/search.php?&req=speech+synthesis&phrase=1&view=simple&column=def&sort=year&sortmode=DESC)
* [Phonologic Patterns of Brazilian Portuguese: a grapheme to phoneme converter based study](http://www.aclweb.org/anthology/W12-0912)
* [End-to-End Text-to-Speech Synthesis, Part 1](https://youtu.be/RNKrq26Z0ZQ)
* [google.com/search?q=speech+synthesis+course&ie=UTF-8&oe=UTF-8&hl=it-us&client=safari](https://www.google.com/search?q=speech+synthesis+course&ie=UTF-8&oe=UTF-8&hl=it-us&client=safari)
* [itinerarium.github.io/phoneme-synthesis/](https://itinerarium.github.io/phoneme-synthesis/)
* [Speech Synthesis (playlist)](https://youtube.com/playlist?list=PL5ayXBv5T48YA8coSiP5Su_T9CftoVd6M)
* ["Speech Synthesis," Kim Silverman](https://youtu.be/7mjh0PSUv0M)
* [Tacotron: Towards End-to-End Speech Synthesis](https://arxiv.org/pdf/1703.10135.pdf)
* [What are i-vectors and x-vectors in the context of Speech Recognition?](https://dsp.stackexchange.com/questions/59086/what-are-i-vectors-and-x-vectors-in-the-context-of-speech-recognition)
* [mary.dfki.de/](http://mary.dfki.de/)
* [amjadmahayri.wordpress.com/2014/02/17/initial-model-for-speech-synthesis-task/](https://amjadmahayri.wordpress.com/2014/02/17/initial-model-for-speech-synthesis-task/)
* [vBaiCai/vc_tacotron](https://github.com/vBaiCai/vc_tacotron)
* [docs.microsoft.com/en-us/azure/cognitive-services/speech-service/custom-neural-voice](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/custom-neural-voice)
* [Text-to-speech for the hearing impaired](https://arxiv.org/abs/2012.02174)
* [GlottDNN - A full-band glottal vocoder for statistical parametricspeech synthesis](https://www.pure.ed.ac.uk/ws/portalfiles/portal/25545303/glottdnn_final.pdf)
* [ljuvela/GlottDNN](https://github.com/ljuvela/GlottDNN)
* [aalto-speech.github.io/GlottDNN/](https://aalto-speech.github.io/GlottDNN/)
* [COMPLEX SPECTROGRAM ENHANCEMENT BY CONVOLUTIONAL NEURAL NETWORKWITH MULTI-METRICS LEARNING](https://arxiv.org/ftp/arxiv/papers/1704/1704.08504.pdf)
* [Fast Spectrogram Inversion using Multi-head Convolutional Neural Networks](https://arxiv.org/abs/1808.06719)
* [Fast Spectrogram Inversion using Multi-head Convolutional Neural Networks](https://arxiv.org/pdf/1808.06719.pdf)
* [Fast Spectrogram Inversion using Multi-head Convolutional Neural Networks](https://arxiv.org/abs/1808.06719)    Gmm-based voice conversion applied to emotional speech synthesis
* [r12a.github.io/index](https://r12a.github.io/index)
* [patents.google.com/patent/US20190355347A1/en](https://patents.google.com/patent/US20190355347A1/en)    Improving sequence-to-sequence acoustic modeling by adding text-supervision
* [Prof. Simon King - Using Speech Synthesis to give Everyone their own Voice](https://youtu.be/xzL-pxcpo-E)
* [SANE2018 | Yu Zhang - Towards End-to-end Speech Synthesis](https://youtu.be/tHAdlv7ThjA)
* [SC-GlowTTS: an Efficient Zero-Shot Multi-Speaker Text-To-Speech Model](https://arxiv.org/abs/2104.05557)
* [AlignTTS: Efficient Feed-Forward Text-to-Speech System without Explicit Alignment](https://arxiv.org/abs/2003.01950)
* [Towards Natural and Controllable Cross-Lingual Voice Conversion Based on Neural TTS Model and Phonetic Posteriorgram](https://arxiv.org/abs/2102.01991)    Transfer learning from speaker verification to multispeaker text-to-speech synthesis
* [Evaluating Cognitive Load of Text-To-Speech (TTS) synthesis](http://pub.dega-akustik.de/ICA2019/data/articles/001094.pdf)
* [awesomeopensource.com/project/L0SG/WaveFlow?categoryPage=41](https://awesomeopensource.com/project/L0SG/WaveFlow?categoryPage=41)    SLP 26. Automatic Speech Recognition and Text-to-Speech    Text-to-Speech Synthesis    Predicting Prosody from Text for TTS Synthesis    Computer Speech    ESOLA    Speech Synthesis and Recognition    Speech and Audio Processing for Coding, Enhancement and Recognition    SS Slides    Developments in Speech Synthesis
* [LSA 352: Speech Recognition and Synthesis](https://nlp.stanford.edu/courses/lsa352/)
* [speech.zone/courses/speech-synthesis/](https://speech.zone/courses/speech-synthesis/)
* [Schedule - Neural Models for Speech Synthesis](https://mlcogup.github.io/nspeech2018/schedule.html)    Speech Prosody in Speech Synthesis    Source Modeling Techniques for QE in SP Speech Synthesis

## TTS

* [Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions (Tacotron 2))
* [The Implementation of Tacotron2 Based on Pytorch](https://github.com/NVIDIA/tacotron2)
* [18 code implementations (in PyTorch and TensorFlow)](https://paperswithcode.com/paper/natural-tts-synthesis-by-conditioning-wavenet#code)
* [Neural Voice Cloning with a Few Samples - NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper.pdf](https://proceedings.neurips.cc/paper/2018/file/4559912e7a94a9c32b09d894f2bc3c82-Paper.pdf)
* [Vocoder-free End-to-End Voice Conversion with Transformer Network](https://arxiv.org/abs/2002.03808)
* [Voice Transformer Network: Sequence-to-Sequence Voice Conversion Using Transformer with Text-to-Speech Pretraining](https://arxiv.org/abs/1912.06813)
* [cs.cmu.edu/~srallaba/Learn_Synthesis/intro.html](https://www.cs.cmu.edu/~srallaba/Learn_Synthesis/intro.html)
* [wiki.aalto.fi/display/ITSP/Speech+Synthesis](https://wiki.aalto.fi/display/ITSP/Speech+Synthesis)    Multilingual text-to-speech sunthesis: the Bell Labs approach
* [Phone-Level Embeddings for Unit Selection Speech Synthesis](https://hal.archives-ouvertes.fr/hal-01840812/document)
* [mmorise/World](https://github.com/mmorise/World)
* [melgan.ipynb - Colaboratory](https://colab.research.google.com/drive/1ajqFWXneNXBs2bJrlZAxe1fvaG_9ZFl0)

## Transcription    * [(PDF) Example-Based Automatic Phonetic Transcription.](https://www.researchgate.net/publication/220746286_Example-Based_Automatic_Phonetic_Transcription)

* [Example-based Automatic Phonetic Transcription](http://www.lrec-conf.org/proceedings/lrec2010/pdf/299_Paper.pdf)
* [(PDF) Universal Phone Recognition with a Multilingual Allophone System](https://www.researchgate.net/publication/339551416_Universal_Phone_Recognition_with_a_Multilingual_Allophone_System)
* [Automatic Phonetic Segmentation and Pronunciation Detection with Various Approaches of Acoustic Modeling - mizera2018.pdf](https://omilia.com/wp-content/uploads/2019/09/mizera2018.pdf)
* [Evaluating phonemic transcription of low-resource tonal languages for language documentation - document](https://halshs.archives-ouvertes.fr/halshs-01709648/document)
* [Kaldi for phonetic transcription - Google Suche](https://www.google.com/search?client=firefox-b-d&q=Kaldi+for+phonetic+transcription)
* [phonetic transcriptions - Google Scholar](https://scholar.google.de/scholar?hl=de&as_sdt=0%2C5&q=phonetic+transcriptions&btnG=&oq=phonetic+transcription)    Towards Zero-Shot Learning for Automatic Phonemic Transcription - 6341-Article Text-9566-1-10-20200517.pdf    * [Towards Zero-Shot Learning for Automatic Phonemic Transcription | OpenReview](https://openreview.net/forum?id=VPweScGZRmK)
* [Towards Zero-Shot Learning for Automatic Phonemic Transcription | Proceedings of the AAAI Conference on Artificial Intelligence](https://ojs.aaai.org//index.php/AAAI/article/view/6341)
* [paperswithcode.com/paper/listen-attend-and-spell](https://paperswithcode.com/paper/listen-attend-and-spell)
* [researchgate.net/post/Which_software_is_recommended_for_phonetic_transcription_of_languages](https://www.researchgate.net/post/Which_software_is_recommended_for_phonetic_transcription_of_languages)
* [spsc.tugraz.at/databases-and-tools/example-based-automatic-phonetic-transcription.html](https://www.spsc.tugraz.at/databases-and-tools/example-based-automatic-phonetic-transcription.html)
* [Universal Phone Recognition with a Multilingual Allophone System](https://arxiv.org/pdf/2002.11800.pdf)
* [xinjli/allosaurus](https://github.com/xinjli/allosaurus)
* [dictate.app/](https://www.dictate.app/)
* [NeurST: Neural Speech Translation Toolkit](https://arxiv.org/abs/2012.10018)
* [arxiv.org/search/?query=automatic+phonetic+transcription&searchtype=all&source=header](https://arxiv.org/search/?query=automatic+phonetic+transcription&searchtype=all&source=header)
* [search?q=phonetic+ipa+transcription](https://github.com/search?q=phonetic+ipa+transcription)
* [WAV2TEXT](https://github.com/Speech-Lab-IITM/WAV2TEXT)
* [The 'Neural' Phonetic Typewriter](https://booksc.org/book/14499437/d5d582)
* [Automatic Phonetic Transcription of Spontaneous Speech (American English)
* [Phonetic lessons from automatic phonemic transcription: preliminary reflections on Na (Sino-Tibetan) and Tsuut’ina (Dene) data](https://halshs.archives-ouvertes.fr/halshs-02059313v2/document)
* [IPA Transcription](https://github.com/Anaphory/transcription)
* [Foreign English Accent Adjustment by Learning Phonetic Patterns](https://arxiv.org/pdf/1807.03625.pdf)
* [The importance of narrow phonetic transcription for highly unintelligible speech: Some examples](https://www.researchgate.net/publication/24434124_The_importance_of_narrow_phonetic_transcription_for_highly_unintelligible_speech_Some_examples)
* [Automatic Phonetic Transcription: An Overview](https://www.researchgate.net/publication/2930747_Automatic_Phonetic_Transcription_An_Overview/link/0deec51954d1ca5ec3000000/download)    Automatic Phonetic Transcription of Large Speech Corpora
* [Using IPA-Based Tacotron for Data Efficient Cross-Lingual Speaker Adaptation and Pronunciation Enhancement](https://arxiv.org/abs/2011.06392)
* [Discovering Lexical Similarity Through Articulatory Feature-based Phonetic Edit Distance](https://arxiv.org/abs/2008.06865)
* [Phonological Features for 0-shot Multilingual Speech Synthesis](https://arxiv.org/abs/2008.04107)
* [How Phonotactics Affect Multilingual and Zero-shot ASR Performance](https://arxiv.org/abs/2010.12104)
* [GIPFA: Generating IPA Pronunciation from Audio](https://arxiv.org/abs/2006.07573)
* [Learning Joint Acoustic-Phonetic Word Embeddings](https://arxiv.org/pdf/1908.00493)    What do phone embeddings learn about Phonology?
* [Word Embeddings for Speech Recognition](https://static.googleusercontent.com/media/research.google.com/de//pubs/archive/42543.pdf)
* [Automatic Pronunciation Generation by Utilizing a Semi-supervised Deep Neural Networks](https://arxiv.org/abs/1606.05007)
* [Phonetic and Graphemic Systems for Multi-Genre Broadcast Transcription](https://arxiv.org/abs/1802.00254)
* [Automatic Phonetic Transcription of Non-Prompted Speech](https://www.phonetik.uni-muenchen.de/forschung/publikationen/ICPhS99_Schiel.pdf)
* [Sound Analogies with Phoneme Embeddings](https://mpsilfve.github.io/assets/analogies.pdf)    Poetic Sound Similarity Vectors Using Phonetic Features
* [Automatic measurement of vowel duration via structured prediction](https://arxiv.org/abs/1610.08166)    The new accent technologies: recognition, measurement and manipulation of accented speech
* [Pronunciation Modeling in SS (1998)](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1056&context=ircs_reports)
* [Language Through a Prism: A Spectral Approach for Multiscale Language Representations](https://arxiv.org/abs/2011.04823)
* [Audito](https://github.com/Nikhil-Chacharkar/Audito)
* [neural network for ipa phonetic transcription - Cerca con Google](https://www.google.com/search?q=neural+network+for+ipa+phonetic+transcription&ie=UTF-8&oe=UTF-8&hl=it-us&client=safari)
* [google.com/search?q=neural+network+phonetic+transcription&rlz=1C5CHFA_enUS841US841&oq=neural+network+phonetic+transcription&aqs=chrome..69i57.6998j0j7&sourceid=chrome&ie=UTF-8](https://www.google.com/search?q=neural+network+phonetic+transcription&rlz=1C5CHFA_enUS841US841&oq=neural+network+phonetic+transcription&aqs=chrome..69i57.6998j0j7&sourceid=chrome&ie=UTF-8)
* [aparrish/phonetic-similarity-vectors](https://github.com/aparrish/phonetic-similarity-vectors)
* [PwC:](https://paperswithcode.com/paper/what-do-phone-embeddings-learn-about)
* [CMU Sphinx](https://cmusphinx.github.io/wiki/phonemerecognition/)
* [researchgate.net/post/Which_software_is_recommended_for_phonetic_transcription_of_languages](https://www.researchgate.net/post/Which_software_is_recommended_for_phonetic_transcription_of_languages)

## Title Ideas    (Learning) Fine-Grained Phonetic Feature Representations for Transcription and Accent Transfer    Learning Fine-Grained Phonetic Representations for Speech Transcription and Accent Transfer    Fine-Grained Speech Processing for Phonetic Transcription and Accent Transfer    Fine-Grained Phonetic Processing of Speech

## Phonetics / ASR

* [pypi.org/project/wikipron/](https://pypi.org/project/wikipron/)
* [Plosive - Wikipedia](https://en.wikipedia.org/wiki/Plosive)
* [video](https://www.youtube.com/user/cognitivephonetician/playlists)
* [AlloVera](https://arxiv.org/abs/2004.08031v1)
* [NeurST: Neural Speech Translation Toolkit](https://arxiv.org/abs/2012.10018)
* [bytedance/neurst](https://github.com/bytedance/neurst)
* [pavelsof/ipavec](https://github.com/pavelsof/ipavec)
* [Phonetic Vector Representations for Sound Sequence Alignment](https://coltekin.net/cagri/papers/sofroniev2018.pdf)
* [Acoustic Phonetics and Speech Perception - Syllabus](https://faculty.wcas.northwestern.edu/ann-bradlow/docs/Ling450-2-sample-syllabus.pdf)
* [Phonology MIT 2010](https://ocw.mit.edu/courses/linguistics-and-philosophy/24-901-language-and-its-structure-i-phonology-fall-2010/)
* [Transcribing Prosodic Structure](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-911-transcribing-prosodic-structure-of-spoken-utterances-with-tobi-january-iap-2006/)
* [Introduction to Phonology](https://ocw.mit.edu/courses/linguistics-and-philosophy/24-961-introduction-to-phonology-fall-2014/)
* [Advanced Phonology](https://ocw.mit.edu/courses/linguistics-and-philosophy/24-962-advanced-phonology-spring-2005/)
* [Topics in Linguistic Theory: Laboratory Phonology](https://ocw.mit.edu/courses/linguistics-and-philosophy/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/)
* [Acoustic Phonetics Syllabus](https://drive.google.com/file/d/1n1cdZ_UApqi0F0qUuU1s3shQZrVIZWwy/view)
* [pavelsof/ipavec](https://github.com/pavelsof/ipavec)
* [wikipron](https://pypi.org/project/wikipron/)
* [The complete official IPA chart](https://youtu.be/OGYGDQgeh2c)
* [fr.wikipedia.org/wiki/Mod%C3%A8le:Palette_Prononciation_des_langues](https://fr.wikipedia.org/wiki/Mod%C3%A8le:Palette_Prononciation_des_langues)
* [en.wikipedia.org/wiki/Template:Language_phonologies](https://en.wikipedia.org/wiki/Template:Language_phonologies)
* [es.wikipedia.org/wiki/Categor%C3%ADa:Fonolog%C3%ADa_por_idioma](https://es.wikipedia.org/wiki/Categor%C3%ADa:Fonolog%C3%ADa_por_idioma)
* [soloist96/CS230Accent](https://github.com/soloist96/CS230Accent)
* [29 - Lezione 5e - vocali posteriori e centrali (30 marzo 2020)](https://youtu.be/IKuAUgZCJNA)
* [Linguistica e Fonetica lezione 2 parte 2/3: L'IPA e la trascrizione](https://youtu.be/-e0PJ1UF_Js)
* [IPA - Vocali](https://youtu.be/_Mf2I3wxMdw)
* [I suoni dell'italiano e l'alfabeto IPA](https://youtu.be/VikJpAo5rp0)
* Phonetic spectra and their acoustic forms (incl. sociolinguistic aspect)
* Correspondence between IPA description of language vs IPA description of dialect and the generated acoustics
* [ocw.mit.edu/courses/linguistics-and-philosophy/24-915-linguistic-phonetics-fall-2015/readings/](https://ocw.mit.edu/courses/linguistics-and-philosophy/24-915-linguistic-phonetics-fall-2015/readings/)
* [ocw.mit.edu/courses/linguistics-and-philosophy/24-915-linguistic-phonetics-fall-2015/](https://ocw.mit.edu/courses/linguistics-and-philosophy/24-915-linguistic-phonetics-fall-2015/)
* [all-about-linguistics.group.shef.ac.uk/branches-of-linguistics/phonetics/what-do-phoneticians-study/acoustic-phonetics/](https://all-about-linguistics.group.shef.ac.uk/branches-of-linguistics/phonetics/what-do-phoneticians-study/acoustic-phonetics/)
* [uni-bielefeld.de/lili/personen/vgramley/teaching/HTHS/acoustic_2010.html](https://www.uni-bielefeld.de/lili/personen/vgramley/teaching/HTHS/acoustic_2010.html)
* [video](https://www.youtube.com/results?search_query=acoustic+phonetics)
* [video](https://www.youtube.com/results?search_query=acoustic+phonetics&sp=EgIQAw%253D%253D)
* [Acoustic and auditory phonetics: the adaptive design of speech sound systems](https://royalsocietypublishing.org/doi/10.1098/rstb.2007.2153)
* [ello.uos.de/field.php/PhoneticsandPhonology/AcousticPhonetics](http://www.ello.uos.de/field.php/PhoneticsandPhonology/AcousticPhonetics)
* [en.wikipedia.org/wiki/Acoustic_phonetics](https://en.wikipedia.org/wiki/Acoustic_phonetics)
* [The Virtual Linguistics Campus](https://youtube.com/c/LinguisticsMarburg)
* [jsfalk/prosodic1b](https://github.com/jsfalk/prosodic1b)
* [r12a.github.io/pickers/ipa/](https://r12a.github.io/pickers/ipa/)
* [<--](https://pavelsof.com/)

## Acoustics

* [Fundamentals of Musical Acoustics: Second, Revised Edition](https://b-ok.cc/book/2208553/864a10)
* [Principles of Musical Acoustics](https://b-ok.cc/book/2157409/9ac75f)
* [Acoustics: an introduction](https://b-ok.cc/book/592227/960610)
* [акустика](https://b-ok.cc/s/?q=%D0%B0%D0%BA%D1%83%D1%81%D1%82%D0%B8%D0%BA%D0%B0)    Analyse und Synthese akustischer Spektren    Acoustic modeling in Automatic Speech Recognition \342\200\223 A Survey (Waris & Aggarwal).pdf

## DSP

* Intelligent Speech Signal Processing
* Mathematical Modeling and Signal Processing in Speech and Hearing Sciences
* ASR Slides
* [LSA 352: Speech Recognition and Synthesis](https://nlp.stanford.edu/courses/lsa352/)
* Mathematical Models for Speech Technology
* Speech and Audio Signal Processing
* Robust Digital Processing of Speech Signals
* Neural Modeling of Speech Processing and Speech Learning
* Speech and Audio Processing for Coding, Enhancement and Recognition
* Audio Processing and Speech Recognition
* Applied Speech Processing and Audio Processing
* Robust Automatic Speech Recognition
* Speech Enhancement, Modeling and Recognition
* [Traitement de Signal](https://www.youtube.com/playlist?list=PLwL_MKgwKjiLqm47oKIxOg57pwghZJ5kn)
* [Traitement de Signal](https://www.youtube.com/playlist?list=PLIDlTAsMIE3fiNP10xd6Pcs9xxHHaI0tp)
* [Procesamiento de señales](https://www.youtube.com/channel/UCBDju_kFkoJBCRTFWjeHnXw)
* [Signalverarbeitung in der Informatik](https://www.youtube.com/playlist?list=PLNmsVeXQZj7qvLZlbEiRwHoP0mbmAHNId)
* [KIT Signalverarbeitung](https://www.youtube.com/playlist?list=PLfk0Dfh13pBO-lUuk7hOFac81wv88Ii1T)
* Digital and statistical signal processing (2019)
* Hack audio : an introduction to computer programming and digital signal processing in MATLAB
* Introduction to Digital Signal Processing using MatLab with Application to Digital Communications
* Linear Algebra, Signal Processing and Wavelets – a unified Approach. Python Version
* Linear Algebra, Signal Processing and Wavelets – a unified Approach. Matlab Version
* Control Engineering
* Digital signal processing. Fundamentals and applications
* Digital Signal Processing: A Primer with MATLAB®
* Topics in signal processing
* Fourier Transforms: Principles and Applications
* Discrete Fourier Analysis and Wavelets: Applications to Signal and Image Processing
* Schaum's Outline of Signals and Systems, Second Edition

## Alignment

* [Thomas-Schatz/no_phon_cats](https://github.com/Thomas-Schatz/no_phon_cats)
* [Neural Models for Sequence Chunking](http://eecs.csuohio.edu/~sschung/CIS601/Neural%20Models%20for%20Sequence%20Chunking.pdf)
* [Sequence Labeling Approach to the Task of Sentence Boundary Detection](https://paperswithcode.com/paper/sequence-labeling-approach-to-the-task-of)
* [CTC Explanation](https://distill.pub/2017/ctc/)
* [pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html](https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html)
* [Self-Attention Networks for Connectionist Temporal Classification in Speech Recognition](https://paperswithcode.com/paper/self-attention-networks-for-connectionist)
* [Audio Tagging With Connectionist Temporal Classification Model Using Sequential Labelled Data](https://paperswithcode.com/paper/audio-tagging-with-connectionist-temporal)
* [CTC-Segmentation of Large Corpora for German End-to-end Speech Recognition](https://paperswithcode.com/paper/ctc-segmentation-of-large-corpora-for-german)
* [Advances in Joint CTC-Attention based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM](https://paperswithcode.com/paper/advances-in-joint-ctc-attention-based-end-to)
* [weedwind/CTC-speech-recognition](https://github.com/weedwind/CTC-speech-recognition)
* [S18 Lecture 14: Connectionist Temporal Classification (CTC)](https://www.youtube.com/watch?v=c86gfVGcvh4&t=155s)
* [CTC NNs](https://ieeexplore.ieee.org/abstract/document/8788608/?casa_token=aQ34uboGUNEAAAAA:i2F7hWF1FvUDq2MFeG0HAkSWJKu5seQVk3EMXD80aXiRSqbTrPmUNVDdn4VlTGKXrBQq6e_0Fw)
* [Forced alignment using FAVE and DARLA](https://www.youtube.com/watch?v=xqeZgfUVL5A)
* [prosodylab/Prosodylab-Aligner](https://github.com/prosodylab/Prosodylab-Aligner)
* [prosodylab/prosodylab.alignertools](https://github.com/prosodylab/prosodylab.alignertools)
* [prosodylab/prosodylab-alignermodels](https://github.com/prosodylab/prosodylab-alignermodels)
* [prosodylab.cs.mcgill.ca/tools/aligner/](http://prosodylab.cs.mcgill.ca/tools/aligner/)
* [AswinKumar1/Forced-Alignment](https://github.com/AswinKumar1/Forced-Alignment)
* [phon.ucl.ac.uk/resource/sfs/howto/htk.php](https://www.phon.ucl.ac.uk/resource/sfs/howto/htk.php)
* [pettarin/forced-alignment-tools](https://github.com/pettarin/forced-alignment-tools)
* [http://montreal-forced-aligner.readthedocs.io](http://montreal-forced-aligner.readthedocs.io/)
* [MontrealCorpusTools/Montreal-Forced-Aligner](https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner)
* [nawarhalabi/Prosodylab-Aligner](https://github.com/nawarhalabi/Prosodylab-Aligner)
* [paperswithcode.com/paper/neural-machine-translation-by-jointly](https://paperswithcode.com/paper/neural-machine-translation-by-jointly)
* [Automatic Phonetic Segmentation and Pronunciation Detection with Various Approaches of Acoustic Modeling](https://omilia.com/wp-content/uploads/2019/09/mizera2018.pdf)
* [Phoneme boundary detection from speech: A rule based approach](https://www.sciencedirect.com/science/article/abs/pii/S0167639317302029)
* [Colab Phoneme Prediction](https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW02/HW02-1.ipynb)
* [anjandeepsahni/speech_phoneme_prediction](https://github.com/anjandeepsahni/speech_phoneme_prediction)
* [Phoneme recognition in TIMIT with BLSTM-CTC](https://arxiv.org/abs/0804.3269)
* [TIMIT and NTIMIT Phone Recognition Using Convolutional Neural Networks](https://link.springer.com/chapter/10.1007/978-3-030-05499-1_5)
* [intechopen.com/books/speech-technologies/phoneme-recognition-on-the-timit-database](https://www.intechopen.com/books/speech-technologies/phoneme-recognition-on-the-timit-database)
* [machinelearningmastery.com/predictive-model-for-the-phoneme-imbalanced-classification-dataset/](https://machinelearningmastery.com/predictive-model-for-the-phoneme-imbalanced-classification-dataset/)
* [amjadmahayri.wordpress.com/2014/02/27/frame-prediction-given-phoneme-window/](https://amjadmahayri.wordpress.com/2014/02/27/frame-prediction-given-phoneme-window/)    Event Selection from Phone Posteriorgrams Using Matched Filters
* [sciforce/phones-las](https://github.com/sciforce/phones-las)
* [researchgate.net/publication/302921797_Investigation_of_DNN-Based_Keyword_Spotting_in_Low_Resource_Environments](https://www.researchgate.net/publication/302921797_Investigation_of_DNN-Based_Keyword_Spotting_in_Low_Resource_Environments)
* [jaekookang/Articulatory-Data-Extractor](https://github.com/jaekookang/Articulatory-Data-Extractor)
* [real.mtak.hu/54848/1/aling.49.2002.3-4.9.pdf](http://real.mtak.hu/54848/1/aling.49.2002.3-4.9.pdf)
* [blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)
* [cjerry1243/TIMIT_Phoneme_Recognition](https://github.com/cjerry1243/TIMIT_Phoneme_Recognition)
* [Faur/TIMIT](https://github.com/Faur/TIMIT)
* [rash-me-not/TIMIT-phoneme-recognition-with-Recurrent-Neural-Nets](https://github.com/rash-me-not/TIMIT-phoneme-recognition-with-Recurrent-Neural-Nets)
* [MLSpeech/AutoAligner/tree/a1ae0fdf7cfe5e13b4901e0c51e59fd2a216cac5](https://github.com/MLSpeech/AutoAligner/tree/a1ae0fdf7cfe5e13b4901e0c51e59fd2a216cac5)
* [palsanjoy/ASR_Project1/tree/master/p1](https://github.com/palsanjoy/ASR_Project1/tree/master/p1)
* [smtasr/ASR_Project1/tree/master/p1](https://github.com/smtasr/ASR_Project1/tree/master/p1)
* [pranaviiit1997/ASR_project](https://github.com/pranaviiit1997/ASR_project)
* [felixkreuk/SegFeat](https://github.com/felixkreuk/SegFeat)
* [distill.pub/2017/ctc/](https://distill.pub/2017/ctc/)
* [plaxi0s/pytorch-MDN](https://github.com/plaxi0s/pytorch-MDN)
* [duckduckgo.com/?t=ffab&q=ctc+phonetic+segmentation+in+pytorch&ia=web](https://duckduckgo.com/?t=ffab&q=ctc+phonetic+segmentation+in+pytorch&ia=web)
* [Automated Audio Segmentation Using Forced Alignment](http://www.voxforge.org/home/dev/autoaudioseg/)
* [voxforge.org/home/dev/autoaudioseg](http://www.voxforge.org/home/dev/autoaudioseg)
* [sourceforge.net/p/cmusphinx/discussion/help/thread/d3815ba3/](https://sourceforge.net/p/cmusphinx/discussion/help/thread/d3815ba3/)
* [scholar.google.fr/scholar?cites=10082076182467922702&as_sdt=2005&sciodt=0,5&hl=de](https://scholar.google.fr/scholar?cites=10082076182467922702&as_sdt=2005&sciodt=0,5&hl=de)
* [ACCURATE SPEECH SEGMENTATION BY MIMICKING HUMAN AUDITORY PROCESSING](http://www.isle.illinois.edu/~sborys/king_icassp13.pdf)
* [Phonetic Segmentation Using Knowledge from Visual and Perceptual Domain](https://link.springer.com/chapter/10.1007/978-3-319-64206-2_44)
* [A loss-balanced multi-task model for simultaneous detection and segmentation](https://www.sciencedirect.com/science/article/abs/pii/S0925231220318105)    Classification of Phonemes by GMMs on TIMIT
* [intechopen.com/books/speech-technologies/phoneme-recognition-on-the-timit-database](https://www.intechopen.com/books/speech-technologies/phoneme-recognition-on-the-timit-database)
* [janekzimoch/Speech_Recognition/blob/main/ASR_project_H801L.pdf](https://github.com/janekzimoch/Speech_Recognition/blob/main/ASR_project_H801L.pdf)
* [persephone-tools/persephone](https://github.com/persephone-tools/persephone)
* [A Study of All-Convolutional Encoders for Connectionist Temporal Classification](https://arxiv.org/abs/1710.10398)
* [jzlianglu/dynet/blob/master/examples/cpp/segrnn-sup/segrnn-speech.cc](https://github.com/jzlianglu/dynet/blob/master/examples/cpp/segrnn-sup/segrnn-speech.cc)    Segmental Recurrent Neural Networks for End-to-EndSpeech Recognition
* [Probabilistic Linear Discriminant Analysis (PLDA) with Bottleneck Features for Speech Recognition (slides)](https://home.ttic.edu/~llu/pdf/IS2014_lianglu.pdf)
* [linguistics.berkeley.edu/plab/guestwiki/index.php?title=Berkeley_Phonetics_Machine](https://linguistics.berkeley.edu/plab/guestwiki/index.php?title=Berkeley_Phonetics_Machine)
* Bitext word alignment - Wikipedia
* [Text-Speech Alignment: A Robin Hood Approach for Endangered Languages](https://elischolar.library.yale.edu/cgi/viewcontent.cgi?article=1105&context=dayofdata)
* [A survey of sequence alignment algorithms for next-generation sequencing](https://pubmed.ncbi.nlm.nih.gov/20460430/)    Sequence Alignment Algorithms (pdf)    Sequence Alignment (pdf slides)    SailAlign: Robust long speech-text alignment (pdf)
* [Reading--alignment and SNP calling algorithms](https://hc1023.github.io/2020/05/19/reading-alignment-and-SNP-calling-algorithms/)
* [Evaluation of iterative alignment algorithms for multiple alignment](https://academic.oup.com/bioinformatics/article/21/8/1408/249176)
* [Smith–Waterman algorithm](https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm)
* [Sequence alignment](https://en.wikipedia.org/wiki/Sequence_alignment)    Alignment  Algorithms  for  Learning  to  Read  Aloud (pdf)    A Part-of-Speech-Based Alignment Algorithm (pdf)    Text-to-Speech Alignment (chapter 6 of Computed Synchronization for Multimedia Applications, pdf; see chapter 5 as well)
* [Performance of Forced-Alignment Algorithms on Children's Speech (pdf)](https://pubs.asha.org/doi/10.1044/2020_JSLHR-20-00268)    Speaker Identification on the SCOTUS Corpus (pdf)    Montreal Forced Aligner: trainable text-speech alignment using Kaldi (pdf)
* [Forced Alignment: How to match audio with a transcript via Machine Learning?](https://techfirst.medium.com/forced-alignment-how-to-match-audio-with-a-transcript-via-machine-learning-dd19da8c0f04)    A Dynamic Alignment Algorithm for Imperfect Speech and Transcript (pdf)    Automatic text alignment for speech system evaluation (pdf)    Simple linguistic methods for improving a word alignment algorithm (pdf)    Introduction to Various Algorithms of Speech Recognition: Hidden Markov Model, Dynamic Time Warping and Artificial Neural Networks (pdf)
* [Audio Deep Learning Made Simple: Automatic Speech Recognition (ASR), How it Works](https://towardsdatascience.com/audio-deep-learning-made-simple-automatic-speech-recognition-asr-how-it-works-716cfce4c706)    Glow-TTS: A Generative Flow for Text-to-Speech viaMonotonic Alignment Search (pdf)    Fast Algorithm for Automatic Alignment of Speech and Imperfect Text Data (Chapter in Speech and Computer: 15th International Conference, SPECOM 2013, pdf)    Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces (pdf)
* [cmusphinx.github.io/wiki/tutorialbeforestart/](https://cmusphinx.github.io/wiki/tutorialbeforestart/)    JTrans, an open-source software for semi-automatictext-to-speech alignment (pdf)    A Recursive Algorithm for the Forced Alignment of Very Long Audio Segments (pdf)    Automatic time alignment of speech with a phonetic transcription (pdf)    Using Adaptation to Improve Speech Transcription Alignment in Noisy and Reverberant Environments (pdf)    Phoneme Similarity Matrices to Improve Long Audio Alignment for Automatic Subtitling (pdf)    A Search for Durational Rules in a Real-Speech Data Base (pdf)    Bitext Alignment (book)
* [GitHub - lowerquality/gentle: gentle forced aligner](https://github.com/lowerquality/gentle)
* [cs.paperswithcode.com/paper/sound-event-detection-and-time-frequency](https://cs.paperswithcode.com/paper/sound-event-detection-and-time-frequency)
* [Alignment Constraints in Optimality Theory: Two Examples](http://archives.bukkyo-u.ac.jp/rp-contents/ER/0019/ER00190L036.pdf)
* [ogrisel.github.io/scikit-learn.org/sklearn-tutorial/modules/generated/sklearn.hmm.MultinomialHMM.html](https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/modules/generated/sklearn.hmm.MultinomialHMM.html)
* [montreal-forced-aligner.readthedocs.io/en/stable/data_prep.html](https://montreal-forced-aligner.readthedocs.io/en/stable/data_prep.html)
* [dopefishh/praatalign](https://github.com/dopefishh/praatalign)
* [Automatic Speech Alignment](https://www.youtube.com/watch?v=ZC57sVT5Gio)
* [?](https://eleanorchodroff.com/tutorial/autovot.html)
* [lowerquality/gentle](https://github.com/lowerquality/gentle)
* [Praat tutorial | Maria Gouskova](https://www.gouskova.com/2016/09/03/praat-tutorial/#Opening_playing_recording_and_editing_audio_files_in_Praat)
* [API Reference — Parselmouth 0.4.0 documentation](https://parselmouth.readthedocs.io/en/stable/api_reference.html)
* [praatio.tgio API documentation](http://timmahrt.github.io/praatIO/praatio/tgio.html)
* [OSF | Praat Scripts](https://osf.io/huz7d/)
* [OSF | Parselmouth Praat Scripts in Python](https://osf.io/6dwr3/)
* [view textgrid alignment in python at DuckDuckGo](https://duckduckgo.com/?t=ffab&q=view+textgrid+alignment+in+python&ia=web)
* [Forced alignment | Kranti S Wadhai](https://wkranti.github.io/kragstrob/2018/05/19/Forced-Alignment/)
* [Forced alignment | NCSU Phonetics Lab](https://phon.wordpress.ncsu.edu/lab-manual/forced-alignment/)
* [babel.ling.upenn.edu/phonetics/old_website_2015/p2fa/readme.txt](https://babel.ling.upenn.edu/phonetics/old_website_2015/p2fa/readme.txt)
* [5 Penn Forced Aligner (Legacy) | Corpus Phonetics Tutorial](https://eleanorchodroff.com/tutorial/penn-forced-aligner-legacy.html)
* [Forced Alignment | xuchenziamy](https://xchenzi1994.wixsite.com/xuchenziamy/forced-alignment)
* [praat script to save textgrid spectrogram image at DuckDuckGo](https://duckduckgo.com/?t=ffab&q=praat+script+to+save+textgrid+spectrogram+image&ia=web)
* [Praat scripts - Christian DiCanio](https://www.acsu.buffalo.edu/~cdicanio/scripts.html)
* [Praat script for more efficient manual TextGrid annotation](https://gist.github.com/scjs/ffbbba71cc8b3ff9d0476c82b2df9d0f)
* [Praat — pympi 1.69 documentation](http://dopefishh.github.io/pympi/Praat.html)
* [jcaa.caa-aca.ca/index.php/jcaa/article/view/2476/2225](https://jcaa.caa-aca.ca/index.php/jcaa/article/view/2476/2225)
* [MFA LJSpeech.ipynb](https://gist.github.com/NTT123/12264d15afad861cb897f7a20a01762e)
* [MFA InfoRe Tutorial](https://gist.github.com/NTT123/9c1fb092d76acb767cfd930386eeb1ce)
* [PLA](https://github.com/prosodylab/Prosodylab-Aligner)
* [Forced Phonetic Alignment Using Deep Neural Networks](https://www.cs.mcgill.ca/~acoles/Forced_Phonetic_Alignment_Coles.pdf)    learning to align
* [Montreal Forced Aligner: trainable text-speech alignment using Kaldi](https://montrealcorpustools.github.io/Montreal-Forced-Aligner/images/MFA_paper_Interspeech2017.pdf)
* [MontrealCorpusTools/Montreal-Forced-Aligner](https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner)
* [A Robin Hood approach to forced alignment: English-trained algorithms and their use on Australian languages](https://journals.linguisticsociety.org/proceedings/index.php/PLSA/article/download/4468/4083)
* [EasyAlign](http://latlcui.unige.ch/phonetique/easyalign.php)

## Disentanglement

* [3 code implementations (in PyTorch and TensorFlow)](https://paperswithcode.com/paper/unsupervised-learning-of-disentangled-and#code)
* [Latent Sequence Decompositions](https://arxiv.org/pdf/1610.03035.pdf)
* [Supervised Determined Source Separation with Multichannel Variational Autoencoder](https://direct.mit.edu/neco/article/31/9/1891/8494/Supervised-Determined-Source-Separation-with)
* [Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data](https://arxiv.org/abs/1709.07902)
* [Unsupervised Speech Decomposition via Triple Information Bottleneck](https://arxiv.org/abs/2004.11284)

## Data

* [The KTH speech database](https://www.sciencedirect.com/science/article/abs/pii/016763939090013Y)
* [ldc.upenn.edu/](https://www.ldc.upenn.edu/)
* [arXiv Bulk Data Access | arXiv e-print repository](https://arxiv.org/help/bulk_data)
* [AccentDB: A Database of Non-Native English Accents toAssist Neural Speech Recognition](https://www.aclweb.org/anthology/2020.lrec-1.659.pdf)
* [C-PROM corpus libre de parole multigenre](https://sites.google.com/site/corpusprom/)    Data Augmentation for Audio Medium    * [GitHub - pzelasko/torch-audiomentations: Audio data augmentation in PyTorch. Inspired by audiomentations. Useful for deep learning.](https://github.com/pzelasko/torch-audiomentations)
* [dumps.wikimedia.org/enwiktionary/20210401/](https://dumps.wikimedia.org/enwiktionary/20210401/)
* [psi.engr.tamu.edu/l2-arctic-corpus/](https://psi.engr.tamu.edu/l2-arctic-corpus/)
* [Kaldi: Data preparation](https://kaldi-asr.org/doc/data_prep.html)    L2-ARCTIC: A non-native English speech corpus    * [speech accent archive: browse](https://accent.gmu.edu/browse_language.php?function=detail&speakerid=4)    The Variation in Conversation (ViC) Project: Creation of the Buckeye Corpus of Conversational Speech (Drive)
* $$$ english speech corpora accents
* [TIMIT GH](https://github.com/philipperemy/timit)
* [TIMIT Dataset GH](https://github.com/topics/timit-dataset)
* [Open SLR](https://www.openslr.org/resources.php)
* [Best SR Datasets](https://lionbridge.ai/datasets/best-speech-recognition-datasets-for-machine-learning/)
* [google.com/search?q=how+much+audio+data+do+you+need+for+style+transfer%3F&ie=UTF-8&oe=UTF-8&hl=it-us&client=safari](https://www.google.com/search?q=how+much+audio+data+do+you+need+for+style+transfer%3F&ie=UTF-8&oe=UTF-8&hl=it-us&client=safari)
* $$$ CSTR VCTK Corpus
* [festvox.org/cmu_arctic/](http://festvox.org/cmu_arctic/)
* [psi.engr.tamu.edu/l2-arctic-corpus/](https://psi.engr.tamu.edu/l2-arctic-corpus/)
* [Montreal Forced Aligner](https://montreal-forced-aligner.readthedocs.io/en/latest/)
* [Spoken Wikipedia Corpus](https://nats.gitlab.io/swc/)
* [^](https://corpora.uni-hamburg.de/hzsk/de/islandora/object/spoken-corpus:swc-2.0#additional-files)
* [festvox.org/cmu_arctic/packed/](http://festvox.org/cmu_arctic/packed/)
* [en.arabicspeechcorpus.com/](http://en.arabicspeechcorpus.com/)
* [towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad](https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad)
* [Indian Booktuber](https://www.youtube.com/channel/UCh1HFP6vj3LmmujJSSfRoog)
* [Sikh Podcast](https://open.spotify.com/show/1KTrY4vsFAKQH9oFzYdGWY?si=SZc41_lNS5-_yc_mrS1-LQ)
* [Stories of Mahabharata](https://open.spotify.com/show/3tVoPB7cTVqKnt3GbAUfIu?si=QavlMVisTd6m0G3Gpu2WJA)
* [C++ Intro](https://youtube.com/watch?v=l0qvxPPISuY&feature=share)
* [Indian Hindu Mythoogy Audiobook](https://www.youtube.com/watch?v=WltCjz63w8w)
* [Matrices 1](https://youtube.com/watch?v=eV3NidpjfNg&feature=share)
* [Java Coding Standards](https://youtu.be/h0-tyR5JceM)
* [...](https://xploringindia.in/best-youtubers-in-india/)
* [Sreetama Guin](https://www.youtube.com/channel/UCXrIa5nK09TTjtEHFvWaVCg)
* [Geekyranjit](https://www.youtube.com/channel/UCO2WJZKQoDW4Te6NHx4KfTg)
* [indian-podcasts.com/podcast/the-history-of-india-podcast](https://indian-podcasts.com/podcast/the-history-of-india-podcast)
* [Tutorials Point (India) Ltd.](https://www.youtube.com/channel/UCVLbzhxVTiTLiVKeGV7WEBg)
* [Studytonight](https://www.youtube.com/channel/UCURY4RZois483CPY8prhx7g)
* [Intellipaat](https://www.youtube.com/user/intellipaaat)
* [video](https://www.youtube.com/user/edurekaIN)
* [Indian Monk](https://www.youtube.com/channel/UClb8OEBhrUnZgMWc0wTmMvg)    Librivox - search by author
* [Indian Language Speech Database](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.258.7759&rep=rep1&type=pdf)
* [A Database of Non-Native English Accents to Assist Neural Speech Recognition | AccentDB](https://accentdb.org/)
* [Indian Accent Speech Recognition. Traditional ASR (Signal & Cepstral… | by Anand P V | Towards Data Science](https://towardsdatascience.com/indian-accent-speech-recognition-2d433eb7edac)
* [A brief overview of the resources available at IndicTTS website](https://www.youtube.com/watch?v=ZQM_0G5R3HU&feature=youtu.be)
* [quora.com/Are-there-any-open-source-speech-to-text-models-ASR-for-Indian-languages](https://www.quora.com/Are-there-any-open-source-speech-to-text-models-ASR-for-Indian-languages)
* [researchgate.net/post/Any-tool-for-Speech-to-Text-conversion-especially-for-Indian-English](https://www.researchgate.net/post/Any-tool-for-Speech-to-Text-conversion-especially-for-Indian-English)
* [Username:](mailto:isaac.r.riley@gmail.com)    Password: l^XCWDSxbtC1M%Ft
* [iitm.ac.in/donlab/tts/database.php](https://www.iitm.ac.in/donlab/tts/database.php)
* [Indic TTS - Database](https://www.iitm.ac.in/donlab/tts/database.php)
* [Indic Doc](https://www.iitm.ac.in/donlab/tts/downloads/publications/resources.pdf)
* [csl.uni-bremen.de/GlobalPhone/](https://www.csl.uni-bremen.de/GlobalPhone/)    Speech Accent Archive
* [en.wikipedia.org/wiki/IPA_vowel_chart_with_audio](https://en.wikipedia.org/wiki/IPA_vowel_chart_with_audio)
* [scholars.sil.org/kenneth_s_olson/ipa_illustrations](https://scholars.sil.org/kenneth_s_olson/ipa_illustrations)    Victoria IPA illustrations
* [stylerw/int-ipa](https://github.com/stylerw/int-ipa)
* [forvo.com/](https://forvo.com/)
* [en.wikipedia.org/wiki/International_Phonetic_Alphabet_chart](https://en.wikipedia.org/wiki/International_Phonetic_Alphabet_chart)
* [en.wikipedia.org/wiki/IPA_pulmonic_consonant_chart_with_audio](https://en.wikipedia.org/wiki/IPA_pulmonic_consonant_chart_with_audio)
* [wikt2pron.readthedocs.io/en/latest/usage.html](https://wikt2pron.readthedocs.io/en/latest/usage.html)
* Major questions:
  * how to learn length for transcriptions? simply relative length?
  * how to map phonetic features to letters?
  * how to learn and transfer distribution of sequences (in GAN)
* [Tapakapa Erklärt](https://www.youtube.com/channel/UCCbvX81KC1LCOn8jjsqoR7Q)
* [Schwäbisch schwätza](https://www.schwaebisch-schwaetza.de/)
* [Mundartradio](https://mundartradio.de/)
* [Hubert Klausmann](https://uni-tuebingen.de/fakultaeten/wirtschafts-und-sozialwissenschaftliche-fakultaet/faecher/fachbereich-sozialwissenschaften/empirische-kulturwissenschaft/institut/personen/professorinnen/hubert-klausmann/)
* [Die Welt auf Schwäbisch](https://www.youtube.com/playlist?list=PLnT1rJE3HXMzzCHsaoQTqTCTeahbV59KA)
* [Heinrich Del Core](https://www.youtube.com/channel/UCpyQ0yz5sq1AbLgZSqLrNwQ)
* [Hillu's Herzdropfa - Auf dem Landratsamt Göppingen bei Schwäbische Fasne...](https://youtu.be/VDzBRenNaJ0)
* [Hannes Bürgermeister](https://www.youtube.com/c/HannesB%C3%BCrgermeister)
* [Krisensitzung auf schwäbisch - Christoph Sonntag](https://youtu.be/On7nHUIS5gQ)
* [Klaus Birk](https://www.youtube.com/channel/UCsTV94VvkpsBoIAAKWV0vdA)
* [...](https://github.com/Accent-/auto-youtube-downloader)
* [...](https://vloggergear.com/top-10-best-youtube-to-mp3-converters/)
* [video.google.com/timedtext?lang=en&v=tBBJ2TSTa1Q](http://video.google.com/timedtext?lang=en&v=tBBJ2TSTa1Q)
* [2conv.com/de69/](https://2conv.com/de69/)
* [...](https://gadgets.ndtv.com/how-to/features/download-youtube-videos-playlist-bulk-computer-phone-4k-video-downloader-videoder-2279719)
* [...](https://ontiva.com/de/youtube-to-wav-converter)
* [sourceforge.net/projects/google2srt/files/latest/download](https://sourceforge.net/projects/google2srt/files/latest/download)
* [savesubs.com/de](https://savesubs.com/de)
* [downsub.com/](https://downsub.com/)
* [video.google.com/timedtext?lang=en&v=VqP2tREMvt0&name=CC](http://video.google.com/timedtext?lang=en&v=VqP2tREMvt0&name=CC)
* [video.google.com/timedtext?type=list&v=VqP2tREMvt0](http://video.google.com/timedtext?type=list&v=VqP2tREMvt0)
* [...](https://github.com/search?q=youtube+download)
* [CrypticSignal/av-converter](https://github.com/CrypticSignal/av-converter)    ProtonVPN free
* [...](https://www.tecmint.com/download-mp3-song-from-youtube-videos/)
* [...](https://github.com/ytdl-org/youtube-dl/)
* -> Methods of data-augmentation for speech data
* [(PDF) A cross-linguistic database of phonetic transcription systems](https://www.researchgate.net/publication/334135409_A_cross-linguistic_database_of_phonetic_transcription_systems)
* [(PDF) Non-native speech databases](https://www.researchgate.net/publication/4311062_Non-native_speech_databases)
* [A collection of databases for phonetic purposes](http://www.ipds.uni-kiel.de/links/datenmaterial.en.html)
* [A Database of Non-Native English Accents to Assist Neural Speech Recognition | AccentDB](https://accentdb.org/)    alignment algorithms
* [Content Listing](https://www.cambridge.org/core/journals/journal-of-the-international-phonetic-association/listing)
* [richardbeare.github.io/marijatabain/ipa_illustrations_all.html](https://richardbeare.github.io/marijatabain/ipa_illustrations_all.html)
* [csl.uni-bremen.de/cms/images/documents/publications/SchlippeOchsSchultz_IS10.pdf](https://www.csl.uni-bremen.de/cms/images/documents/publications/SchlippeOchsSchultz_IS10.pdf)
* [tensorflow.org/datasets/catalog/vctk?hl=it](https://www.tensorflow.org/datasets/catalog/vctk?hl=it)
* [Illustrations of the IPA article collection](https://www.cambridge.org/core/journals/journal-of-the-international-phonetic-association/illustrations-of-the-ipa)
* [Illustrations of the IPA published in JIPA 1998–2013 | Journal of the International Phonetic Association | Cambridge Core](https://www.cambridge.org/core/journals/journal-of-the-international-phonetic-association/article/abs/illustrations-of-the-ipa-published-in-jipa-19982013/FDC2BDDE9B14B026BB7690470B00084A)
* [IPA Illustrations | Scholars](https://scholars.sil.org/kenneth_s_olson/ipa_illustrations)
* [Journal of the International Phonetic Association | Cambridge Core](https://www.cambridge.org/core/journals/journal-of-the-international-phonetic-association)
* [NISP: A Multi-lingual Multi-accent Dataset for Speaker Profiling](https://arxiv.org/abs/2007.06021)
* [Speech Accent Archive | Kaggle](https://www.kaggle.com/rtatman/speech-accent-archive)
* [Aligning Text and Phonemes for Speech Technology Applications Using an EM-Like Algorithm](https://link.springer.com/article/10.1007/s10971-005-2166-7)
