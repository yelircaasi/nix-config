# Math Notes

## SORT

* {notes} [Algebraic Structures](https://docs.google.com/spreadsheets/d/1RU8z6sIVkcinPuUW3Im6C2fMvHrU-mgwCOiowgSBQLM/edit?usp=sharing)
* {notes LA} [matrices - Integrating a matrix - Mathematics Stack Exchange](https://math.stackexchange.com/questions/450560/integrating-a-matrix)
notes LA - [*linear algebra - Determinant of transpose? - Mathematics Stack Exchange](https://math.stackexchange.com/questions/598258/determinant-of-transpose/636198#636198)
notes LA - [*linear algebra - What is the geometric interpretation of the transpose? - Mathematics Stack Exchange](https://math.stackexchange.com/questions/37398/what-is-the-geometric-interpretation-of-the-transpose)
notes LEARN HOW TO WRITE BROWSER APPLETS FOR MATH VISUALIZATIONS!
notes Proposals for improved math education - more engaging, more immediately (and eventually) relevant for everyday life, more "mathematical" (set theory, abstract algebra) → state of current discourse and research
* {proof} [Archive of Proofs](https://www.isa-afp.org/)
* {proof} [Category:Proofs by Topic - ProofWiki](https://proofwiki.org/wiki/Category:Proofs_by_Topic)
* {proof} [Proof of the Law of Cosines - Math Open Reference](https://www.mathopenref.com/lawofcosinesproof.html)
* {proof} [proof of Euler's Theorem in complex numbers](https://math.stackexchange.com/questions/1936296/proof-of-eulers-theorem-in-complex-numbers/1936327)
* {proof} [math.hu-berlin.de/~fsr/studium/Beweise_Vorlesung.pdf](http://www.math.hu-berlin.de/~fsr/studium/Beweise_Vorlesung.pdf)
* {proof} [mathematik.de/ger/information/landkarte/stichpunkte/beweis.html](http://www.mathematik.de/ger/information/landkarte/stichpunkte/beweis.html)
* {proof} [proofwiki.org/wiki/Equivalence_of_Definitions_of_Complex_Number](https://proofwiki.org/wiki/Equivalence_of_Definitions_of_Complex_Number)
proof - [*E0 201 : Proofs and Measurs, Fall 2015](https://ece.iisc.ac.in/~parimal/2015/proofs.html)
* power laws
* e^(pi*i)
pi as infinite product
Taylor Series
* A norm |.| is non-Archimedian if |x+y| <= max( |x|, |y| ). Example: p-adic numbers.
Calculus at a Fifth Grade Level
e (Euler's Number) is seriously everywhere | The strange times it shows up and why it's so important
Envelope Theorem
Newton’s “Principia”
* Great question from Sandra: Why do I love mathematics? What makes it so interesting?
* Bézier curves
* → clear proof of Law of Cosines
* → proof of formula for angle between two vectors
* → proof of triangle inequality
* → eigendecomposition to SVD
* → proof of vector projection formula
* Idea: matrix graph: network of matrix types
* → Variations on graph theory, e.g. graph where n>1 nodes are connected by an edge
* Create E-book: Selected Proofs with Detailed Explanation
* Introduction: → Assumed prerequisite background knowledge
* book This book's approach is to work backwards, motivating with advanced results, then gradually building up to them.
* book The reason for creating this is that work in any field of mathematics (or neighboring fields) builds upon a core of key results. In my experience, most of the difficulties faced in mathematics are due to weaknesses or gaps in prerequisite background knowledge. Econometrics was extremely hard for me, not because the material itself was impossibly challenging, but because I lacked the necessary understanding of linear algebra and statistics. Starting out in NLP was hard because I wanted to understand advanced advanced results despite lacking an understanding of their mathematical foundations. It was only when I finally decided to start from the beginning that it finally began to click and I was able to really enjoy it. | This book is for people whose minds work like mine, who:
* insist on using their intuition to "grok" mathematical concepts, but who also want to pair their intuition with mathematical rigor
* find that, in addition to the precision and unambiguity of symbolic proof, stating or sumarizing a proof in natural human language can be quite helpful
* find that math is easier when you have a solid grasp of some key ideas upon which to build
* appreciate detailed, step-by-step explanations and don't feel insulted or talked down to when explanations err on the side of throughness
* book This book mkes no claims to completeness. Some inclusions and omissions may feel arbitrary. Maybe they are.
* Sections:
* Philosophy of Mathematics
* Logic
* Abstract Algebra
* Geometry
* Trigonometry
* Number Theory
* Linear Algebra
* Calculus and Real Analysis
* Multivariable Calculus
* Differential Equations
* Combinatorics
* Probability Theory
* Statistics
* Measure Theory
* Topology
* Digital Mathematics
* Algorithms
* Graph Theory
* Complex Analysis
* Fourier Analysis
* Category Theory
* Selected Applications:
* Physics
* Cryptography

^ Proofs to master:
* Cauchy-Schwarz Inequality
* Triangle Inequality
* Vector Projection Formula
* Constructive Derivation of Generalized Determinant
* Constructive Derivation of Matrix Multiplication
* Law of Cosines
* Pythagorean Theorem
* Taylor Series (generalized)
* Derivated Laws (Power, Quotient, Product, Exponent, Logarithm)
* Jordan Normal Form
* SVD
* LU
* QR
* For each proof:
* why it's important
* where it's used
* necessary background and preliminary vocab (references to read, minimal in-text explanations and definitions)
* proofs it builds on
* proofs that build on it
* formal proof
* verbal proof sketch
* notes on the proof, helpful intuition
* references

| OLS |  |  |
| --- | --- | --- |
|  | OLS-HC |  |
|  | OLS-HAC |  |
|  | Interaction Terms |  |
|  | Dummy Variables |  |
|  | Heteroskedasticity-Robust Standard Errors |  |
|  | Cluster-Robust Standard Errors |  |
|  | Time Series |  |
|  | Panel Data |  |
|  | Fixed Effects |  |
|  | Linearization of non-linear models |  |
|  | Differences-in-Differences |  |
|  | Regression Discontuinity |  |
|  | Segmented Regression |  |
|  |  |  |
|  |  |  |
| Non-Linear Specification |  |  |
|  | Log-Linear |  |
|  | Hyperbolic |  |
|  | Polynomial |  |
|  | Logarithmic |  |
|  | Parabolic |  |
|  |  |  |
| Selection Bias Corrections |  |  |
|  | 2SLS |  |
|  | Control Function Models |  |
|  |  | Heckman Correction |
|  | Censored Normal Regression |  |
|  | Truncated Normal Regression |  |
|  |  |  |
| Matching |  |  |
|  | Propensity Score Matching |  |
|  | Nearest-Neighbor Matching |  |
|  |  |  |
|  |  |  |
| GLS |  |  |
|  | FGLS |  |
|  | WLS |  |
|  | Random Effects |  |
| Method of Moments |  |  |
|  | GMM |  |
|  | Efficient GMM |  |
|  | Nonlinear MM |  |
|  | HAC-Covariance Estimator |  |
|  |  |  |
| Limited dependent variable regressions (discrete choice) |  |  |
|  | Probit |  |
|  |  | ordered |
|  |  | multinomial |
|  | Logit |  |
|  |  | ordered |
|  |  | multinomial |
|  |  | nested |
|  |  | mixed |
|  | Tobit |  |
|  | Heckit |  |
|  | Poisson Regression |  |
|  |  |  |
| Monte Carlo |  |  |
| Jackknife |  |  |
| Bootstrap |  |  |
| MLE |  |  |
| Quantile Regression |  |  |
| ANOVA |  |  |
|  | One-Way |  |
|  | MANOVA |  |
|  |  |  |
| ANCOVA |  |  |
|  | MANCOVA |  |
|  |  |  |
| Multi-Level Models |  |  |
|  | Random intercepts model |  |
|  | Random slopes model |  |
|  | Random intercepts and slopes model |  |
|  |  |  |
|  |  |  |
|  |  |  |
| Factor Analysis |  |  |
|  |  |  |
| ElasticNet Regression |  |  |
| Stepwise Regression |  |  |
|  | forward selection |  |
|  | backward elimination |  |
|  | bidirectional elimination |  |
| Bayesian Regression |  |  |
| Partitioned Regression (Frisch-Waugh-Lovell Theorem) |  |  |
|  |  |  |
| Non-Parametric Regression |  |  |
|  | Kernel Regression |  |
|  | Gaussian Process Regression |  |
| Decision Tree |  |  |
| Discriminant Analysis |  |  |
|  | Partial Least Squares |  |
|  |  |  |
| K Nearest Neighbors |  |  |
| PCR |  |  |
|  |  |  |
| Least-angle Regression |  |  |
| Mixed Models |  |  |
|  |  |  |
| Ridge Regression |  |  |
| Lasso Regression |  |  |
|  |  |  |
|  |  |  |
|  |  |  |
| Robust Regressions |  |  |
|  | LAD Regression |  |
|  | LTS (Deming regression) |  |
|  | Theil-Sen |  |
|  | M-estimation |  |
|  | unit weights |  |
|  |  |  |
| Percentage Regression |  |  |
|  |  |  |
|  |  |  |
* Econometrics Notes

## Roadmaps Notes

* Wikipedia page "Mathematics"
grundsätzliche Rechenarten kennen (Arithmetik)
Überblick verschaffen
einfache Beispielprobleme aus der Geometrie
einfache Beispielprobleme aus der Zahlentheorie
einfache Beispielprobleme aus der Kombinatorik
Grundlagen der mathematischen Logik und der Beweistechniken
Grundlagen der Mengenlehre
Grundlagen der Geometrie
Grundlagen der allgemeinen Algebra (Funktionen, Strukturen, Polynome)
Grundlagen der Zahlentheorie
Grundlagen der reellen Analysis
Grundlagen der Kombinatorik und der Stochastik
elementare Differenzialrechnung einer Variablen
elementare Integralrechnung einer Variablen
elementare Lineare Algebra
allgemeine Algebra. (Strukturen)
reelle Analysis
Grundlagen der Topologie
Trigonometrie
elementare Differenzialrechnung mehrer Variablen
elementare Integralrechnung mehrer Variablen
fortgeschrittene lineare Algebra
komplexe Analysis
Statistik
Differentialgleichungen
Kategorietheorie

## Miscellaneous

* Voici un petit théorème sympa : il n’existe aucun complexe z ∈ C* tel que z et e^z soient simultanément dans Q(i)={a + bi, (a, b) ∈ Q^2}.
* Dummy proof of CRT: Use the relative primeness of the n_i, together with Corrollary 2.5 (au+bv=1) to create a number for each n_i that is divisible by all other n but has a remainder of 1 when divided by n_i. Then multiply that by the desired remainder. Add together the results for each divisor-remainder pair, snf that number is the desired result and is unique modulo N (where N = n_1 * n_2 * n_3 * ... * nn_k).

## Conchoids

point-line-ruler problem (conchoids)
largest and flattest area of contour (surface? better term? → like land)
* {book} [A Collection of Examples of the Applications of the Differential and ... - George Peacock - Google Книги](https://books.google.de/books?id=2f43AAAAMAAJ&pg=PA163&lpg=PA163&dq=applications+of+conchoids&source=bl&ots=8n69X6VyAv&sig=ACfU3U3MAvwYQZnMlD5gCiQwI_edL5Iaew&hl=ru&sa=X&ved=2ahUKEwj-7I_HppzoAhUMIMUKHY8mCo4Q6AEwCXoECAkQAQ#v=onepage&q=applications%20of%20conchoids&f=false)
* {book} [THE DERIVATION AND APPLICATIONS OF THE CONCHOID OF NICOMEDES AND THE CISSOID OF DIOCLES - Roeser - 1914 - School Science and Mathematics - Wiley Online Library](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1949-8594.1914.tb17589.x)
* [Rational Conchoids of Algebraic Curves](https://arxiv.org/pdf/0901.4652.pdf)
* [An Algebraic Analysis of Conchoids to Algebraic Curves](https://arxiv.org/pdf/0705.4590.pdf)
* [Conchoid of de Sluze - Wikipedia](https://en.wikipedia.org/wiki/Conchoid_of_de_Sluze)
* [Conchoid of Dürer - Wikipedia](https://en.wikipedia.org/wiki/Conchoid_of_D%C3%BCrer)
Analyse geometry of all points where a point is a fixed distance from a point on a line and the segment connecting the two points runs through the same point for all pairs

## NN Implementation

* [CNN](https://medium.com/deep-math-machine-learning-ai/chapter-8-1-code-for-convolutional-neural-networks-tensorflow-and-keras-theano-33bef285dd93)
* [Great one](https://thecodacus.com/neural-network-scratch-python-no-libraries/)
* [Decent Example KDNuggets](https://www.kdnuggets.com/2018/10/simple-neural-network-python.html)
How can I describe mathematically everything under the hod in an ANN?
* [Beginner Guide to Math of NN](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.161.3556&rep=rep1&type=pdf)
* [Wikipedia Backprop](https://en.wikipedia.org/wiki/Backpropagation#Finding_the_derivative_of_the_error)
* [Backprop - deriv of softmax](https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function)
* [Great overview](http://home.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html)
* How can I create my own NN in Tensorflow?
* What is the general algorithm for computing the eigendecomposition?
What is the general algorithm for computing the Q-R decompostion?
What is the general algorithm for computing the LU decomposition?
General:
* [Great Blog](https://medium.com/@madhusanjeevi.ai)
* [Another good example](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/)
* [Good NB](https://github.com/dennybritz/nn-from-scratch/blob/master/nn-from-scratch.ipynb)
* [Mario NN Code](https://pastebin.com/ZZmSNaHX)
* [~Same Blog](https://medium.com/deep-math-machine-learning-ai)
* [Vector/Matrix/Tensor Derivatives](http://cs231n.stanford.edu/vecDerivs.pdf)
* [es.wikipedia.org/wiki/Matem%C3%A1ticas](https://es.wikipedia.org/wiki/Matem%C3%A1ticas)

```python
* import numpy as np
* import matplotlib.pyplot as plt
* def softmax(A):
* expA = np.exp(A)
* return expA / expA.sum(axis=1, keepdims=True)
* def sigmoid(x):
* return 1 / (1 + np.exp(-x))
* def softmax(A):
* expA = np.exp(A)
* return expA / expA.sum(axis=1, keepdims=True)
* np.random.seed(1)

# generate three Gaussian clouds each holding 500 points

* X1 = np.random.randn(500, 2) + np.array([0, -2])
* X2 = np.random.randn(500, 2) + np.array([2, 2])
* X3 = np.random.randn(500, 2) + np.array([-2, 2])

# put them all in a big matrix

* X = np.vstack([X1, X2, X3])

# generate the one-hot-encodings

* labels = np.array([0]*500 + [1]*500 + [2]*500)
* T = np.zeros((1500, 3))
* for i in range(1500):
* T[i, labels[i]] = 1

# visualize the data

* plt.scatter(X[:,0], X[:,1], c=labels, s=100, alpha=0.5)
* plt.show()
* samples = X.shape[0] # 1500 samples
* features = X.shape[1] # 2 features
* hidden_nodes = 5
* classes = 3

# randomly initialize weights

* W1 = np.random.randn(features, hidden_nodes)
* b1 = np.random.randn(hidden_nodes)
* W2 = np.random.randn(hidden_nodes, classes)
* b2 = np.random.randn(classes)
* alpha = 10e-6
* costs = []
* for epoch in range(10000):
# forward pass

* A = sigmoid(X.dot(W1) + b1) # A = sigma(Z)
* Y = softmax(A.dot(W2) + b2) # Y = softmax(Z2)

# backward pass

* delta2 = Y - T
* delta1 = (delta2).dot(W2.T) * A * (1 - A)
* W2 -= alpha * A.T.dot(delta2)
* b2 -= alpha * (delta2).sum(axis=0)
* W1 -= alpha * X.T.dot(delta1)
* b1 -= alpha * (delta1).sum(axis=0)

# save loss function values across training iterations

* if epoch % 100 == 0:
loss = np.sum(-T * np.log(Y))
print('Loss function value: ', loss)
costs.append(loss)
* plt.plot(costs)
* plt.show()
```

## Matrix Types

```txt
* [Template:Matrix classes - Wikipedia](https://en.wikipedia.org/wiki/Template:Matrix_classes)
* [Matrix Mathematics](https://drive.google.com/file/d/1-NJvYoUxIpsh7b10Qri9KkZ13sOwQYG3/view?usp=sharing)
* [List of named matrices - Wikipedia](https://en.wikipedia.org/wiki/List_of_named_matrices)
* [Matrix Types](...)
|  |  |  |  | Square? | Symmetric? | Cross-symmetric? | Triangular? | Diagonal? | Block-Diagonal? | Constraint on all entries? | Normalized? | Orthogonal? | Band? | Identity Qualifies? |  |  |  | Applications | Prerequisite Concepts |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| [en.wikipedia.org/wiki/Logical_matrix](https://en.wikipedia.org/wiki/Logical_matrix) | A matrix with all elements either 0 or 1. | Synonym forbinary matrix or logical matrix. |  | * | * | * | * | * | * | ✓ | * | * | * | ✓ |  |  |  |  |  |
| [en.wikipedia.org/wiki/Logical_matrix](https://en.wikipedia.org/wiki/Logical_matrix) | A matrix whose entries are all either 0 or 1. | Synonym for (0,1)-matrix or logical matrix.[1] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Elementary_matrix](https://en.wikipedia.org/wiki/Elementary_matrix) | A square matrix derived by applying an elementary row operation to the identity matrix. |  |  | ✓ | ? | ? | ✓ | ? | ? | x | ? | ✓ | ? | ✓ |  |  |  |  |  |
| [en.wikipedia.org/wiki/Block_matrix](https://en.wikipedia.org/wiki/Block_matrix) | A matrix partitioned in sub-matrices called blocks. |  |  | ? | ? |  | ? | ? | ? | ? | ? | ? | ? | ✓ |  |  |  |  |  |
| [en.wikipedia.org/wiki/Nonnegative_matrix](https://en.wikipedia.org/wiki/Nonnegative_matrix) | A matrix with all nonnegative entries. |  |  | ? | ? | ? | ? | ? | ? | ✓ | ? | ? | ? | ✓ |  |  |  |  |  |
| [en.wikipedia.org/wiki/Diagonally_dominant_matrix](https://en.wikipedia.org/wiki/Diagonally_dominant_matrix) | A matrix whose entries satisfy{\displaystyle |a_{ii}|>\sum _{j\neq i}|a_{ij}|}. |  |  | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ✓ |  |  |  |  |  |
| [en.wikipedia.org/wiki/Diagonal_matrix](https://en.wikipedia.org/wiki/Diagonal_matrix) | [en.wikipedia.org/wiki/Main_diagonal](https://en.wikipedia.org/wiki/Main_diagonal) |  |  | ✓ | ✓ | ? | ✓ | ✓ | ✓ | ? | ? | ✓ | ✓ | ✓ |  |  |  |  |  |
| [en.wikipedia.org/wiki/Band_matrix](https://en.wikipedia.org/wiki/Band_matrix) | A square matrix whose non-zero entries are confined to a diagonalband. |  |  | ✓ | ? | ? | ✓ | ? | ? | ? | ? | ? | ✓ | ✓ |  |  |  |  |  |
| Positive Definite matrix | A square, symmetric matrix A such that x'Ax > 0 |  |  | ✓ | ✓ | ? | ? | ? | ? | ? | ? | ? | ? | ✓ |  |  |  |  |  |
| Positive semidefinite matrix | A square, symmetric matrix A such that x'Ax >= 0 |  |  | ✓ | ✓ | ? | ? | ? | ? | ? | ? | ? | ? | ✓ |  |  |  |  |  |
| Negative definite matrix | A square, symmetric matrix A such that x'Ax < 0 |  |  | ✓ | ✓ | ? | ? | ? | ? | ? | ? | ? | ? | ✓ |  |  |  |  |  |
| Negative Semidefinite matrix | A square, symmetric matrix A such that x'Ax <= 0 |  |  | ✓ | ✓ | ? | ? | ? | ? | ? | ? | ? | ? | ✓ |  |  |  |  |  |
| [en.wikipedia.org/wiki/Boolean_matrix](https://en.wikipedia.org/wiki/Boolean_matrix) | A matrix whose entries are taken from a Boolean algebra. |  |  | ? | ? | ? | ? | ? | ? | ✓ | ? | ? | ? | ✓ |  |  |  |  |  |
| [en.wikipedia.org/wiki/Centrosymmetric_matrix](https://en.wikipedia.org/wiki/Centrosymmetric_matrix) | A matrix symmetric about its center; i.e.,aij = an−i+1,n−j+1. |  |  | ✓ | ? | ? | ✘ | ? | ? | ? | ? | ? | ? | ✓ |  |  |  |  |  |
| [en.wikipedia.org/wiki/Hermitian_matrix](https://en.wikipedia.org/wiki/Hermitian_matrix) | [en.wikipedia.org/wiki/Conjugate_transpose](https://en.wikipedia.org/wiki/Conjugate_transpose) |  |  | ✓ | ✓ | ? | ? | ? | ? | ? | ? | ? | ? | ✓ |  |  |  |  |  |
| [en.wikipedia.org/wiki/Bidiagonal_matrix](https://en.wikipedia.org/wiki/Bidiagonal_matrix) | A matrix with elements only on the main diagonal and either the superdiagonal or subdiagonal. | Sometimes defined differently, see article. |  |  |  |  |  |  |  |  |  |  |  | ✘ |  |  |  |  |  |
| [en.wikipedia.org/wiki/Cauchy_matrix](https://en.wikipedia.org/wiki/Cauchy_matrix) | A matrix whose elements are of the form 1/(xi+yj) for (xi), (yj) injective sequences (i.e., taking every value only once). |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Alternant_matrix](https://en.wikipedia.org/wiki/Alternant_matrix) | A matrix in which successive columns have a particular function applied to their entries. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Anti-diagonal_matrix](https://en.wikipedia.org/wiki/Anti-diagonal_matrix) | A square matrix with all entries off the anti-diagonal equal to zero. |  |  | ✓ |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Skew-Hermitian_matrix](https://en.wikipedia.org/wiki/Skew-Hermitian_matrix) | Synonym for skew-Hermitian matrix. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Skew-symmetric_matrix](https://en.wikipedia.org/wiki/Skew-symmetric_matrix) | Synonym for skew-symmetric matrix. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Arrowhead_matrix](https://en.wikipedia.org/wiki/Arrowhead_matrix) | A square matrix containing zeros in all entries except for the first row, first column, and main diagonal. |  |  | ✓ |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Bisymmetric_matrix](https://en.wikipedia.org/wiki/Bisymmetric_matrix) | A square matrix that is symmetric with respect to its main diagonal and its main cross-diagonal. |  |  | ✓ | ✓ |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Block-diagonal_matrix](https://en.wikipedia.org/wiki/Block-diagonal_matrix) | [en.wikipedia.org/wiki/Block_matrix](https://en.wikipedia.org/wiki/Block_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Block_tridiagonal_matrix](https://en.wikipedia.org/wiki/Block_tridiagonal_matrix) | A block matrix which is essentially a tridiagonal matrix but with submatrices in place of scalar elements. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Conference_matrix](https://en.wikipedia.org/wiki/Conference_matrix) | A square matrix with zero diagonal and +1 and −1 off the diagonal, such that CTC is a multiple of the identity matrix. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Complex_Hadamard_matrix](https://en.wikipedia.org/wiki/Complex_Hadamard_matrix) | A matrix with all rows and columns mutually orthogonal, whose entries are unimodular. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Compound_matrix](https://en.wikipedia.org/wiki/Compound_matrix) | A matrix whose entries are generated by the determinants of all minors of a matrix. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Copositive_matrix](https://en.wikipedia.org/wiki/Copositive_matrix) | A square matrixAwith real coefficients, such that{\displaystyle f(x)=x^{T}Ax}is nonnegative for every nonnegative vectorx |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/DFT_matrix](https://en.wikipedia.org/wiki/DFT_matrix) | Multiplying by a vector gives the DFT of the vector as result. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Equivalent_matrix](https://en.wikipedia.org/wiki/Equivalent_matrix) | A matrix that can be derived from another matrix through a sequence of elementary row or column operations. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Frobenius_matrix](https://en.wikipedia.org/wiki/Frobenius_matrix) | A square matrix in the form of an identity matrix but with arbitrary entries in one column below the main diagonal. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Generalized_permutation_matrix](https://en.wikipedia.org/wiki/Generalized_permutation_matrix) | A square matrix with precisely one nonzero element in each row and column. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Hadamard_matrix](https://en.wikipedia.org/wiki/Hadamard_matrix) | A square matrix with entries +1, −1 whose rows are mutually orthogonal. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Hankel_matrix](https://en.wikipedia.org/wiki/Hankel_matrix) | A matrix with constant skew-diagonals; also an upside down Toeplitz matrix. | A square Hankel matrix is symmetric. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Hessenberg_matrix](https://en.wikipedia.org/wiki/Hessenberg_matrix) | An "almost" triangular matrix, for example, an upper Hessenberg matrix has zero entries below the first subdiagonal. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Hollow_matrix](https://en.wikipedia.org/wiki/Hollow_matrix) | A square matrix whose main diagonal comprises only zero elements. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Integer_matrix](https://en.wikipedia.org/wiki/Integer_matrix) | A matrix whose entries are all integers. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Logical_matrix](https://en.wikipedia.org/wiki/Logical_matrix) | A matrix with all entries either 0 or 1. | [en.wikipedia.org/wiki/Relation_(mathematics)](https://en.wikipedia.org/wiki/Relation_(mathematics)) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Markov_matrix](https://en.wikipedia.org/wiki/Markov_matrix) | A matrix of non-negative real numbers, such that the entries in each row sum to 1. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Metzler_matrix](https://en.wikipedia.org/wiki/Metzler_matrix) | A matrix whose off-diagonal entries are non-negative. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Generalized_permutation_matrix](https://en.wikipedia.org/wiki/Generalized_permutation_matrix) | A square matrix with exactly one non-zero entry in each row and column. | Synonym for generalized permutation matrix. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Moore_matrix](https://en.wikipedia.org/wiki/Moore_matrix) | A row consists ofa,aq,aq², etc., and each row uses a different variable. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Block_matrix](https://en.wikipedia.org/wiki/Block_matrix) | A matrix partitioned into sub-matrices, or equivalently, a matrix whose entries are themselves matrices rather than scalars. | Synonym for block matrix. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/w/index.php?title=Parisi_matrix&action=edit&redlink=1](https://en.wikipedia.org/w/index.php?title=Parisi_matrix&action=edit&redlink=1) | A block-hierarchical matrix. It consist of growing blocks placed along the diagonal, each block is itself a Parisi matrix of a smaller size. | In theory of spin-glasses is also known as a replica matrix. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Pentadiagonal_matrix](https://en.wikipedia.org/wiki/Pentadiagonal_matrix) | A matrix with the only nonzero entries on the main diagonal and the two diagonals just above and below the main one. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Permutation_matrix](https://en.wikipedia.org/wiki/Permutation_matrix) | [en.wikipedia.org/wiki/Permutation](https://en.wikipedia.org/wiki/Permutation) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Persymmetric_matrix](https://en.wikipedia.org/wiki/Persymmetric_matrix) | A matrix that is symmetric about its northeast-southwest diagonal, i.e.,aij = an−j+1,n−i+1. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Polynomial_matrix](https://en.wikipedia.org/wiki/Polynomial_matrix) | [en.wikipedia.org/wiki/Polynomial](https://en.wikipedia.org/wiki/Polynomial) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Positive_matrix](https://en.wikipedia.org/wiki/Positive_matrix) | A matrix with all positive entries. |  |  | ? | ? | ? | ? | ? | ? | ✓ | ? | ? | ? | ✓ |  |  |  |  |  |
| [en.wikipedia.org/wiki/Quaternionic_matrix](https://en.wikipedia.org/wiki/Quaternionic_matrix) | [en.wikipedia.org/wiki/Quaternion](https://en.wikipedia.org/wiki/Quaternion) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Sign_matrix](https://en.wikipedia.org/wiki/Sign_matrix) | A matrix whose entries are either +1, 0, or −1. |  |  | ? | ? | ? | ? | ? | ? | ✓ | ? | ? | ? | ✓ |  |  |  |  |  |
| [en.wikipedia.org/wiki/Signature_matrix](https://en.wikipedia.org/wiki/Signature_matrix) | A diagonal matrix where the diagonal elements are either +1 or −1. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Single-entry_matrix](https://en.wikipedia.org/wiki/Single-entry_matrix) | A matrix where a single element is one and the rest of the elements are zero. |  |  | ? | ? | ? | ? | ? | ? | ✓ | ? | ? | ? | ? |  |  |  |  |  |
| [en.wikipedia.org/wiki/Skew-Hermitian_matrix](https://en.wikipedia.org/wiki/Skew-Hermitian_matrix) | [en.wikipedia.org/wiki/Conjugate_transpose](https://en.wikipedia.org/wiki/Conjugate_transpose) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Skew-symmetric_matrix](https://en.wikipedia.org/wiki/Skew-symmetric_matrix) | [en.wikipedia.org/wiki/Transpose](https://en.wikipedia.org/wiki/Transpose) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Skyline_matrix](https://en.wikipedia.org/wiki/Skyline_matrix) | A rearrangement of the entries of a banded matrix which requires less space. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Sparse_matrix](https://en.wikipedia.org/wiki/Sparse_matrix) | A matrix with relatively few non-zero elements. | Sparse matrix algorithms can tackle huge sparse matrices that are utterly impractical for dense matrix algorithms. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Sylvester_matrix](https://en.wikipedia.org/wiki/Sylvester_matrix) | A square matrix whose entries come from coefficients of two polynomials. | [en.wikipedia.org/wiki/Coprime](https://en.wikipedia.org/wiki/Coprime) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Symmetric_matrix](https://en.wikipedia.org/wiki/Symmetric_matrix) | [en.wikipedia.org/wiki/Transpose](https://en.wikipedia.org/wiki/Transpose) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Toeplitz_matrix](https://en.wikipedia.org/wiki/Toeplitz_matrix) | A matrix with constant diagonals. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Triangular_matrix](https://en.wikipedia.org/wiki/Triangular_matrix) | A matrix with all entries above the main diagonal equal to zero (lower triangular) or with all entries below the main diagonal equal to zero (upper triangular). |  |  |  | * | * | ✓ | * | ✓ | * | * | * | ✓ | ✓ |  |  |  |  |  |
| [en.wikipedia.org/wiki/Tridiagonal_matrix](https://en.wikipedia.org/wiki/Tridiagonal_matrix) | A matrix with the only nonzero entries on the main diagonal and the diagonals just above and below the main one. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Vandermonde_matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix) | A row consists of 1,a,a2,a3, etc., and each row uses a different variable. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Walsh_matrix](https://en.wikipedia.org/wiki/Walsh_matrix) | A square matrix, with dimensions a power of 2, the entries of which are +1 or −1, and the property that the dot product of any two distinct rows (or columns) is zero. |  |  |  |  |  |  |  | ✓ |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Z-matrix_(mathematics)](https://en.wikipedia.org/wiki/Z-matrix_(mathematics)) | A matrix with all off-diagonal entries less than zero. |  |  | * | * |  | * | * | ✓ |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Exchange_matrix](https://en.wikipedia.org/wiki/Exchange_matrix) | [en.wikipedia.org/wiki/Binary_matrix](https://en.wikipedia.org/wiki/Binary_matrix) | aij= δn+1−i,j | [en.wikipedia.org/wiki/Permutation_matrix](https://en.wikipedia.org/wiki/Permutation_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Hilbert_matrix](https://en.wikipedia.org/wiki/Hilbert_matrix) |  | aij = (i + j − 1)−1. | [en.wikipedia.org/wiki/Hankel_matrix](https://en.wikipedia.org/wiki/Hankel_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Identity_matrix](https://en.wikipedia.org/wiki/Identity_matrix) | A square diagonal matrix, with all entries on the main diagonal equal to 1, and the rest 0. | aij= δij |  | ✓ | ✓ |  | ✓ | ✓ | ✓ |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Lehmer_matrix](https://en.wikipedia.org/wiki/Lehmer_matrix) |  | aij= min(i,j) ÷ max(i,j). | Apositivesymmetric matrix. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Matrix_of_ones](https://en.wikipedia.org/wiki/Matrix_of_ones) | A matrix with all entries equal to one. | aij= 1. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Pascal_matrix](https://en.wikipedia.org/wiki/Pascal_matrix) | [en.wikipedia.org/wiki/Pascal%27s_triangle](https://en.wikipedia.org/wiki/Pascal%27s_triangle) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Pauli_matrices](https://en.wikipedia.org/wiki/Pauli_matrices) | A set of three 2 × 2 complex Hermitian and unitary matrices. When combined with theI2identity matrix, they form an orthogonal basis for the 2 × 2 complex Hermitian matrices. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Redheffer_matrix](https://en.wikipedia.org/wiki/Redheffer_matrix) |  | aijare 1 ifidividesjor ifj= 1; otherwise,aij= 0. | A (0, 1)-matrix. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Shift_matrix](https://en.wikipedia.org/wiki/Shift_matrix) | A matrix with ones on the superdiagonal or subdiagonal and zeroes elsewhere. | aij= δi+1,joraij= δi−1,j | Multiplication by it shifts matrix elements by one position. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Zero_matrix](https://en.wikipedia.org/wiki/Zero_matrix) | A matrix with all entries equal to zero. | aij= 0. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Companion_matrix](https://en.wikipedia.org/wiki/Companion_matrix) | A matrix whose eigenvalues are equal to the roots of the polynomial. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Convergent_matrix](https://en.wikipedia.org/wiki/Convergent_matrix) | [en.wikipedia.org/wiki/Zero_matrix](https://en.wikipedia.org/wiki/Zero_matrix) | [en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Defective_matrix](https://en.wikipedia.org/wiki/Defective_matrix) | A square matrix that does not have a complete basis ofeigenvectors, and is thus notdiagonalisable. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Diagonalizable_matrix](https://en.wikipedia.org/wiki/Diagonalizable_matrix) | [en.wikipedia.org/wiki/Similar_matrix](https://en.wikipedia.org/wiki/Similar_matrix) | It has aneigenbasis, that is, a complete set oflinearly independenteigenvectors. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Hurwitz_matrix](https://en.wikipedia.org/wiki/Hurwitz_matrix) | A matrix whose eigenvalues have strictly negative real part. A stable system of differential equations may be represented by a Hurwitz matrix. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/M-matrix](https://en.wikipedia.org/wiki/M-matrix) | A Z-matrix with eigenvalues whose real parts are nonnegative. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Positive-definite_matrix](https://en.wikipedia.org/wiki/Positive-definite_matrix) | A Hermitian matrix with every eigenvalue positive. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Stability_matrix](https://en.wikipedia.org/wiki/Stability_matrix) |  | [en.wikipedia.org/wiki/Hurwitz_matrix](https://en.wikipedia.org/wiki/Hurwitz_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Stieltjes_matrix](https://en.wikipedia.org/wiki/Stieltjes_matrix) | A real symmetric positive definite matrix with nonpositive off-diagonal entries. | [en.wikipedia.org/wiki/M-matrix](https://en.wikipedia.org/wiki/M-matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Circular matrixorConinvolutory matrix | A matrix whose inverse is equal to its entrywise complex conjugate:A−1=A. | Compare with unitary matrices. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Matrix_congruence](https://en.wikipedia.org/wiki/Matrix_congruence) | Two matricesAandBare congruent if there exists an invertible matrixPsuch thatPTAP=B. | Compare with similar matrices. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/EP_matrix](https://en.wikipedia.org/wiki/EP_matrix) | [en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Idempotent matrixorProjection Matrix | A matrix that has the propertyA² =AA=A. | [en.wikipedia.org/wiki/Projection_(linear_algebra)#Properties_and_classification
        [en.wikipedia.org/wiki/Projection_(linear_algebra)#Properties_and_classification](https://en.wikipedia.org/wiki/Projection_(linear_algebra)#Properties_and_classification](https://en.wikipedia.org/wiki/Projection_(linear_algebra)#Properties_and_classification](https://en.wikipedia.org/wiki/Projection_(linear_algebra)#Properties_and_classification)
        [en.wikipedia.org/wiki/Projection_(linear_algebra)#Properties_and_classification)](https://en.wikipedia.org/wiki/Projection_(linear_algebra)#Properties_and_classification)) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Invertible_matrix](https://en.wikipedia.org/wiki/Invertible_matrix) | [en.wikipedia.org/wiki/Inverse_matrix](https://en.wikipedia.org/wiki/Inverse_matrix) | [en.wikipedia.org/wiki/General_linear_group](https://en.wikipedia.org/wiki/General_linear_group) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Involutory_matrix](https://en.wikipedia.org/wiki/Involutory_matrix) | A square matrix which is its own inverse, i.e.,AA=I. | Signature matrices,Householder Matrices(Also known as 'reflection matrices'to reflect a point about a plane or line) have this property. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Nilpotent_matrix](https://en.wikipedia.org/wiki/Nilpotent_matrix) | A square matrix satisfyingAq= 0 for some positive integerq. | Equivalently, the only eigenvalue ofAis 0. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Normal_matrix](https://en.wikipedia.org/wiki/Normal_matrix) | [en.wikipedia.org/wiki/Conjugate_transpose](https://en.wikipedia.org/wiki/Conjugate_transpose) | [en.wikipedia.org/wiki/Spectral_theorem](https://en.wikipedia.org/wiki/Spectral_theorem) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Orthogonal_matrix](https://en.wikipedia.org/wiki/Orthogonal_matrix) | [en.wikipedia.org/wiki/Transpose](https://en.wikipedia.org/wiki/Transpose) | [en.wikipedia.org/wiki/Orthogonal_group](https://en.wikipedia.org/wiki/Orthogonal_group) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Orthonormal_matrix](https://en.wikipedia.org/wiki/Orthonormal_matrix) | [en.wikipedia.org/wiki/Orthonormal](https://en.wikipedia.org/wiki/Orthonormal) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Singular_matrix](https://en.wikipedia.org/wiki/Singular_matrix) | A square matrix that is not invertible. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Unimodular_matrix](https://en.wikipedia.org/wiki/Unimodular_matrix) | [en.wikipedia.org/wiki/Integer_matrix](https://en.wikipedia.org/wiki/Integer_matrix) | Necessarily the determinant is +1 or −1. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Unipotent_matrix](https://en.wikipedia.org/wiki/Unipotent_matrix) | A square matrix with all eigenvalues equal to 1. | [en.wikipedia.org/wiki/Unipotent_group](https://en.wikipedia.org/wiki/Unipotent_group) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Unitary_matrix](https://en.wikipedia.org/wiki/Unitary_matrix) | [en.wikipedia.org/wiki/Conjugate_transpose](https://en.wikipedia.org/wiki/Conjugate_transpose) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Totally_unimodular_matrix](https://en.wikipedia.org/wiki/Totally_unimodular_matrix) | A matrix for which every non-singular square submatrix isunimodular. This has some implications in thelinear programmingrelaxationof aninteger program. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Weighing_matrix](https://en.wikipedia.org/wiki/Weighing_matrix) | A square matrix the entries of which are in{0, 1, −1}, such thatAAT=wIfor some positive integerw. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Adjugate_matrix](https://en.wikipedia.org/wiki/Adjugate_matrix) | [en.wikipedia.org/wiki/Minor_(linear_algebra)](https://en.wikipedia.org/wiki/Minor_(linear_algebra)) | Calculatinginverse matricesviaLaplace expansion. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Alternating_sign_matrix](https://en.wikipedia.org/wiki/Alternating_sign_matrix) | A square matrix of with entries 0, 1 and −1 such that the sum of each row and column is 1 and the nonzero entries in each row and column alternate in sign. | [en.wikipedia.org/wiki/Dodgson_condensation](https://en.wikipedia.org/wiki/Dodgson_condensation) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Augmented_matrix](https://en.wikipedia.org/wiki/Augmented_matrix) | A matrix whose rows are concatenations of the rows of two smaller matrices. | [en.wikipedia.org/wiki/Inverse_matrix](https://en.wikipedia.org/wiki/Inverse_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/B%C3%A9zout_matrix](https://en.wikipedia.org/wiki/B%C3%A9zout_matrix) | A square matrix which may be used as a tool for the efficient location of polynomial zeros | Control theory,Stable polynomials |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Carleman_matrix](https://en.wikipedia.org/wiki/Carleman_matrix) | A matrix that converts composition of functions to multiplication of matrices. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Cartan_matrix](https://en.wikipedia.org/wiki/Cartan_matrix) | A matrix associated with a finite-dimensionalassociative algebra, or asemisimple Lie algebra(the two meanings are distinct). |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Circulant_matrix](https://en.wikipedia.org/wiki/Circulant_matrix) | A matrix where each row is a circular shift of its predecessor. | System of linear equations,discrete Fourier transform |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Cofactor_matrix](https://en.wikipedia.org/wiki/Cofactor_matrix) | A containing thecofactors, i.e., signedminors, of a given matrix. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Commutation_matrix](https://en.wikipedia.org/wiki/Commutation_matrix) | A matrix used for transforming the vectorized form of a matrix into the vectorized form of its transpose. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Coxeter_matrix](https://en.wikipedia.org/wiki/Coxeter_matrix) | A matrix related toCoxeter groups, which describesymmetriesin a structure or system. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/w/index.php?title=Derogatory_matrix&action=edit&redlink=1](https://en.wikipedia.org/w/index.php?title=Derogatory_matrix&action=edit&redlink=1) | [en.wikipedia.org/wiki/Minimal_polynomial_(linear_algebra)](https://en.wikipedia.org/wiki/Minimal_polynomial_(linear_algebra)) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Distance_matrix](https://en.wikipedia.org/wiki/Distance_matrix) | [en.wikipedia.org/wiki/Point_(geometry)](https://en.wikipedia.org/wiki/Point_(geometry)) | Computer vision,network analysis. | [en.wikipedia.org/wiki/Euclidean_distance_matrix](https://en.wikipedia.org/wiki/Euclidean_distance_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Duplication_matrix](https://en.wikipedia.org/wiki/Duplication_matrix) | [en.wikipedia.org/wiki/Vectorization_(mathematics)](https://en.wikipedia.org/wiki/Vectorization_(mathematics)) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Elimination_matrix](https://en.wikipedia.org/wiki/Elimination_matrix) | [en.wikipedia.org/wiki/Vectorization_(mathematics)](https://en.wikipedia.org/wiki/Vectorization_(mathematics)) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Euclidean_distance_matrix](https://en.wikipedia.org/wiki/Euclidean_distance_matrix) | A matrix that describes the pairwise distances betweenpointsinEuclidean space. |  | [en.wikipedia.org/wiki/Distance_matrix](https://en.wikipedia.org/wiki/Distance_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Fundamental_matrix_(linear_differential_equation)](https://en.wikipedia.org/wiki/Fundamental_matrix_(linear_differential_equation)) | [en.wikipedia.org/wiki/Ordinary_differential_equation](https://en.wikipedia.org/wiki/Ordinary_differential_equation) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Generator_matrix](https://en.wikipedia.org/wiki/Generator_matrix) | [en.wikipedia.org/wiki/Linear_code](https://en.wikipedia.org/wiki/Linear_code) | [en.wikipedia.org/wiki/Coding_theory](https://en.wikipedia.org/wiki/Coding_theory) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Gramian_matrix](https://en.wikipedia.org/wiki/Gramian_matrix) | [en.wikipedia.org/wiki/Inner_product_space](https://en.wikipedia.org/wiki/Inner_product_space) | Testlinear independenceof vectors, including ones infunction spaces. | They are real symmetric. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Hessian_matrix](https://en.wikipedia.org/wiki/Hessian_matrix) | [en.wikipedia.org/wiki/Partial_derivative](https://en.wikipedia.org/wiki/Partial_derivative) | Detectinglocal minimaand maxima of scalar-valued functions in several variables;Blob detection(computer vision) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Householder_transformation](https://en.wikipedia.org/wiki/Householder_transformation) | A transformation matrix widely used in matrix algorithms. | [en.wikipedia.org/wiki/QR_decomposition](https://en.wikipedia.org/wiki/QR_decomposition) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Jacobian_matrix](https://en.wikipedia.org/wiki/Jacobian_matrix) | A matrix of first-order partial derivatives of a vector-valued function. | Implicit function theorem;Smooth morphisms(algebraic geometry). |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Moment_matrix](https://en.wikipedia.org/wiki/Moment_matrix) | [en.wikipedia.org/wiki/Monomials](https://en.wikipedia.org/wiki/Monomials) | [en.wikipedia.org/wiki/Sum-of-squares_optimization](https://en.wikipedia.org/wiki/Sum-of-squares_optimization) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Payoff_matrix](https://en.wikipedia.org/wiki/Payoff_matrix) | A matrix ingame theoryandeconomics, that represents the payoffs in anormal form gamewhere players move simultaneously |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Pick_matrix](https://en.wikipedia.org/wiki/Pick_matrix) | A matrix that occurs in the study of analytical interpolation problems. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Random_matrix](https://en.wikipedia.org/wiki/Random_matrix) | [en.wikipedia.org/wiki/Random_distribution](https://en.wikipedia.org/wiki/Random_distribution) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Rotation_matrix](https://en.wikipedia.org/wiki/Rotation_matrix) | A matrix representing a rotational geometric transformation. | Special orthogonal group,Euler angles |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Seifert_matrix](https://en.wikipedia.org/wiki/Seifert_matrix) | [en.wikipedia.org/wiki/Knot_theory](https://en.wikipedia.org/wiki/Knot_theory) | [en.wikipedia.org/wiki/Alexander_polynomial](https://en.wikipedia.org/wiki/Alexander_polynomial) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Shear_matrix](https://en.wikipedia.org/wiki/Shear_matrix) | [en.wikipedia.org/wiki/Shear_transformation](https://en.wikipedia.org/wiki/Shear_transformation) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Similarity_matrix](https://en.wikipedia.org/wiki/Similarity_matrix) | A matrix of scores which express the similarity between two data points. | [en.wikipedia.org/wiki/Sequence_alignment](https://en.wikipedia.org/wiki/Sequence_alignment) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Symplectic_matrix](https://en.wikipedia.org/wiki/Symplectic_matrix) | A square matrix preserving a standard skew-symmetric form. | Symplectic group,symplectic manifold. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Totally_positive_matrix](https://en.wikipedia.org/wiki/Totally_positive_matrix) | [en.wikipedia.org/wiki/Determinant](https://en.wikipedia.org/wiki/Determinant) | Generating the reference points ofBézier curveincomputer graphics. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Transformation_matrix](https://en.wikipedia.org/wiki/Transformation_matrix) | [en.wikipedia.org/wiki/Linear_transformation](https://en.wikipedia.org/wiki/Linear_transformation) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/w/index.php?title=Wedderburn_matrix&action=edit&redlink=1](https://en.wikipedia.org/w/index.php?title=Wedderburn_matrix&action=edit&redlink=1) | A matrix of the form{\displaystyle A-(y^{T}Ax)^{-1}Axy^{T}A}, used for rank-reduction & biconjugate decompositions | Analysis of matrix decompositions |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/X-Y-Z_matrix](https://en.wikipedia.org/wiki/X-Y-Z_matrix) | A generalisation of the (rectangular) matrix to a cuboidal form (a 3-dimensional array of entries). |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Bernoulli matrix — a square matrix with entries +1, −1, with equal probability of each. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Centering_matrix](https://en.wikipedia.org/wiki/Centering_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Correlation matrix — a symmetric n×n matrix, formed by the pairwise correlation coefficients of several random variables. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Covariance matrix — a symmetric n×n matrix, formed by the pairwise covariances of several random variables. Sometimes called a dispersion matrix. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Dispersion_matrix](https://en.wikipedia.org/wiki/Dispersion_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Doubly_stochastic_matrix](https://en.wikipedia.org/wiki/Doubly_stochastic_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Fisher_information_matrix](https://en.wikipedia.org/wiki/Fisher_information_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Hat_matrix](https://en.wikipedia.org/wiki/Hat_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Orthostochastic_matrix](https://en.wikipedia.org/wiki/Orthostochastic_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Precision_matrix](https://en.wikipedia.org/wiki/Precision_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Stochastic matrix — a non-negative matrix describing a stochastic process. The sum of entries of any row is one. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Transition matrix — a matrix representing the probabilities of conditions changing from one state to another in a Markov chain |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Unistochastic_matrix](https://en.wikipedia.org/wiki/Unistochastic_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Adjacency_matrix](https://en.wikipedia.org/wiki/Adjacency_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Biadjacency matrix — a special class of adjacency matrix that describes adjacency in bipartite graphs. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Degree matrix — a diagonal matrix defining the degree of each vertex in a graph. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Edmonds_matrix](https://en.wikipedia.org/wiki/Edmonds_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Incidence matrix — a matrix representing a relationship between two classes of objects (usually vertices and edges in the context of graph theory). |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Laplacian_matrix](https://en.wikipedia.org/wiki/Laplacian_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Seidel adjacency matrix — a matrix similar to the usual adjacency matrix but with −1 for adjacency; +1 for nonadjacency; 0 on the diagonal. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Skew-adjacency matrix — an adjacency matrix in which each non-zero aij is 1 or −1, accordingly as the direction i → j matches or opposes that of an initially specified orientation. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Tutte_matrix](https://en.wikipedia.org/wiki/Tutte_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Cabibbo-Kobayashi-Maskawa matrix — a unitary matrix used in particle physics to describe the strength of flavour-changing weak decays. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Density matrix — a matrix describing the statistical state of a quantum system. Hermitian, non-negative and with trace 1. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Fundamental matrix (computer vision) — a 3 × 3 matrix in computer vision that relates corresponding points in stereo images. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Fuzzy associative matrix — a matrix in artificial intelligence, used in machine learning processes. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Gamma matrices — 4 × 4 matrices in quantum field theory. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Gell-Mann matrices — a generalisation of the Pauli matrices; these matrices are one notable representation of the infinitesimal generators of the special unitary group SU(3). |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Hamiltonian matrix — a matrix used in a variety of fields, including quantum mechanics and linear-quadratic regulator (LQR) systems. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Irregular matrix — a matrix used in computer science which has a varying number of elements in each row. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Overlap matrix — a type of Gramian matrix, used in quantum chemistry to describe the inter-relationship of a set of basis vectors of a quantum system. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| S matrix — a matrix in quantum mechanics that connects asymptotic (infinite past and future) particle states. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/State-transition_matrix](https://en.wikipedia.org/wiki/State-transition_matrix) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Substitution matrix — a matrix from bioinformatics, which describes mutation rates of amino acid or DNA sequences. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Supnick matrix — a square matrix used in computer science. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Z-matrix — a matrix in chemistry, representing a molecule in terms of its relative atomic geometry. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Jordan_canonical_form](https://en.wikipedia.org/wiki/Jordan_canonical_form) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Linear independence — two or more vectors are linearly independent if there is no way to construct one from linear combinations of the others. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Matrix exponential — defined by the exponential series. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Matrix_representation_of_conic_sections](https://en.wikipedia.org/wiki/Matrix_representation_of_conic_sections) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Pseudoinverse — a generalization of the inverse matrix. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Row echelon form — a matrix in this form is the result of applying the forward elimination procedure to a matrix (as used in Gaussian elimination). |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| [en.wikipedia.org/wiki/Wronskian](https://en.wikipedia.org/wiki/Wronskian) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
```

## Matrix Multiplication and Linear Transformations

why does the -1 trick work?
minus one trick linear algebra

### Micellaneous LA Notes

* General intuitive interpretation of A'A and AA'
* make Anki deck with mathematical formulae, e.g. prob distributions. And names translated in multiple languages
* sum of main diagonal values
* order = size of square matrix
* np.tril, np.triu, np.identity
* diagonal not necessarily square
* An orthogonal matrix is a square matrix whose rows are mutually orthonormal and whose columns are mutually orthonormal
* Multiplication by an orthogonal matrix preserves lengths.
* Q^T· Q = Q · Q^T = I
* A matrix is orthogonal if its transpose is equal to its inverse.
* The rank of a matrix is the estimate of the number of linearly independent rows or columns in a matrix.

### Intuition and Interpretation

* [3B1B animation code](https://github.com/3b1b/manim)
* [matrix 2d](https://github.com/3b1b/manim/blob/master/old_projects/matrix_as_transform_2d.py)
* [engineersCode/EngComp4_landlinear: Using computational thinking to get deep insights on the foundations of linear algebra](https://github.com/engineersCode/EngComp4_landlinear)
* [Exploring Linear Transformations with Python](https://notgnoshi.github.io/linear-transformations/)

```tex
* F: [a, b \\ c, d]
* G: [e, f \\ g, h]
* G(F(x,y)) = G(ax+by, cx+dy)

= e(ax+by) + f(cx+dy), g(ax+by) + h(cx+dy)

= (ea+fc)x + (eb+fd)y, (ga+hc)x + (gb+hd)y
* [ea+fc, eb+fd \\ ga+hc, gb+hd]

```

* Matrix multiplication as the composition of linear transformations
* [Visualizing Linear Transformations – GeoGebra](https://www.geogebra.org/m/YCZa8TAH)
* [mathinsight.org/determinant_linear_transformation](https://mathinsight.org/determinant_linear_transformation)
* I agree, it should start here. It's a simple idea and formal in a simple way. Maybe this sounds weird but I think students often really like formality when it's simple since it lets you just calculate stuff. T(2v1+v2) = T(2v1)+T(v2) = 2T(v1)+T(v2). This is very systematic/practical and easy to explain.
* [Interactive Matrix Visualization](https://shad.io/MatVis/)
* [linear algebra: motivation for matrix multiplication ? (Yahoo Answers)](https://answers.yahoo.com/question/index?qid=20081012135509AA1xtKz)
* [matplotlib.transforms — Matplotlib 3.1.2 documentation](https://matplotlib.org/3.1.1/api/transformations.html)
* [[question] Visualizing a 3D transform? : computergraphics](https://www.reddit.com/r/computergraphics/comments/35ue6l/question_visualizing_a_3d_transform/)
* [Search · 3d linear transformation](https://github.com/search?q=3d+linear+transformation)**
* [02. Visualizing 2D linear transformations](https://dododas.github.io/linear-algebra-with-python/posts/16-12-29-2d-transformations.html)
* [fragen.letsrockmathe.de/](https://fragen.letsrockmathe.de/)
* [codingthematrix.com/python_and_inverse_index_labs.pdf](http://codingthematrix.com/python_and_inverse_index_labs.pdf)
* [Videos: Coding the Matrix, Fall 2014](https://cs.brown.edu/video/channels/coding-matrix-fall-2014/)
* Prove that left multiplication by elementary matrices is equivalent to performing the corresponding elementary row operations.
* 1) interchanging rows
* 3) multiplying a row by a scalar
* [math.utah.edu/~wortman/1050-text-2b2m.pdf](http://www.math.utah.edu/~wortman/1050-text-2b2m.pdf)
* [linear algebra - why don't we define vector multiplication component-wise? - Mathematics Stack Exchange](https://math.stackexchange.com/questions/185888/why-dont-we-define-vector-multiplication-component-wise)
* [*(best answer by Michael Hardy) matrices - Intuition behind Matrix Multiplication - Mathematics Stack Exchange](https://math.stackexchange.com/questions/31725/intuition-behind-matrix-multiplication)
* [* Yahoo linear algebra: motivation for matrix multiplication ? | Yahoo Answers](https://answers.yahoo.com/question/index?qid=20081012135509AA1xtKz)
* [Properties of Matrix Arithmetic](http://sites.millersville.edu/bikenaga/linear-algebra/matrix-properties/matrix-properties.html)
* [linear algebra - Matrix multiplication: interpreting and understanding the process - Mathematics Stack Exchange](https://math.stackexchange.com/questions/24456/matrix-multiplication-interpreting-and-understanding-the-process/24469#24469)
* [Matrix multiplication - Wikipedia](https://en.wikipedia.org/wiki/Matrix_multiplication)
* [Matrix multiplication algorithm - Wikipedia](https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm)
* [*My attempted explanation of the intuition behind matrix multiplication. : math](https://www.reddit.com/r/math/comments/aqgt15/my_attempted_explanation_of_the_intuition_behind/)
* [Visualizing Linear Transformations in R3](https://www.radford.edu/~thompson/Fall10/434/Chapter3/Car/FW%20The%20Smart%20Car_files/LinearTransf3D.pdf)
* [Visualizing linear transformations (article) | Khan Academy](https://www.khanacademy.org/math/linear-algebra/matrix-transformations/linear-transformations/a/visualizing-linear-transformations)
Derive matrix multiplication (general formula) from a set of desiderata, leaving no gaps. (No hand-waving!)
Also, clearly articulate concerns with it and resolve those concerns.
* [math.stackexchange.com/questions/1945329/can-you-transpose-a-matrix-using-matrix-multiplication](https://math.stackexchange.com/questions/1945329/can-you-transpose-a-matrix-using-matrix-multiplication)
* [Visualizing linear transformations (article) | Khan Academy](https://www.khanacademy.org/math/linear-algebra/matrix-transformations/linear-transformations/a/visualizing-linear-transformations)
* [radford.edu/~thompson/Fall10/434/Chapter3/Car/FW](https://www.radford.edu/~thompson/Fall10/434/Chapter3/Car/FW) The Smart Car_files/LinearTransf3D.pdf
* [Matrix Multiplication and Function Composition | Physics Forums](https://www.physicsforums.com/threads/matrix-multiplication-and-function-composition.700209/)
* [linear.ups.edu/html/section-MM.html](http://linear.ups.edu/html/section-MM.html)
* [Game Math: Alternate Views on Matrix Multiplication | Ming-Lun "Allen" Chou | 周明倫](http://allenchou.net/2014/02/game-math-alternate-views-on-matrix-multiplication/)
* [Interpretation of matrix mutliplication XX^t and X^tX : askmath](https://www.reddit.com/r/askmath/comments/3lf5re/interpretation_of_matrix_mutliplication_xxt_and/)
* [matrices - Intuition behind Matrix Multiplication - Mathematics Stack Exchange](https://math.stackexchange.com/questions/31725/intuition-behind-matrix-multiplication?noredirect=1&lq=1)
* [Matrix multiplication - Wikipedia](https://en.wikipedia.org/wiki/Matrix_multiplication)
* [linear algebra - Matrix multiplication: interpreting and understanding the process - Mathematics Stack Exchange](https://math.stackexchange.com/questions/24456/matrix-multiplication-interpreting-and-understanding-the-process/24469#24469)
* [math.stackexchange.com/questions/185888/why-dont-we-define-vector-multiplication-component-wise](https://math.stackexchange.com/questions/185888/why-dont-we-define-vector-multiplication-component-wise)
* [My attempted explanation of the intuition behind matrix multiplication.](https://www.reddit.com/r/math/comments/aqgt15/my_attempted_explanation_of_the_intuition_behind/)
When they understand this, then they will end up doing the equivalent of matrix multiplication by accident. After that matrix multiplication won't look as alien and unnatural.
* [Matrix as sum of elementary products](https://www.google.com/search?q=determinant+as+sum+of+elenentary+products&ie=UTF-8&oe=UTF-8&hl=it-us&client=safari)
* [MM Origins](https://ia802908.us.archive.org/9/items/philtrans05474612/05474612.pdf)
* [Why, historically, do we multiply matrices as we do? - Mathematics Stack Exchange](https://math.stackexchange.com/questions/271927/why-historically-do-we-multiply-matrices-as-we-do)
* [math.stackexchange.com/questions/271927/why-historically-do-we-multiply-matrices-as-we-do](https://math.stackexchange.com/questions/271927/why-historically-do-we-multiply-matrices-as-we-do)
* [When was Matrix Multiplication invented?](http://www.math.harvard.edu/~knill/history/matrix/)
* [math.stackexchange.com/questions/24456/matrix-multiplication-interpreting-and-understanding-the-process/24469#24469](https://math.stackexchange.com/questions/24456/matrix-multiplication-interpreting-and-understanding-the-process/24469#24469)
* [math.stackexchange.com/questions/31725/intuition-behind-matrix-multiplication?noredirect=1&lq=1](https://math.stackexchange.com/questions/31725/intuition-behind-matrix-multiplication?noredirect=1&lq=1)
* [math.stackexchange.com/questions/64631/matrix-multiplication-by-columns?rq=1](https://math.stackexchange.com/questions/64631/matrix-multiplication-by-columns?rq=1)
* [mathinsight.org/matrices_linear_transformations](http://mathinsight.org/matrices_linear_transformations)
* [math.stackexchange.com/questions/375934/matrix-multiplication-why-rows-cdot-columns-columns?noredirect=1&lq=1](https://math.stackexchange.com/questions/375934/matrix-multiplication-why-rows-cdot-columns-columns?noredirect=1&lq=1)
* [math.stackexchange.com/questions/372045/why-is-matrix-multiplication-not-defined-like-this?noredirect=1&lq=1](https://math.stackexchange.com/questions/372045/why-is-matrix-multiplication-not-defined-like-this?noredirect=1&lq=1)
* [math.stackexchange.com/questions/24456/matrix-multiplication-interpreting-and-understanding-the-process?noredirect=1&lq=1](https://math.stackexchange.com/questions/24456/matrix-multiplication-interpreting-and-understanding-the-process?noredirect=1&lq=1)
* [math.stackexchange.com/questions/438398/why-is-matrix-multiplication-defined-a-certain-way?noredirect=1&lq=1](https://math.stackexchange.com/questions/438398/why-is-matrix-multiplication-defined-a-certain-way?noredirect=1&lq=1)
* [math.stackexchange.com/questions/1550010/why-is-the-matrix-multiplication-defined-as-it-is?noredirect=1&lq=1](https://math.stackexchange.com/questions/1550010/why-is-the-matrix-multiplication-defined-as-it-is?noredirect=1&lq=1)
* [math.stackexchange.com/questions/31725/intuition-behind-matrix-multiplication](https://math.stackexchange.com/questions/31725/intuition-behind-matrix-multiplication)
* [math.stackexchange.com/questions/271927/why-historically-do-we-multiply-matrices-as-we-do](https://math.stackexchange.com/questions/271927/why-historically-do-we-multiply-matrices-as-we-do)
* [math.stackexchange.com/questions/396079/composition-of-systems-of-equations](https://math.stackexchange.com/questions/396079/composition-of-systems-of-equations)
* [quora.com/topic/Matrix-Multiplication](https://www.quora.com/topic/Matrix-Multiplication)
* [sosmath.com/matrix/matrix.html](http://www.sosmath.com/matrix/matrix.html)
* [quora.com/Why-matrix-multiplication-isnt-just-componentwise-multiplication](https://www.quora.com/Why-matrix-multiplication-isnt-just-componentwise-multiplication)
* [Linear Transformations](http://math.mercyhurst.edu/~lwilliams/Applets/algebra/linearTransformations.php)
* [Linear Transform Visualizer](http://wosugi.sakura.ne.jp/app/linear-transform/)
* [Matrix Visualizer](https://web.ma.utexas.edu/users/ysulyma/matrix/)
* [Interactive Matrix Visualization](https://shadanan.github.io/MatVis/)
* [math.ucdavis.edu/~linear/linear.pdf](https://www.math.ucdavis.edu/~linear/linear.pdf)
* [khanacademy.org/math/linear-algebra/matrix-transformations](https://www.khanacademy.org/math/linear-algebra/matrix-transformations)
* [notgnoshi.github.io/linear-transformations/](https://notgnoshi.github.io/linear-transformations/)
* [pyephyomaung/matrix-transformation-visualizer-applet](https://github.com/pyephyomaung/matrix-transformation-visualizer-applet)
* [rifoag/lineartransformation](https://github.com/rifoag/lineartransformation)
* [zhangyt3/2D-Linear-Transformation-Plotter](https://github.com/zhangyt3/2D-Linear-Transformation-Plotter)
* [basweber/sympy/wiki/GSoC-2014-Application-Rrubaa-Panchendrarajan:-Implementing-Linear-Transformation](https://github.com/basweber/sympy/wiki/GSoC-2014-Application-Rrubaa-Panchendrarajan:-Implementing-Linear-Transformation)
* [JuliaPackageMirrors/CoordinateTransformations.jl](https://github.com/JuliaPackageMirrors/CoordinateTransformations.jl)
* [TimAlderson/Math-1503-Linear-Algebra-Slides/blob/e00d4cec06c27c6d8e2bf684e1637222a9ac3177/week%208/old/5.3%20Properties.tex](https://github.com/TimAlderson/Math-1503-Linear-Algebra-Slides/blob/e00d4cec06c27c6d8e2bf684e1637222a9ac3177/week%208/old/5.3%20Properties.tex)
* [christianwbsn/richeese-transformator](https://github.com/christianwbsn/richeese-transformator)
* [wildansupernova/Linear-Transformation-Simulation-with-OPENGLAPI](https://github.com/wildansupernova/Linear-Transformation-Simulation-with-OPENGLAPI)
* [KnairdA/Wandler](https://github.com/KnairdA/Wandler)
* [hafizhbudiman/LTVisualizer](https://github.com/hafizhbudiman/LTVisualizer)
* [josh-byster/linear-transform-animator](https://github.com/josh-byster/linear-transform-animator)
* [jakebildy/linalg](https://github.com/jakebildy/linalg)
* [J0315C-human/3Dshapes](https://github.com/J0315C-human/3Dshapes)
* [math.stackexchange.com/questions/1603902/online-visualization-tool-for-planes-spans-in-linear-algebra](https://math.stackexchange.com/questions/1603902/online-visualization-tool-for-planes-spans-in-linear-algebra)
* [ncase.me/matrix/](https://ncase.me/matrix/)
* [realpython.com/python-matplotlib-guide/](https://realpython.com/python-matplotlib-guide/)
* [mathstat.dal.ca/~selinger/linear-algebra/downloads/LinearAlgebra.pdf](https://www.mathstat.dal.ca/~selinger/linear-algebra/downloads/LinearAlgebra.pdf)
* [ssloy/tinyrenderer/wiki](https://github.com/ssloy/tinyrenderer/wiki)
* [proofwiki.org/wiki/Elementary_Row_Operations_Commute_with_Matrix_Multiplication](https://proofwiki.org/wiki/Elementary_Row_Operations_Commute_with_Matrix_Multiplication)
* [Reddit - 3Blue1Brown - Geometric interpretation of the Transpose of a matrix](https://www.reddit.com/r/3Blue1Brown/comments/80wjco/geometric_interpretation_of_the_transpose_of_a/)

### Determinant and Trace

* [Geometric Interpretation of Matrix Transpose](https://www.quora.com/What-is-the-geometric-interpretation-of-the-transpose-of-a-matrix)
* Properties of a determinant: TODO
* [best (constructive proof)](https://amatsukawa.github.io/determinant/)
* [en.wikipedia.org/wiki/Leibniz_formula_for_determinants](https://en.wikipedia.org/wiki/Leibniz_formula_for_determinants)
* [en.wikipedia.org/wiki/Determinant](https://en.wikipedia.org/wiki/Determinant)
* [web.mit.edu/18.06/www/Spring17/Determinants.pdf](http://web.mit.edu/18.06/www/Spring17/Determinants.pdf)
* [ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/least-squares-determinants-and-eigenvalues/properties-of-determinants/MIT18_06SCF11_Ses2.5sum.pdf](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/least-squares-determinants-and-eigenvalues/properties-of-determinants/MIT18_06SCF11_Ses2.5sum.pdf)
* [encyclopediaofmath.org/index.php/Determinant](https://www.encyclopediaofmath.org/index.php/Determinant)
* [phenomenal intro 1](http://www.math.ubc.ca/~anstee/math223/223determinants.pdf)
* [phenomenal intro 2](https://www.cis.upenn.edu/~cis515/dets-ala-Artin.pdf)
* [phenomenal intro 3](http://shannon.cm.nctu.edu.tw/la/la5s09.pdf)
* [likely very good and deep](https://www-m10.ma.tum.de/foswiki/pub/Lehre/ProjektiveGeometrieWS0607/chap6.pdf)
* [dig into deeper](https://amatsukawa.github.io/determinant/)
* [www-users.math.umn.edu/~garrett/m/algebra/notes/26.pdf](http://www-users.math.umn.edu/~garrett/m/algebra/notes/26.pdf)
* [old, but good](https://files.eric.ed.gov/fulltext/ED135631.pdf)
* [geometric approach (basic, but good)
* [brief historical sketch](https://www-history.mcs.st-and.ac.uk/HistTopics/Matrices_and_determinants.html)
* [possibly interesting](https://sheelganatra.com/spring2013_math113/notes/wedge_products.pdf)
* [mitmath/1806/blob/master/summaries.md](https://github.com/mitmath/1806/blob/master/summaries.md)
* [en.wikipedia.org/wiki/Determinant](https://en.wikipedia.org/wiki/Determinant)
* [proofwiki.org/wiki/Determinant_of_Matrix_Product](https://proofwiki.org/wiki/Determinant_of_Matrix_Product)
* [math.stackexchange.com/questions/854459/why-do-determinants-have-their-particular-form](https://math.stackexchange.com/questions/854459/why-do-determinants-have-their-particular-form)
* [math.stackexchange.com/questions/194579/what-is-the-origin-of-the-determinant-in-linear-algebra](https://math.stackexchange.com/questions/194579/what-is-the-origin-of-the-determinant-in-linear-algebra)
* [math.stackexchange.com/questions/1776174/finding-determinant-of-matrix-through-row-operations-problem-help?rq=1](https://math.stackexchange.com/questions/1776174/finding-determinant-of-matrix-through-row-operations-problem-help?rq=1)
* [math.stackexchange.com/questions/1659583/how-is-the-formula-of-determinant-of-3-times-3-or-greater-matrices-derived](https://math.stackexchange.com/questions/1659583/how-is-the-formula-of-determinant-of-3-times-3-or-greater-matrices-derived)
* [math.stackexchange.com/questions/2013886/why-is-the-definition-of-the-determinant-so-weird](https://math.stackexchange.com/questions/2013886/why-is-the-definition-of-the-determinant-so-weird)
* [math.stackexchange.com/questions/668/whats-an-intuitive-way-to-think-about-the-determinant](https://math.stackexchange.com/questions/668/whats-an-intuitive-way-to-think-about-the-determinant)
* [math.stackexchange.com/questions/668/whats-an-intuitive-way-to-think-about-the-determinant](https://math.stackexchange.com/questions/668/whats-an-intuitive-way-to-think-about-the-determinant)
* [proofs for determinant identities](http://users.math.cas.cz/~hrubes/PDFs/DetSIAM.pdf)
* [Determinants and the volumes of parallelotopesand zonotopes](https://core.ac.uk/download/pdf/82756704.pd)
* [Determinants Doud](https://math.byu.edu/~sag/teaching/m313F12/DoudDeterminants.pdf)
* [proof wiki 1](https://proofwiki.org/wiki/Expansion_Theorem_for_Determinants)
* [proof wiki 2](https://proofwiki.org/wiki/Category:Determinants)
* [why is the definition of the determinant so weird?](https://math.stackexchange.com/questions/2013886/why-is-the-definition-of-the-determinant-so-weird)
* [possibly good](https://www.toppr.com/guides/maths/determinants/properties-of-determinants/)
* [Down with Determinants!](https://www.maa.org/sites/default/files/pdf/awards/Axler-Ford-1996.pdf)
* [НОУ ИНТУИТ | Лекция | Определители и их свойства](https://www.intuit.ru/studies/courses/992/207/lecture/5341)
* [linear algebra - What's an intuitive way to think about the determinant? - Mathematics Stack Exchange](https://math.stackexchange.com/questions/668/whats-an-intuitive-way-to-think-about-the-determinant)
* [How to show that $\det(AB) =\det(A) \det(B)$?](https://math.stackexchange.com/questions/60284/how-to-show-that-detab-deta-detb)
* [people.math.osu.edu/husen.1/teaching/571/2_2.pdf](https://people.math.osu.edu/husen.1/teaching/571/2_2.pdf)
* [Is there a definition of determinants that does not rely on how they are calculated?](https://math.stackexchange.com/questions/21614/is-there-a-definition-of-determinants-that-does-not-rely-on-how-they-are-calcula)
* 1) Function Rn --> R
* 2) Switching two rows or columns changes the sign
* 3) Multuplying a row by a constant changes the determinant by the same amount -> The determinant is linear in each row
* 4) The determinant of I is 1

==> These imply a unique function that is the determinant

* Geometric representation: Unit cube of n dimensions, stretched at will
* Think of determinant as oriented volumne
* Determinant is associative in multiplication: Det(AB)=Det(A)Det(B)
* Important takeaway: determinants detect (by becoming 0) the linear dependence of n vectors in dimension n, and they are an expression in the coordinates of those vectors (rather than for instance an algorithm)

-> volume scaling factor of the linear transformation described by the matrix
-> signed volume of the n-dimensional parallelopiped spanned by the column or row vectors of the matrix (easy to prove in the 2D parallelogram case)

* General formula: sum(even elements of one vector * determinant of each one's respective minor)-sum(odd elements of one vector * determinant of each one's respective minor)

-> determines invertibility
-> matrix is singular iff det=0
-> used to define the characteristic polynomial of a matrix (whose roots are the eigenvalues)
-> determinant of an orthogonal matrix is always +-1
-> determinant of a complex Hermitian matrix is always real
-> Jacobian determinants used in calculus in change of variable

* the determinant is the product of all the eigenvalues of the matrix.

#### Trace

* [en.wikipedia.org/wiki/Trace_(linear_algebra)](https://en.wikipedia.org/wiki/Trace_(linear_algebra)?wprov=sfti1)

### SVD

* SVD Steps
* Process:

1. Given A, compute A'A and AA'.
2. Compute eigenvalues (sorted in descending order) and corresponding normalized eigenvector matrix for AA'.
3. S is the diagonal matrix whose values are the square roots of the sorted eigenvalues.
4. Compute U = AVS^-1
5. A = USV'

* Sanderson and Strang Playlists

### Inverse

Matrices: déterminer l'inverse d'une matrice 3x3 par méthode de Gauss (méthode 1)de ENJOY STUDYING

* [Generalized Inverses of Partitioned Matrices Useful in Statistical Applications](https://core.ac.uk/download/pdf/82491713.pdf)
* [math.chalmers.se/~rootzen/highdimensional/blockmatrixinverse.pdf](http://www.math.chalmers.se/~rootzen/highdimensional/blockmatrixinverse.pdf)
* [math.stackexchange.com/questions/1377634/inverse-of-partitioned-matrices](https://math.stackexchange.com/questions/1377634/inverse-of-partitioned-matrices)
* [Inverses of 2 × 2 Block Matrices](http://msvlab.hre.ntou.edu.tw/grades/now/inte/Inverse%20&%20Border/border-LuTT.pdf)
* Partitioned matrix inverse
* [MIT Linear Algebra, Lecture 3: Matrix Multiplication and Inverse Matrices](https://catonmat.net/mit-linear-algebra-part-three)

## Other Writing Notes

```txt
Introduction
Note to reader: This book was originally written for myself. I wrote it to solidify my understanding of mathematics, and the approach I developed came from a few key ideas:
1) Most of my difficulties with mathematics came from gaps in my understanding of fairly basic concepts.
2) Proofs are the best way to understand mathematics. This is because a good proof shows exactly why something is true.
3) Really understanding a proof means being able to paraphrase it in words, and explain each step.
* Set Theory
* Logic
* Geometry and Basic Trigonometry
Sum of Interior Angles of a Triangle
Sum of Interior Angles of an n-gon
Thales' Theorem
Pythagorean Theorem
sin^2+cos^2=1
Law of Cosines
* Number Theory
Prime Factorization for Every Integer
Chinese Remainder Theorem
Euclid's Algorithm
* Linear Algebra
* Matrix Multiplication as Linear Transformation
Invariance of Dot Product w.r.t. Linear Transformation
Projection of a Vector onto a Vector
Angle between Vectors
Core Properties o the Determinant
2D Determinant as Area of Transformation of Unit Square
* Real Analysis
* Miscellaneous
* Differential and Integral Calculus
Fundamental Theorem of Calculus
Green-Stokes-Gauss-Ostogradski theorem
* Derivative Formula
Power Rule
Product Rule
Quotient Rule
Exponent Rule
Logarithm Rule
* Integration by Parts
Integration by Substitution
* Taylor Series
* Optimization Theory
Kuhn-Tucker Equations
* Advanced Trigonometry
* Differential Equations
* Multivariable Calculus
* Combinatorics
Permutation
Binary Coefficients
* Probability Theory
Central Limit Theorem
Law of Large Numbers
Bayes' Law
* Statistics
Digital Mathematics
Algorithms
Artificial Intelligence and Machine Learning
Graph Theory
Complex Analysis
Fourier Analysis
Category Theory
Applications - Physics
Applications - Music
Beginner’s Guide to Matrices
* Front Matter
1. Introduction - How Much Can We Learn from a Single Matrix?
2. What do Matrices Represent?
3. Basic Matrix Features
3. Matrix Types
4. Matrix Factorizations
5. More Advanced Matrix Features
6. Whirlwind Tour of Matrix Applications
Appendix
| Core Mathematical Proofs with Detailed Intuitive Explanation |  |  |
| --- | --- | --- |
book | Matrix Cookbook Proofs |  |  |
| Proofs and Discussion of Non-obvious Matrix Facts |  |  |
| Matrix Family Tree | [en.wikipedia.org/wiki/List_of_matrices](https://en.wikipedia.org/wiki/List_of_matrices) |  |
|  |  |  |
| Intuitive Linear Algebra |  |  |
| Mathematical Definitions for Contrarians: Why Is It Defined That Way? |  |  |
| ELI15 Series |  |  |
|  | Matrix Multiplication |  |
|  | Projection |  |
|  | Determinant |  |
|  | Inverse |  |
|  | Key Ideas about Triangles and Vectors (Triangle Similarity, Pythagorean Theorem, Cauchy-Schwarz, Triangle Inequality) |  |
|  | Elementary Row Operations |  |
|  | LU |  |
|  | QR |  |
|  | SVD |  |
|  | PCA |  |
|  | Jordan Normal Form |  |
|  |  |  |
|  |  |  |
|  | Foundations of Probability (sigma-algebras) |  |
|  | Probability Distributions |  |
|  | Statistical Inference |  |
|  | Fundamental Theorem of Calculus |  |
|  | Taylor Series |  |
|  | Key Differentiation Rules |  |
|  | Key Integration Rules |  |
| Guided Tour of Algebraic Structures |  |  |
| Visualization Scripts: Python, R, Julia, Java, C++ |  |  |
|  | Jordan Normal Form |  |
|  | LU |  |
|  |  |  |
|  |  |  |
* Core Math in an Hour
* Logic & truth tables
Types of Proof
Set Theory
Cartesian Product
Relation
Binary Operation
Function - domain, codomain, range
* > Types of functions
Limits
Derivatives and Integrals
Analytic geometry - PT, vectors
Trigonometry
Linear Algebra
Combinatorics and Probability
Statistics
```

* Math Writing

```txt
* blog post on derivative notation (+notation or related concepts)
Step-by-Step Guide to PCA
Proving the Jordan Decomposition
Deep Dive into Determinants
Overview of Matrix Decompositions (overview, then deeper)
Dot Products, Projections, and Linear Regression
Mathematical Details behind Cosine Similarity
Why Eigenstuff Is Such a Big Deal
How Are Eigenvalues Actually Computed?
Guided Tour of Matrix Types
Painless Introduction to Tensors
Deep Dive into Dot Products
* > Why does the zero dot product imply orthogonality?
* > Why is the dot product onvariant to rotation?
* > Why does theta=0maximize the dot product?
A Closer Look at Gradients in Deep Learning
* > Why do gradients give us the direction of steepest ascent?
* Matrix Types
* squareness

* determinateness

* symmetry

* triangularity

* orthogonality

* normality

* permutation

* invertibility / singularity

* projection matrix

* adjunct matrix

* element-restricted matrices

* elementary matrix

* defective matrices

book - Book Idea: Mathematical Foundations of Natural Language Processing
* Main sources:
* NLP:
* Manning & Schütze
* Jurafsky & Martin
* Eisenstein
* Computerlinguistik und Sprachtechnologie
* Mathematical Methods in Linguistics
* Math & Stats:
* Casella & Berger
* Bernstein
* Hogben
* Vialar
* Aggarwal
* Goodfellow et al.
* Matrix Analysis
* Gallier & Quaintance
* Elements of Statistical Learning

* Guided Tour of Linear Algebra Proofs
-
```

## Mathematisch-logische Grundlagen der Informatik:

* Bayessche Netze
* Expertensysteme
* Modallogik
* MYCIN
* Graphentheorie
* Petri-Netze
* Halbgruppe
* Verband (Algebra)
* Morphismen
* Rekursivität
* Automat
* PROLOG
* Unscharfe Logik
* Undcharfe Menge (Fuzzy-Menge)
* Dynamisches System
* Komplexität
* Berechenbarkeit
* Stetige Funktion in der Topologie
* algebraische Strukturen
* Menge
* Operation (Verknüpfung)
* Halbgruppe (Assoziativität)
* Monoid (Einselement = neutrales Element)
* Gruppe (Inverses)
* abelsche (kommutative) Gruppe (Kommutativität)
* Ordnung einer Gruppe
* Homomorphismus
* Isomorphismus (Bijektivität)
* Isomorph
* Kern
* Bild
* Linksnebenklasse
* Rechtsnebenklasse
* Normalteiler
* Faktorgruppen
* Homomorphiesatz für Gruppen
* Index
* Endomorphismus (Urbild=Bild)
* Monomorphismus (injektiv)
* Epimorphismus (surjektiv)
* Automorphismus (Endo- und Isomorphismus)
* Hybrid structures
* topological group
* Lie group
* ordered group
* ordered ring
* ordered field
* Archimedean group
* topological vector space
* normed vector space
* Hilbert space
* Vertex operator algebra
* Von Neumann algebra

## Stats Notes

```txt
* White
Durbin-Watson
Wu-Hausman
RESET Test
Mann-Whitney test
Kruskal-Wallis test
Efficiency
Bias
Consistency
Clustering
% Tipos de regresiones
bivariate, multiple
Linear regression
Logarithmic regression
Ridge regression
Lasso regression
Ecologic regression
Regression in unusual spaces
Logic regression
Bayesian regression
Quantile regression
LAD regression
Jackknife regression
Linear Regression
Logistic Regression
Polynomial Regression
Stepwise Regression
Ridge Regression
Lasso Regression
ElasticNet Regression
IV
GLS
Fixed Effects
Times Series
RD
MLE
Probit
Logit
RD
DID
Time Series Techniques
Panel Technique
Fixed Effects
Selection Bias Corrections
Matching Techniques
GLS
VKQ
Poisson
Other LDV Models
Monte Carlo
Jackknife
Bootstrap
MLE
Quantile Regression
ANOVA and ANCOVA Models
Multi-Level Models
Factor Analysis
Control Functions
Heckman
Heckman
R
Random Effects
MM
Probit
Logit
* Idea: investigate properties of average between median and mean
* Clustering (of errors)
Durbin Test
Wu-Hausmann Test
R-sq of instruments
Eigenvalue statistic of IV
Sargan Test
Basmann Test
Heckman Correction
* 14. CEF Decomposition Property
15. CEF Prediction Property
16. Anova Theorem
17. Conditional and Unconditional Expectation
18. randomization inference
19. dividing by (n+1)
* 85. Casella & Berger
88. ANOVA
regression theory with matrices
skewness formula
kurtosis formula
Rasch scaling approach
clustering
Moulton Factor
Logit
Probit
Oaxaca-Blinder Decomposition
Heckit, Heckman correction
Huber-White standard errors
Monte Carlo
R-sq of instruments
eigenvalue statistic of IV
Sargan Test
Basmann Test
loops in Stata
Roy model
Monte Carlo

* point estimation and point estimation
* convex fold
* cubic spline
* proxy variable
* response function
* moment restriction
* linear-in-means model
* adjacency matrix
* log normal distribution
* Monte Carlo
* bootstrap
* Wald Estimator
* Permutaion Test
* DGP
* demeaning data
* randomization test
* Roy selection
* different types of charter school lotteries
* $2tapes de l'économétrie:
  1) formuler la question
  2) développer un modèle économique
  3) utiliser l'intuition
  4) spécifier un modèle économique
  5) estimer les paramètres (avec les données) et procéder aux tests
* learn about:
  * different auction designs and models
  * demand estimation
  * BLP estimator
  * moments
  * "non-parametric" regressions
  * nested fixed-point estimation
  * field theory
```
